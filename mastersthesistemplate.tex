\documentclass[10pt]{ucthesis}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%    \pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue \fi

\usepackage{url}
%\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=black,citecolor=black,%
                                       linkcolor=black,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor={YOUR NAME},
                                       pdftitle={YOUR THESIS TITLE},
                                       pdfkeywords={thesis, masters, cal poly}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

%\else
%    \usepackage{graphicx}
%\fi

\usepackage{booktabs} % To thicken table lines
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[letterpaper]{geometry}	
\usepackage[overload]{textcase}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage{array}
%\hypersetup{draft}
%\usepackage[draft]{hyperref}
%\usepackage{nohyperref}  % This makes hyperref commands do nothing without errors
%\usepackage{url}  % This makes \url work
%\usepackage[morefloats=125]{morefloats}
%\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}
\usepackage{color}
\usepackage[nonumberlist,toc]{glossaries}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{morefloats}
\usepackage{float}
\usepackage{listings}
\usepackage{makecell}
\usepackage{appendix}
\usepackage[]{algorithm2e}
\usepackage{titlesec}
\usepackage{tikz}
\usetikzlibrary{angles,quotes}
\usepackage{amsfonts}

%\usepackage[breaklinks=true,hidelinks,pdfusetitle]{hyperref}
% \usepackage{cleveref}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% Added to avoid windows and orphans
\usepackage[all]{nowidow}
% Added to fix spacing between footnote entries
\usepackage{setspace}


\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{example}[definition]{Example}
\newtheorem{algo}[definition]{Algorithm}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{remark}[definition]{Remark}
\newtheorem{corrolary}[definition]{Corrolary}


%\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1in,bmargin=1in,lmargin=1.5in,rmargin=1.5in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

   
\titleformat{\chapter}[hang]
{\normalfont%
    % \huge% %change this size to your needs for the first line
    \bfseries}{\chaptertitlename\ \thechapter}{10pt}{%
    % \huge %change this size to your needs for the second line
    }
    
\titleformat{\section}[hang]
{\normalfont%
    % \huge% %change this size to your needs for the first line
    \bfseries}{ \thesection}{10pt}{%
    % \huge %change this size to your needs for the second line
    }

% \renewcommand*{\chapterheadendvskip}{%
%   \vspace{0.725\baselineskip plus 0.115\baselineskip minus 0.192\baselineskip}%
% }

\titleformat{\chapter}[display]
        {\normalfont\normalsize\centering%\bfseries
        }
        {\ifthenelse{\equal{\thechapter}{A}}{APPENDICES\\[4.3ex]}{}\chaptertitlename\ \thechapter}
        {0pt}{\normalsize\uppercase}
\titlespacing*{\chapter}{0pt}{-10pt}{4.3ex plus .2ex}


\titleformat*{\section}{\normalsize%\bfseries
}
\titleformat*{\subsection}{\small%\bfseries
}
\titleformat*{\subsubsection}{\small%\bfseries
}
\titleformat*{\paragraph}{\small%\bfseries
}
\titleformat*{\subparagraph}{\small%\bfseries}
}

% \hypersetup{nolinks=true}
\begin{document}
\hypersetup{nolinks=true}


% Declarations for Front Matter

% Update fields below!
\title{Representation Theory in Braid Groups}
\author{Jaxon Green}
\degreeyear{2024}
\degreesemester{June}
\degree{Master of Science}
\chair{Professor Ben Richert\\  Professor of Mathematics} 
\othermembers{Professor Sean Gasiorek}
%\othermemberA{OTHER MEMBER HERE \\ & Professor of Mathematics}
%\othermemberB{OTHER MEMBER HERE\\ & Professor of Mathematics} 
\prevdegrees{None}
\numberofmembers{1}
\field{Mathematics} 
\campus{San Luis Obispo}
%\copyrightyears{seven}



\maketitle

\begin{frontmatter}

\copyrightpage


\begin{abstract}

Write an abstract here.

\vspace*{-10pt}
\end{abstract}

\begin{acknowledgements}

Any acknowledgements?

\end{acknowledgements}

\tableofcontents


\listoftables

\listoffigures

% Add CHAPTER into table of contents.

%\addtocontents{toc}{%
   %\noindent Representation Theory\\
   %\noindent Representations of Groups in Physics
%}

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


% \chapter{CHAPTER}
% ------------- Main chapters here --------------------
%\chapter{Sample Chapter 1}
%\label{intro}

 %\section{First Section of Introduction}
 %\label{intro1}
 %This is an equation:
 %\begin{equation}
 %c^2=a^2+b^2.
 %\end{equation}










%\chapter{Sample Chapter 2}

%\section{MY FIRST SECTION}
%There are lots of great resources on the internet to help you learn \LaTeX.  
%Perhaps start with examples like the ones at 
%\begin{verbatim}http://en.wikibooks.org/wiki/LaTeX/Sample_LaTeX_documents.\end{verbatim} 

%It is important to cite references.
%\cite{Blum}  \cite{Gill} \cite{Ped2}
%\cite{Blum, Gill, Ped2}

%Organize the paper into sections and subsections.  

%\subsection{Interesting subsection title}

%You get the idea.  Hey, this one has some displayed math, 
%\[
   %\frac{2}{x} = \sin(\epsilon), 
%\]
%not that it makes any sense whatsoever.  And here is how you 
%do a numbered equation, 
%\begin{equation}
  % \int_{0}^{y^2} f(x) \, dx = \sqrt{z+y}.  
%\end{equation}
%Don't forget to punctuate your equations as part of the sentence.  
%You can do inline math, too, as in $f(x) = \lim_{n\to \infty} n f(x)/n$, which is trivial. 

%\subsection{Another interesting subsection title}

%Okay, not really.  



%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   %\centering
   %\includegraphics[width=4in]{meme.jpg} 
   %\caption{This is a sample figure.  It is not interesting. Note that 
   %figures will ``float'' to wherever \LaTeX \ wants to put them.  }
   %\label{fig:example}
%\end{figure}


\chapter{Representation Theory}

\section{Introduction to Representations}

Over the course of this chapter, we will develop the theory and utility of representations. At a glance, representations give us the ability to dial back the complexity of a mysterious group by viewing its elements as matrices. Thanks to the rigorous development and study of linear algebra, groups of matrices are well-understood structures. Representations allow us to unravel the mystery of an unknown group structure and reveal a group's fundamental properties as results of linear algebra techniques. 

\begin{definition}
	(First Definition) A \textbf{representation} of degree n is a group homomorphism that maps a group into $GL_n(\mathbb{C})$
	$$\phi:G\rightarrow GL_n(\mathbb{C})$$
	We say that $\phi$ is a representation of $G$. If $\phi$ is an injective homomorphism, we say that the representation is \textbf{faithful}. Otherwise, the representation is called \textbf{degenerate}.
\end{definition}

To illustrate to concept of representations, we will consider the group of all roots of unity, $G$, for the following examples. We can construct multiple homomorphisms from $G$ to showcase different kinds of representations. 

\noindent Note: Since $GL_1(\mathbb{C})$ as $\mathbb{C}$ are isomorphic, we identify 3each $1x1$ matrix with its corresponding entry with its element in $\mathbb{C}$.

\begin{example}
	(Trivial Representation)\\\\
	\renewcommand{\arraystretch}{0.7}
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_1(\mathbb{C})$\\
		$\hspace{6mm}g\mapsto 1$
		\end{tabular}
\end{example}
\noindent This map is the trivial homomorphism from $G$ to $GL_1(\mathbb{C})$ and therefore it easily satisfies the requirement of a degree $1$ representation of $G$. We say that $\phi$ is the \textbf{trivial representation} of $G$.

	

\begin{example}
	(Nontrivial Degree 1 Representation) \\
	By construction of $G$, if $g\in G$, then $g = e^{\frac{2\pi im }{n}}$ where $m,n\in \mathbb{Z}$	
	\renewcommand{\arraystretch}{0.7}\\\\
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_1(\mathbb{C})$\\
		$\hspace{6mm}g\mapsto g$
		\end{tabular}
\end{example}
\noindent where we view $G$ as a multiplicative subgroup of $\mathbb{C}$. This observation trivializes the argument that $\phi$ is a homomorphism. Therefore, $\phi$ is a degree $1$ representation of $G$.		


\begin{example}
	(Degree 2 Representation) \\\\
	\renewcommand{\arraystretch}{1}
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_2(\mathbb{C})$\\
		$\hspace{0mm}e^{2\pi i\frac{m}{n}}\mapsto \begin{bmatrix}
							\cos(\frac{2\pi m}{n}) & \sin(\frac{2\pi m}{n}) \\
							-\sin(\frac{2\pi m}{n}) & \cos(\frac{2\pi m}{n})
						      \end{bmatrix}$
		\end{tabular}
\end{example}
\noindent To show this map is a homomorphism, we will take two elements of $G$, say $e^{2\pi i\frac{x}{y}}$ and $e^{2\pi i\frac{a }{b}}$ and track the image of their product under $\phi$. \\
	\begin{equation}
		\begin{aligned}
			\phi(e^{2\pi i\frac{x}{y}} * e^{2\pi i\frac{a}{b}} ) &= \phi(e^{2\pi i(\frac{x}{y}+\frac{a}{b})})\\ 
												    &= \begin{bmatrix}
														\cos(2\pi(\frac{x}{y}+\frac{a}{b})) & \sin(2\pi(\frac{x}{y}+\frac{a}{b})) \\
														-\sin(2\pi(\frac{x}{y}+\frac{a}{b})) & \cos(2\pi(\frac{x}{y}+\frac{a}{b}))
													  \end{bmatrix}\\
												    &= \begin{bmatrix}
														\cos(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \sin(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})   &\sin(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) + \cos(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})\\
														-\sin(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \cos(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b}) & \cos(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \sin(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})
													  \end{bmatrix}\\ 
												    &= \begin{bmatrix}
														\cos(2\pi\frac{x}{y}) & \sin(2\pi\frac{x}{y}) \\
														-\sin(2\pi\frac{x}{y}) & \cos(2\pi\frac{x}{y})
												          \end{bmatrix}
											  		  \begin{bmatrix}
														\cos(2\pi\frac{a}{b}) & \sin(2\pi\frac{a}{b}) \\
														-\sin(2\pi\frac{a}{b}) & \cos(2\pi\frac{a}{b})
													  \end{bmatrix} \\
		                                                                                    &= \phi(e^{2\pi i\frac{x}{y}})*\phi(e^{2\pi i\frac{a}{b}})
		\end{aligned}
	\end{equation}
	Since, $\phi$ has been shown to be a homomorphism, we can conclude that $\phi$ is also a degree $2$ representation of $G$.\\

	\noindent Is $\phi$ faithful or degenerate?A faithful representation would have a trivial kernel. Suppose $\phi(e^{2\pi i\frac{x}{y}}) = I_2$ ($I_n$ is the Identity Matrix of dimension $n\times n$). 
	\begin{equation}
		%\begin{aligned}
			\begin{bmatrix}
				\cos(2\pi\frac{x}{y}) & \sin(2\pi\frac{x}{y}) \\
				-\sin(2\pi\frac{x}{y}) & \cos(2\pi\frac{x}{y})
			\end{bmatrix}\\
			= \begin{bmatrix}
				1 & 0 \\
				0 & 1 \\
			\end{bmatrix}\\
		%\end{aligned}
	\end{equation}
	\noindent Comparing entrywise, we see that $\cos(2\pi\frac{x}{y}) = 1$ and $\pm\sin(2\pi\frac{x}{y}) = 0$. Using any of these equations, we see that $\frac{x}{y}= n$ for some $n\in\mathbb{Z}$. Therefore, $ker(\phi)=\mathbb{Z}$ and this representation is degenerate.\\


Alternatively, we can formulate the definition of a representation in a different context, illuminating a useful interpretation that will be used extensively throughout this paper.

\begin{definition}
	(Second Definition) Let $G$ be a group, let $V$ be a linear vector space, and let $\mathcal{L}(V)$ be the group of linear operators on V together with the operation of composition. A \textbf{representation} of $G$ is a group homomorphism that maps $G$ into $\mathcal{L}(V)$.
	$$\phi : G \rightarrow \mathcal{L}(V)$$
The degree of the representation is the dimension of $V$.
\end{definition}

\begin{remark}
	In the case where we have a finite dimensional vector space, we can make an interesting observation. Suppose $V$ is finite dimensional and $G$ is a group. It is easy to identify both definitions of representations with one another. Let $\{e_i\}_{i=1}^n$ be a basis for $V$. Let $\phi : G \rightarrow \mathcal{L}(V)$ be a representation of $G$. Then $\forall g \in G$, $U_g\coloneq\phi(g)$ is a linear operator on $V$. $U_g$ has a corresponding matrix, $M(U_g)$, with coefficients defined by the image of our basis vectors of $V$.
$$M(U_g) = \begin{array}{c c}
			U_g(e_1) \hspace{2mm} U_g(e_2) \hspace{2mm} \hdots \hspace{2mm}  U_g(e_n) & \\
			\begin{bmatrix}
				m_{11} & m_{12} & \hdots & m_{1n}\\
				m_{21} & m_{22} & \hdots & m_{2n} \\
				\vdots & \vdots & \ddots & \vdots\\
				m_{n1} & m_{n2} & \hdots & m_{nn}\\
			\end{bmatrix}
			&
		        %\def\arraystretch{0.75}
			\begin{array}{c}
				e_1\\
				e_2\\
				\vdots\\
				e_n\\
			\end{array}
		\end{array}$$
$$U_g(e_j) = \sum_{i=1}^n m_{ij}e_i$$
\renewcommand{\arraystretch}{0.5}
Does the map \begin{tabular}{l}
			$\psi:G\rightarrow GL_n(\mathbb{C})$\\
			\hspace{6mm}$g\mapsto M(U_g)$
       		 \end{tabular}
satisfy the criteria to be considered a representation (by the first definition)? If $g,h \in G$, then

	$$\psi(gh) = M(\phi(gh)) = M(\phi(g)\circ \phi(h)) = M(\phi(g))*M(\phi(h)) = \psi(g)\psi(h)$$

Where the homomorphism property of $\phi$ is used in succession with the relationship between the composition of operators and the multiplication of their corresponding matrices. This observation illustrates a special conenction between the two definitions of a representation. If we are given a representation defined in the either way, we can interpret the target space of the homomorphism in the context of both definitions. That is to say, every $n\times n$ matrix can be interpreted as a linear operator on an $n$-dimensional vector space, and vice-versa. The homomorphism property of one definition is a necessary condition for the homomorphism in the other definition. Hence, we can see the two definitions of representations are equivalent in the case of a finite dimensional vector space.
\end{remark}

\begin{example}
	Let $G$ be the group defined by the complex unit circle and the operation of multiplication and let $V = \mathbb{C}$ 
	\begin{center}
		 \begin{tabular}{l}$\phi:G\rightarrow \mathcal{L}(V)$\\
				$\hspace{4mm}e^{i\theta}\mapsto U_{e^{i\theta}}$
		\end{tabular}
	\end{center}
	where $U_{e^{i\theta}}$ is the linear operator (on $V$) that multiplies its input by $e^{i\theta}$. 
\end{example}

Each operator is clearly linear. The process of confirming a map is a representation is relatively standard. However, there does not seem to be any intuitive way to come up with a new representation. The rest of this chapter will be devoted the process of comparing and characterizing every representation of a given group. We appeal to Definition 3.5 to argue that this map is a representation. Let $e^{i\theta}$, $e^{i\psi} \in G$. Then $\phi(e^{i\theta} * e^{i\psi}) = U_{e^{i(\theta+\psi)}}$. For all $re^{i\gamma} \in V$, we have

	\begin{equation}
		\begin{aligned}
			U_{e^{i(\theta+\psi)}}(re^{i\gamma}) &= re^{i\gamma} * e^{i(\theta+\psi)} \\
										&= re^{i\gamma + i\psi + i\theta} \\
										&= U_{e^{i\theta}}(re^{i\gamma + i\psi}) \\
										&= U_{e^{i\theta}}(U_{e^{i\psi}}(re^{i\gamma})) \\
										&= (U_{e^{i\theta}}\circ U_{e^{i\psi}}) (re^{i\gamma})
		\end{aligned}
	\end{equation}

Since $\phi(e^{i\psi})*\phi(e^{i\theta}) = U_{e^{i\theta}}\circ U_{e^{i\psi}}$, we have shown that the homomorphism property of $\phi$ is satisfied. Therefore, $phi$ is a representation. We can now identify this definition of a representation with the initial formulation in two possible ways.

\begin{assumption}
	$V$ is a vector space over $\mathbb{C}$ (as a field). 
\end{assumption}

If we consider $V$ to be a vector space over $\mathbb{C}$, then it is a one-dimensional vector space. This means that the matrix of any operator defined on $V$ will be a $1\times 1$ matrix (or, an element of $\mathbb{C}$). Taking the basis $\{1\}$ of $V$ and $e^{i\theta} \in G$, we see that 

$$M(U_{e^{i\theta}}) = \begin{bmatrix}
					e^{i\theta}
				\end{bmatrix} = e^{i\theta} $$
This is clearly a degree one representation given by Definition 3.1.

\begin{assumption}
	$V$ is a vector space over $\mathbb{R}$
\end{assumption}

If $V$ is a vector space over $\mathbb{R}$, then it is a two-dimensional vector space. This means that the matrix of any operator defined on $V$ will have be of shape $2\times2$. Taking the basis $\{1,i\}$ of $V$, any $e^{i\theta} \in G$, and the identity $e^{i\theta} = \cos(\theta) + i\sin(\theta)$ we see that the following equalities hold:
$$(a+bi)e^{i\theta} = (a\cos(\theta)-b\sin(\theta))+i(a\sin(\theta)+b\cos(\theta))$$
$$M(U_{e^{i\theta}}) = \begin{bmatrix}
					\cos(\theta) & -\sin(\theta) \\
                                        \sin(\theta) & \cos(\theta)
				\end{bmatrix}$$

This representation will be revisited later in greater detail as multiplication of complex numebrs by $e^{i\theta}$ corresponds to rotation in the complex plane by the angle $\theta$ about the origin.

For the rest of the paper, we shall almost exclusively be considering finite dimensional vector spaces and therefore will interchangeably use both definitions of representations as needed.

\section{Decomposing and Characterizing Representations}

While we have shown it is relatively straightforward to argue whether a given map is a representation, it is not yet clear how we can come up with our own, compare different ones, or what kinds of properties a representation has. This section will explore the properties of representations and how we can use them to deepen our understanding of representations.

\begin{definition}
	Two representations, $\phi$ and $\psi$, are said to be \textbf{equivalent representations} if there exists some invertible operator/matrix (depending on definition of representation), $M$, such that $$\phi = M \psi M^{-1}$$
\end{definition}

In the context of linear algebra, this conjugation by an invertible matrix can most easily be thought of as a change of basis transformation. With this in mind, we can see that representations of groups can be spilt into equivalence classes based on matrix similarity (or similarity of any matrix of the operator). In order to deduce whether or not representations are equivalent, we need to utilize matrix-similarity-preserving opertations to find common traits. A natural first choice is the trace operation on matrices.

\begin{definition}
	The \textbf{character} of a representation, $\phi$, on $g \in G$, denoted $\chi^{\phi}(g)$, is defined by $$\chi^{\phi}(g)=trace(\phi(g))$$
\end{definition}

\begin{theorem}
	If two representations are equivalent, then character of both representations are the same.
\end{theorem}

(Pf.) Suppose $\phi$ and $\psi$ be two equivalent representations. Then $\exists M$ such that $\forall g \in G$, $\phi(g) = M\psi(g)M^{-1}$. Then $\forall g \in G$,

\begin{equation}
	\begin{aligned}
		\chi^{\phi}(g) &= trace(\phi(g)) \\
						&= trace(M\psi(g)M^{-1}) \\
						&= trace(\psi(g)M^{-1}M) \\
						&= trace(\psi(g)) = \chi^{\psi}(g) \\
	\end{aligned}
\end{equation}

Therefore, $\chi^{\phi} = \chi^{\psi}$. $\qedsymbol$\\

The character of a representation gives us the ability to quickly rule out equivalence of representations without getting into messy matrix calculations, especially in higher degree representations.

\begin{example}
	Let $S_3$ be the symmetric group of degree 3 and $\phi$ and $\psi$ be defined below. Comparing the outputs of each map, it is clear that the maps are not identical. Are these representations equivalent? We can use the trace argument to justify why they are not. We can observe the following equalities directly: 
$$\chi^{\phi}(\sigma) = \chi^{\psi}(\sigma) \hspace{1mm} for \hspace{1mm} \sigma \in \{e, (12), (13), (23)\}$$
$$\chi^{\phi}(\sigma) \neq \chi^{\psi}(\sigma) \hspace{1mm} for \hspace{1mm} \sigma \in \{(132), (123)\}$$
As a result, $\chi^{\phi} = \chi^{\psi}$, and therefore, these representations are not equivalent. Despite this fact, the significance of this example is that we can identify similarities in both representations that lead us to believe that there is something inherently similar about them. Specifically, both representations send every permutation in $S_3$ to a matrix with its first row (column) fixed as $[1 \hspace{1mm} 0 \hspace{1mm} 0](^T)$. This observation is reminiscent of the trivial representation, defined in Example 3.2. We will revisit this matter later.
\\
	\renewcommand{\arraystretch}{1.25}
	\begin{center}
		$\begin{array}{c c}
			\begin{array}{c}
				\phi:S_3\rightarrow M_3(\mathbb{R}) \\
				\hspace{0mm} e \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (12) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (13) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (23) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (123) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (132) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
			\end{array} &
	
			\begin{array}{c}
			\psi:S_3\rightarrow M_3(\mathbb{R}) \\
				\hspace{0mm} e \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (12) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & -1 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (13) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & -1 & -1 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (23) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 0 & 1 \\
												0 & 1 & 0 \\
											\end{bmatrix}\\
				\hspace{0mm} (123) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 0 & 1 \\
												0 & -1 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (132) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & -1 & -1 \\
												0 & 1 & 0 \\
											\end{bmatrix}\\
			\end{array}
		\end{array}$
	\end{center}
\end{example} 

As eluded to in the previous example, it seems that representations can share "pieces" in common without being considered equivalent. In the same way we decompose linear operators (and their matrices) into a block diagonal structure for the simplicity of our study, we can decompose the linear operators (and matrices) of a representation in the same way to make strikingly similar conclusions. In this way, we can reveal a more intuitive understanding of the underlying representation and create an natural way to fully decompose any arbitrary representation into natural components. 

\begin{definition}
	Let $\phi$ be a representation of the group $G$ and $U_G \coloneq \{\phi(g)=U_g: V\rightarrow V \hspace{1mm}| \hspace{1mm} g\in G\}$. Let $W \subset V$. $W$ is said to be an \textbf{invariant subspace} with respect to $U_G$ if $\forall v \in W$ and $\forall g \in G$, $U_g(v)\in W$.
\end{definition}

In general, we say that a subspace satisfying the above condition is invariant with respect to $\phi$. Our most familiar understanding of invariant subspaces comes from our study of decomposing generic linear operators (matrices) into its corresponding eigenspaces. This process is not unfamiliar from what we will study next, but with a heavier restriction, since our new definition considers many operators at once.

\begin{definition}
	A representation is said to be \textbf{irreducible} if there is no nontrivial, invariant subspace with respect to it. Otherwise, we say that the representation is reducible.
\end{definition}

It will turn out that the irreducible representations of a group will be the "pieces" that we can recognize as the building blocks of each representation. Due to the nature of invariant subspaces, it is no surprise that we can see a block diagonal structure form in the matrices of representations.

\begin{definition}
	If $M$ and $N$ are square matrices, then let $$M\oplus N \coloneq \begin{bmatrix}
																			M & 0\\
																			0 & N\\
																		\end{bmatrix}$$
	we call this new matrix the \textbf{direct sum of M and N}
\end{definition}

\begin{example}
	Referring back to one of the matrices from Example 3.13, we can see that
$$\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & -1 & -1
\end{bmatrix} = \begin{bmatrix}
					1
					\end{bmatrix} \oplus
					\begin{bmatrix}
						1 & 0 \\
						-1 & -1
					\end{bmatrix}$$
It becomes increasingly clear that the fixed first row and column of these matrices are directly linked to the trivial representation.
\end{example}

\begin{theorem}
	Let $\phi$ be a representation of a group $G$.  Then there exists a set of irreducible representations of $G$, $\{\psi_i\}_{i=1}^j$, such that $$\phi = T\left(\bigoplus_{i=1}^j \alpha_i*\psi_i\right)T^{-1}$$ where $\alpha_i*\psi_i \coloneq \underbrace{\psi_i \oplus \psi_i \oplus \hdots \oplus \psi_i}_{\alpha_i times}$ and $T$ is some invertible matrix/operator. 
\end{theorem}
 
\noindent (Pf.) By induction on the degree of the representation $n$. \\

(Base Case: $n = 1$) Suppose that $\phi$ is a degree one representation of $G$, with $U_G$ being defined as in Definition 3.14. Then $\forall U_g \in U_G$, we have seeking to show that every invariant subspace of $V$ with respect to $U_g$ is trivial. Suppose that $W \subset V$ is a subspace and let $dim(W)$ denote the dimension of $W$. Then, $dim(W) \leq dim(V) = 1$ as given by the degree of the representation. If $dim(W) = 1$, then $W=V$ and therefore, $W$ is a trivial subspace. If $dim(W) < dim(V)$, then it can only be the case that $dim(W) = 0$, and therefore, $W= \{0\}$, which is also trivial. Therefore, every possible subspace of $V$ must be trivial, and as a result, every invariant subspace must also be trivial. Hence, $\phi$ is an irreducible representation. \\

(Inductive Hypothesis) Suppose that for any representation of degree $0<k\leq n$, $\phi$, we have that $\phi =T\left(\bigoplus_{i=1}^j \alpha_i*\psi_i\right)T^{-1}$ where $T$ is some invertible matrix/operator and $\{\psi_i\}_{i=1}^j$ is some set of irreducible representations of $G$. \\

Let $\phi'$ be a representation of G of degree $n+1$. If $\phi'$ is irreducible, then we are done. If $\phi'$ is reducible, then $\exists W \subset V$ such that $W$ is a nontrivial, $\phi'$-invariant subspace. Let $W = span\{w_i\}_{i=1}^k$ where $k\leq n$ and choosing a basis for $V$ to be $\{w_i\}_{i=1}^k \cup \{v_i\}_{i=k+1}^{n+1}$ with $v_i \notin W \hspace{1.5mm} \forall i$. Then, there exists an invertible matrix $T$ such that
\begin{equation}
	M(\phi') = T\begin{bmatrix}
					M_W& 0\\
					0 & M_{X}\\
					\end{bmatrix}T^{-1}
\end{equation}
where $M_W$ is a $k\times k$ block representing the invariant subspace $W$ and $M_{X}$ is a $(n-k+1)\times (n-k+1)$ block representing the subspace defined by $X \coloneq span\{v_i\}_{i=k+1}^{n+1}$. Given the structure of our block matrices, both the $M_W$ and $M_X$ blocks are $k$ degree and $n+1-k$ degree representations of $G$. Therefore, we can apply our induction hypothesis to each of the blocks and to argue that 
$$M_W = A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}$$
$$M_W = B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}$$
\begin{equation}
	M(\phi') = T\begin{bmatrix}
					A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}& 0\\
					0 &  B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}\\
					\end{bmatrix}T^{-1}
\end{equation}
We can break up our matrix $T$ into block structure to match the size of our blocks in Equation 3.5.
\begin{equation}
	M(\phi') =  \begin{bmatrix}
					T_{11} & T_{12} \\
					T_{21} & T_{22}
				\end{bmatrix}
				\begin{bmatrix}
					A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}& 0\\
					0 &  B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}\\
				\end{bmatrix}
				\begin{bmatrix}
					T_{11}^{-1} & T_{12}^{-1} \\
					T_{21}^{-1} & T_{22}^{-1}
				\end{bmatrix}
\end{equation}
Using algebraic manipulations, we can pull back our change of basis matrices into corresponding blocks of $T$.
\begin{equation}
	M(\phi') =  \begin{bmatrix}
					T_{11}A & T_{12}B \\
					T_{21}A & T_{22}B
				\end{bmatrix}
				\begin{bmatrix}
					\bigoplus_{i=1}^j \alpha_i\psi_i & 0\\
					0 &  \bigoplus_{i=1}^{l} \beta_i\mu_i \\
				\end{bmatrix}
				\begin{bmatrix}
					A^{-1}T_{11}^{-1} & A^{-1}T_{12}^{-1} \\
					B^{-1}T_{21}^{-1} & B^{-1}T_{22}^{-1}
				\end{bmatrix}
\end{equation}
%Prove that our new T and T^{-1} are actually inverses
Given that the flanking matrices are both inverses of each other, which we will refer to as $P$ and $P^{-1}$ respectively, we take $\{\nu_i\}_{i=1}^{j+l}$ and  $\{\gamma_i\}_{i=1}^{j+l}$ to be defined by 
$$\begin{array}{c c}
	\nu_i = \begin{cases}
		\psi_i & i \leq j\\
		\mu_{i-j} & i > j
	\end{cases}
&
	\gamma_i = \begin{cases}
		\alpha_i & i \leq j\\
		\beta_{i-j} & i > j
	\end{cases}
\end{array}$$
to conclude that
\begin{equation}
	\phi' = P\left(\bigoplus_{i=1}^{j+l} \gamma_i*\nu_i\right)P^{-1}
\end{equation} \qedsymbol

\begin{remark}
	The main purpose of this theorem is to illustrate that any representation can be thought of as a "linear combination" of irreducible representations of that group. Being able to readily compare the irreducible representation decomposition of any two given representations is key to understanding similarities and differences between them.
\end{remark}

\begin{example}
	Let $\phi$ and $\psi$ be defined as they were in Example 3.13. 
\end{example}

\noindent There have been three different irreducible representations that were used to construct these maps. Consider $\mu_1$ to be the trivial representation and $\mu_2$ and $\mu_3$ to be defines as below:
$$\begin{array}{c c}
	\begin{array}{c}
		\mu_2 : S_3 \rightarrow \mathbb{R} \\
		\sigma \mapsto sign(\sigma) = \begin{cases}
										1 & \text{if even permutation} \\
										-1 & \text{if odd permutation}
									   \end{cases}\\
		\text{Sign Representation}
	\end{array}
&
	\begin{array}{c}
		\mu_3 : S_3 \rightarrow M_2(\mathbb{R}) \\
		\sigma \mapsto \text{Bottom Right } 2\times2 \text{ Block of } \psi(\sigma)\\
		\text{Degree 2 Irreducible Representation}
	\end{array}
\end{array}$$
We can see the following two equalities hold:
$$\phi = \mu_1 \oplus \mu_1 \oplus \mu_2 = 2\mu_1 \oplus \mu_2$$
$$\psi = \mu_1 \oplus \mu_3$$

Now that we have established that representations are best understood by studying the irreducible representations that compose them, we shall focus our attention to characterizing the irreducible representations. There are many theorems and useful corrolaries that we will make use of later that will be established now.

\begin{theorem}
	If $\phi$ is an irreducible representation of a group $G$, then any representation equivalent to $\phi$ must also be irreducible.
\end{theorem}

\noindent (Pf.) For this proof, we will consider the linear operator definition of representations. Let $\phi_g \coloneq \phi(g)$ and consider each operator to be defined on the vector space $V$. Let $\psi$ be defined as an equivalent representation of $G$, so there exists an invertible, linear operator, $S$ such that $\phi_g = S\psi_g S^{-1}$ $\forall g \in G$. we can assume that for every $g$, $\phi_g$ is defined as an operator on the vector space $V'$. It will suffice to show that any $\psi_g$-invariant subspace of $V'$ is a trivial subspace. \\

Let $W' \subset V'$ be a $\psi_g$-invariant subspace. Let $w' \in W'$. Then, the following equation holds:

$$(\phi_gS)(w') = (S\psi_g)(w')$$

This equality illustrates that for any $v \in S(W')$ (the image of $W'$ under $S$, $\phi_g (v) \in S(W')$. Therefore, $S(W')$ is a $\phi_g$-invariant subspace. By definition, $S(W') = \{0\}$ or $S(W') = V$. \\

Since $S$ is an invertible operator, the rank-nullity theorem tells us that $dim(W')=0$ or $dim(W') = dim(V)$. Then, either $W' = \{0\}$ or the equivalence of representations gives us that $dim(V)=dim(V')$, and therefore, $W' = V'$. \\

Since there are no non-trivial $\psi_g$-invariant subspaces, $\psi$ must also be an irreducible representation of $G$. \qedsymbol



\begin{theorem} \textbf{Schur's Theorem}
	Let $\phi$ and $\psi$ be two irreducible representations of the group $G$. Let $M$ be a matrix/linear map defined such that $M\phi(g) = \psi(g)M \hspace{1.5mm}\forall g \in G$. Then $M$ is invertible or $0$. 
\end{theorem}

\noindent (Pf.) Reminder: When viewing this proof from the perspective of operators, interpret the product of operators as composition as I defined at the beginning. 

Suppose the degree of $\phi$ is $n$ and the degree of $\psi$ is $m$. Then $M$ must be an $m\times n$ matrix. Let $v\in ker(M)$. Then $\forall g \in G$, we have 
\begin{equation}
	 (M\phi(g))v= (\psi(g)M)v = 0
\end{equation}

This shows us that $\phi(g)v \in ker(M)$ as well. As a result, $ker(M)$ shown to be a $\phi$-invariant subspace. Since $\phi$ is irreducible, it must be the case that either $ker(M) =\{0\}$ or $ker(M)$ is the entire space.  

Similarly, if $w \in im(M)$, then we can show that $im(M)$ is a $\psi$-invariant operator with the following argument: $\forall g\in G$,

\begin{equation}
	\psi(g)w =\psi(g)M(x) = M\phi(g)x \in im(M)
\end{equation}

for some $x$ in our space. Then, $im(M)$ must be a $\psi$-invariant subspace, and therefore, $im(M) = \{0\}$ or the whole space.

If $ker(M)$ is the whole space or $im(M) = \{0\}$, then clearly, $M=0$. However, if this is not the case, then $ker(M) = \{0\}$ and $im(M)$ is the whole space, and we have $m=n$ giving us an invertible $M$. \qedsymbol

\begin{corrolary}
	Let $\phi$ be an irreducible representation and $M$ be a matrix/operator such that $M\phi(g)=\phi(g)M \hspace{1.5mm}\forall g \in G$. Then $M$ is a mulptiple of the identity matrix/map.
\end{corrolary}

\noindent (Pf.) Let $\lambda$ be an eigenvalue of $M$. Then $M - \lambda I$ is not invertible. Following from Schur's Theorem, we see that $\forall g \in G$
$$(M-\lambda I)\phi(g) = \phi(g) (M-\lambda I)$$
Therefore, it must be the case that $M-\lambda I = 0$ \cite{Mendes} \qedsymbol 

\begin{corrolary}
	If $G$ is an abelian group, then any irreducible representation of $G$ is can be viewed as a degree one representation.
\end{corrolary}

\noindent(Pf.) Let $\phi$ be an irreducible representation of $G$. Let $g \in G$. Then, $\forall h \in G$, $\phi(g)\phi(h) = \phi(h)\phi(g)$. Then, Schur's Theorem tells us that $\phi(g) = \lambda_g I$ for some $\lambda_g \in \C$. Since $g$ is arbitrary, this argument forces every matrix in the representation to be a scalar multiple of the identity matrix. We can take the degree one representation to be the first entry of each matrix. \cite{Tung} \qedsymbol

\section{Unitary Representations}

While irreducible representations are of great importance to us, we can also deepend our understanding of a representation when our underlying vector space has an inner-product structure. In this case, we can further characterize our representations. 

\begin{definition}
	Let $V$ be an inner-product space. Let $U \in \mathcal{L}(V)$. $U$ is said to be \textbf{unitary} if $U$ is surjective and $\forall x,y \in V$, $\langle x , y \rangle = \langle U(x) , U(y) \rangle$.
\end{definition}

\begin{definition}
	A representation, $\phi$, of a group, $G$, is said to be a \textbf{unitary representation} if $\forall g\in G$, $\phi(g)$ is unitary.
\end{definition}

\begin{theorem}
	Every representation of a finite group on an inner-product space is equivalent to a unitary representation
\end{theorem}

\noindent (Pf.) Let $\phi$ be a non-unitary representation of a finite group $G$ defined on an inner-product space. Let $\phi_g$ denote $\phi(g)$, and let $S$ be defined such that $\langle S(x) , S(y) \rangle = \sum_{g \in G} \langle \phi_g (x) , \phi_g (y) \rangle$. We will show that the operator $\forall g \in G$, $U_g \coloneq S\phi_gS^{-1}$ is unitary. Fixing $g\in G$,


\begin{equation}
	\begin{aligned}
		\langle U_g(x) , U_g(y) \rangle &= \langle S\phi_gS^{-1}(x) , S\phi_gS^{-1}(y)\rangle \\
									&= \sum_{h \in G} \langle \phi_h(\phi_gS^{-1}(x)) , \phi_h(\phi_gS^{-1}(y))\rangle \\
									&= \sum_{h\in G} \langle \phi_{hg}(S^{-1}(x)) , \phi_{hg}S^{-1}(y))\rangle \\
									&= \sum_{l\in G} \langle \phi_{l}(S^{-1}(x)) , \phi_{l}S^{-1}(y))\rangle \\
									&= \langle S(S^{-1}(x)) , S(S^{-1}(y))\rangle \\
									&= \langle x , y \rangle 
	\end{aligned}
\end{equation}

Therefore, $U_g$ is unitary $\forall g \in G$. \qedsymbol

\begin{remark}
	Combining \textbf{Theorem 1.21} and \textbf{Theorem 1.27} allows us to grant the property of unitary to any irreducible representation through similarity transform. 
\end{remark}

The next portion of this section will be devoted to identifying and proving some of the key properties of irreducible and unitary representations. We will extensively use these properties in later sections.

\begin{theorem}
	Let $G$ be a finite group, let $\Phi = \{\phi \mid \phi \text{ is a distinct (inequivalent to others in set) irreducible, unitary representation of }G\}$. Let $\phi,\psi \in \Phi$ with degrees $n_{\phi}$ and $n_{\psi}$ respectively. Let $\phi_g \coloneq \phi(g)$ and $\psi_g \coloneq \psi(g)$. Then the following equality holds:
$$\frac{n_\phi}{|G|} \sum_{g\in G} \left[\phi_g \right]_{ij} \left[\psi_g^\dag \right]_{kl} = \begin{cases}
																						1 & \text{if } \psi = \phi,\hspace{1mm} j=k,\hspace{1mm} \text{and } i=l\\
																						0 & else
																					 \end{cases}$$
where for any matrix $M=\left[m\right]_{ij}$, $\left[M^\dag\right]_{ij} = \overline{m}_{ji} $. We refer to this as the \textbf{orthonormality condition} of unitary, irreducible representations.
\end{theorem}

\noindent (Pf.) Let $A$ be a $n_\phi \times n_\psi$ matrix and let $M_A \coloneq \sum_{g\in G} \phi_{g} A \psi_g^{\dag}$. Note: $\forall h \in G$, we have $\phi_h M_A  = M_A \psi_h^\dag$ by as seen from the following calculation:

\begin{equation}
	\begin{aligned}
		\phi_h M_A &=  \phi_h \left(\sum_{g\in G} \phi_{g} A \psi_g^{\dag}\right) &= \sum_{g\in G} \phi_{hg} A \psi_g^{\dag} &= \sum_{k = hg \hspace{1mm} \in G} \phi_k A \psi_{h^{-1}k}^{\dag} &= \left(\sum_{g\in G} \phi_{k} A \psi_k^{\dag} \right)\psi_h \\ &= M_A\psi_h 
	\end{aligned}
\end{equation}

By Schur's Theorem, $M_A$ is either the zero matrix with $\phi \neq \psi$ or $M_A$ is invertible with $\phi=\psi$. 

Suppose that $M_A = 0$. Then we can recover entries in the right hand side of our equation by observing the following chain of equations:
\begin{equation}
	\begin{aligned}
		    0 &= \left[M_A\right]_{il} \\ 
			&= \left[\sum_{g\in G} \phi_{g} A \psi_g^{\dag}\right]_{il} \\
			&= \sum_{g\in G} \left[\phi_{g} A \psi_g^{\dag}\right]_{il} \\
			&= \sum_{g\in G} \sum_{x=1}^{n_\phi} \sum_{y=1}^{n_\psi}\left[\phi_{g}\right]_{ix} \left[A\right]_{xy} \left[\psi_g^{\dag}\right]_{yl} \\
			&= \sum_{g\in G} \left[\phi_{g}\right]_{i1} \left[A\right]_{11} \left[\psi_g^{\dag}\right]_{1l} + \left[\phi_{g}\right]_{i1} \left[A\right]_{12} \left[\psi_g^{\dag}\right]_{2l} + \hdots + \left[\phi_{g}\right]_{i2} \left[A\right]_{21} \left[\psi_g^{\dag}\right]_{1l} + \hdots
	\end{aligned}
\end{equation}

We can see that this summand contains $n_\phi n_\psi$ terms where $[A]_{xy}$ are arbitrary entries. This means that we can choose $A$ specifically to recover relationships between matrix entries in $\phi_g$, $\psi_g$ and the left hand side of our chain of equations. That is, for a fixed $j,k$, choose $$[A]_{xy} = \begin{cases}
												1 & \text{if }x=j\text{ and } y=k \\
												0 & \text{else}
\end{cases}$$
To uncover that for any combination of $i,j,k,l,$
\begin{equation}
	\begin{aligned}
		\sum_{g\in G} \left[\phi_g\right]_{ij}\left[\psi^\dag_g\right]_{kl} = 0
	\end{aligned}
\end{equation}

Therefore, $\sum_{g\in G}\phi_{g}\psi_g^{\dag}=0$ \\

Suppose that $M_A$ is invertible and $\phi=\psi$. It must be the case then that $M_A = \lambda I_{n_\phi}$ where $\lambda \in \C$ following from Schur's Theorem. Choosing $A$ to be defined as it was in the previous case, we can see that for any $i,j,k,l,$
\begin{equation}
	\begin{aligned}
		\sum_{g\in G} \left[\phi_g\right]_{ij}\left[\phi^\dag_g\right]_{kl} = \begin{cases}
																			\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
	\end{aligned}
\end{equation}

The value of $\lambda$ will be shown to be dependent on the relationship between $j$ and $k$. If $j\neq k$, we can sum over every possible value for $i$ and then interepret this process as the standard inner-product (in $\C^{n_\phi}$) between two columns of a unitary matrix.

\begin{equation}
	\begin{aligned}
		n_\phi \lambda &= \sum_{g\in G} \sum_{i=1}^{n_\phi}\left[\phi_g\right]_{ij}\overline{\left[\phi_g\right]_{ik}} &= \sum_{g\in G} 0 &= 0
	\end{aligned}
\end{equation}


Therefore, whenever $j\neq k$, $\lambda = 0$. However, in the case that $j=k$, we can considering the fact that there are $n_\phi$ ways to choose $j$ and $k$ such that $j=k$. Summing over all these choices gives us a way to solve for $\lambda$:

$$\begin{aligned}
			\sum_{g\in G} \sum_{x=1}^{n_\phi}\left[\phi_g\right]_{ix}\left[\phi^\dag_g\right]_{xl} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			\sum_{g\in G} \left[\phi_g\phi^\dag_g\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			\sum_{g\in G} \left[I_{n_\phi}\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			|G|\left[I_{n_\phi}\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\end{aligned}$$ 

Therefore, whenever $i=l$, $j=k$, and $\phi=\psi$, $\lambda = \frac{|G|}{n_\phi}$. \qedsymbol \\



\begin{corrolary}
	The number of inequivalent, unitary, irreducible representations is bounded in the following way:
$$\sum_{\phi \in \Phi}n^2_\phi \leq |G|$$
where $\Phi = \{\phi \mid \phi$ is a unitary, irreducible representation of $G\}$
\end{corrolary}

\noindent (Pf.) Consider the entry of each matrix of a given representation $\phi$ for every $g \in G$ as denoted $[\phi_g]_{ij}$. We can create vectors in a $|G|$-dimensional vector space in the following way:
$$\sqrt{\frac{n_\phi}{|G|}}\left([\phi_{g_1}]_{ij},\hspace{1mm} [\phi_{g_2}]_{ij}, \hspace{1mm}\hdots \hspace{1mm},\hspace{1mm} [\phi_{g_{|G|}}]_{ij}\right)$$
where each choice of $i$ and $j$ gives us a new vector. Since there are $n_\phi$ choices for both $i$ and $j$, it follows that for a fixed $\phi$, there are $n_\phi^2$ choices of vectors in this space. Thus, we can make $\phi$ arbitrarily to see that the number of vectors that can be formed in this way is identically $\sum_{\phi \in \Phi}n^2_\phi$. The orthonormality condition gives us the pairwise orthogonality of these vectors in $\phi$, $i$, and $j$. Since orthogonal vectors are linearly independent, and the number of linearly independent vectors in a vector space must be at most as large as its dimension, we conclude that $\sum_{\phi \in \Phi}n^2_\phi \leq |G|$. \cite{Tung} \qedsymbol\\


This conclusion is incredibly important because it tells us that the task of finding \textit{every} unitary, irreducible representation is possible to accomplish. In this way, we can not only characterize representations in terms of their unitary, irreducible counterparts, but we can do so for all representations with a finite number of special components. We further develop this idea with the next theorem.


\section{Irreducible Characters and the Regular Representation}

The process of fully characterizing representations in terms of irreducible components will ultimately come to analyzing and comparing characters. Due to the nature of the trace operation, analyzing characters of representation can lead to more powerful conclusions. The main two reasons come down to the matrix-similarity-preserving property of the trace operation. Firstly, representations need not be unitary, and as a result, any conclusion about an irreducible representation will be basis-independent (in construction of matrix or operator). Secondly, we can utilize the structure of the given group to identify how the character function takes values on multiple group elements simultaneously. 

\begin{definition}
	Let $G$ be a group. Two elements, $g,\hspace{1mm}h \in G$ are said to be \textbf{conjugate} to one another if $\exists k \in G$ such that $g = khk^{-1}$.
\end{definition}

Conjugacy is an equivalence relation, we we can use this relation to partition our group into different subsets.

\begin{definition}
	Let $G$ be a group and let $g \in G$. The \textbf{conjugacy class} of $g$ is defined in the following way:
$$C_g = \{h\in G \mid g = khk^{-1} \text{ for some } k \in G \}$$
\end{definition}

Conjugacy classes are especially relevant in the contexts of representation theory because the character operation on representation is invariant on conjugacy classes.

\begin{theorem}
	Let $G$ be a group, $\phi$ be a representation of $G$, and $g\in G$. Then 
$$\chi^\phi(g) = \chi^\phi(h) \hspace{1mm}\forall h \in C_g$$
For future reference, we will denote $\chi^\phi_{C_g}$ to be the character of the representation $\phi$ on the conjugacy class $C_g$
\end{theorem}
\noindent (Pf.) Since $h\in C_g$, $\exists k\in G$ such that $g = khk^{-1}$. So 

\begin{equation}
	\begin{aligned}
		\chi^\phi(g) &= trace(\phi(g)) \\
					&= trace(\phi(khk^{-1}))\\ 
					&= trace(\phi(k)\phi(h)\phi(k^{-1})) \\
					&= trace(\phi(k^{-1})\phi(k)\phi(h)) \\
					&= trace(\phi(h)) \\
					&= \chi^\phi(h) \hspace{1mm}\cite{Mendes}\hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

We can use the character of representations to develop a similar set of condition as we have developed on representations themselves to give us a little more information about while relaxing the condition of unitarity.

\begin{theorem}
	Let $G$ be a group and let $\phi$ be a representation of $G$. Let $C$ be some conjugacy class of $G$. Then 
$$\sum_{g\in C} \phi(g) =\frac{|C|}{n_\phi} \chi^\phi_C I_{n_\phi}$$
where $\chi^\phi_C$ is the character of $\phi$ on the conjugacy class $C$.
\end{theorem}
\noindent (Pf.) Let $A\coloneq \sum_{g\in C} \phi(g)$. It is easy to see that for any $h\in G$, $\phi(h)A\phi(h)^{-1} = A$ due to the nature of conjugacy classes. Therefore, by \textbf{Schur's Theorem}, $A = \lambda I_{n_\phi}$ for some $\lambda \in \C$. Taking the trace of both sides of this equation, we get 
\begin{equation}
	\begin{aligned}
		|C| * \chi_C^\phi = n_\phi * \lambda \hspace{1mm} \cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

With this theorem, we are armed to establish the orthonormality and completeness relation for characters of representations.

\begin{theorem}
	Let $G$ be a group, let $C_G$ be the set of all distinct conjugacy classes of $G$. Let $\Phi$ be the set of all inequivalent irreducible representations of $G$ and let $\phi,\psi\in\Phi$. Then the following equation is defined to be the \textbf{orthoronomality relation of irreducible characters of representations}.

$$\sum_{C \in C_G} \frac{|C|}{|G|} \chi^{\phi}_C \overline{\chi^{\psi}_C } = \begin{cases}
																1 & if \hspace{1mm} \phi = \psi \\
																0 & else
															\end{cases}$$
\end{theorem}
\noindent (Pf.) Following from \textbf{Theorem 1.29}, we have 
$$\frac{n_\phi}{|G|} \sum_{g\in G} \left[\phi_g \right]_{ij} \left[\psi_g^\dag \right]_{kl} = \begin{cases}
																						1 & \text{if } \psi = \phi,\hspace{1mm} j=k,\hspace{1mm} \text{and } i=l\\
																						0 & else
																					 \end{cases}$$
Fixing $i=j$, $k=l$, and summing over $i$ and $k$ respectively, we obtain the character of each representation in the following equality:

\begin{equation}
	\begin{aligned}
		\frac{n_\phi}{|G|} \sum_{g\in G} \chi^\phi(g) \overline{\chi^\psi}(g) = \begin{cases}
																						n_\phi & \text{if } \psi = \phi \\
																						0 & else
																					 \end{cases}
	\end{aligned}
\end{equation}
Observing the fact that the character operation is constant on conjugacy class, we can alter this equation to take the following equivalent form:

\begin{equation}
	\begin{aligned}
		\frac{1}{|G|} \sum_{C\in C_G} |C| \chi^\phi_C \overline{\chi^\psi_C}= \begin{cases}
																						1 & \text{if } \psi = \phi \\
																						0 & else
																					 \end{cases}
	\end{aligned}
\end{equation}
giving the desired conclusion. \cite{Tung} \qedsymbol

Now that we have established some valuable results about irreducible characters, we can establish one of the main results. It turns out that we can compare irreducible characters to the character of a generic representation to determine the number of copies of a given irreducible character in its irreducible decomposition. In other words, we can recover the coeffeicients ($\alpha_i$) from \textbf{Theorem 1.18}. In order to establish this, we first need the following useful theorems.

\begin{theorem}
	Let $M = \bigoplus_{i=1}^k A_i$ where $A_i$ is a square matrix for every $i$. Then $$trace(M) = \sum_{i=1}^k trace(A_i)$$
\end{theorem} 

\noindent (Pf.) $$M = \begin{bmatrix}
						A_1 & 0 & 0 & \hdots & 0 \\
						0 & A_2 & 0 & \hdots & 0 \\
						\vdots & \ddots & \ddots & \ddots & \vdots \\
						0 & 0 & 0 & \hdots & A_k
					\end{bmatrix}$$

$trace(M)$ is the sum of the diagonal entries in $M$, and since $M$ is a block-diagonal matrix (with blocks corresponding to $A_i$) the diagonal entries of $M$ coincide with the diagonal entries in $A_i$ for every $i$.

\begin{equation}
	\begin{aligned}
		trace(M) &= \sum_i [A_1]_{ii} + \sum_i [A_2]_{ii} + \hdots \sum_i [A_k]_{ii} &= \sum_{i=1}^k trace(A_i) \hspace{1mm}\qedsymbol
	\end{aligned}
\end{equation}

\begin{theorem}
	Let $G$ be a group, let $\phi$ be a representation of $G$, and let the irreducible decomposition of $\phi$ be denoted $\phi = T \left( \bigoplus_{i=1}^k \alpha_i *\psi_i\right) T^{-1}$ where $\{\psi_i\}_{i=1}^k$ is a set of irreducible representations of $G$. Then $$\alpha_i = \sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^{\psi_i}_C}$$
\end{theorem}

\noindent (Pf.) First, note that using \textbf{Theorem 1.36}
\begin{equation}
	\begin{aligned}
		\chi^\phi_C = \chi^{ \bigoplus_{i=1}^k \alpha_i *\psi_i}_C = \sum_{i=1}^k \alpha_i \chi^{\psi_i}_C
	\end{aligned}
\end{equation}

So, using the Orthonormality Condition of Irreducible Characters, we get 

\begin{equation}
	\begin{aligned}
\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^{\psi_i}_C} &= \sum_{C\in C_G} \frac{|C|}{|G|} \sum_{j=1}^k \alpha_j \chi^{\psi_j}_C \overline{\chi^{\psi_i}_C} &= \sum_{j=1}^k \alpha_j \sum_{C\in C_G} \frac{|C|}{|G|} \chi^{\psi_j}_C \overline{\chi^{\psi_i}_C}  &= \alpha_i \hspace{1mm} \cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

\begin{theorem}
	Let $G$ be a group, let $\phi$ be a representation of $G$. $\phi$ is an irreducible representation of $G$ if and only if $\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^\phi_C} = 1$
\end{theorem}

\noindent (Pf.) $\Rightarrow$ Use \textbf{Theorem 1.35} for immediate conclusion. 

\hspace{2mm}$\Leftarrow$ Let $\bigoplus_{i=1}^k \alpha_i\psi_i$ be the irreducible decomposition of $\phi$. Then utilizing this decomposition on the character operation and the orthonormality condition, we get the following equation.

\begin{equation}
	\begin{aligned}
		1 &=\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^\phi_C} &= \sum_{i=1}^k \sum_{j=1}^k \alpha_i \overline{\alpha_j} \sum_{C\in C_G} \frac{|C|}{|G|} \chi^{\psi_i}_C \overline{\chi^{\psi_j}_C} &= \sum_{i=1}^k |\alpha_i|^2
	\end{aligned}
\end{equation}

Since the sum of square integers is equal to $1$, it must be the case that $\alpha_m = 1$ for some $m$ and $\alpha_n=0$  $\forall n \neq m$. \cite{Tung} \qedsymbol


The last two theorems give us tools to determine when representations are irreducible and determine how many copies of a given irreducible representation are present in the irreducible decomposition of a generic representation. Armed with these techniques, we can analyze a new representation.

\begin{definition}
	Let $G=\{g_i\}^n_{i=1}$ be a finite group. We define the \textbf{Left Regular Representation} to be the matrix representation given by the following formula:
$$g\stackrel{\phi}{\mapsto} [\phi_g]_{ij} = \begin{cases}
								1 & \text{if } gg_j = g_i \\
								0 & \text{else}
							\end{cases}$$
\end{definition}

One could verify that this map is a representation through explicitly showing the multiplication rule in $G$ is satisfied by the matrices in $\phi(G)$. Due to the construction of our matrices, this representation has degree $|G|$. 

\begin{theorem}
	For any finite group, $G$, the left regular representation ($\phi$) contains every irreducible representation of $G$, $\{\psi_i\}_{i=1}^k$, exactly $n_{\psi_i}$ times.
\end{theorem}

\noindent(Pf.) Let $n=|G|$. Consider the matrices in given by the representation $\phi$. We know that $\phi(e) = I_n$, but what can we say about can we say about non-identity elements of $G$? It turns out, for any $g\in G$ such that $g\neq e$, $\phi_g$ always has a diagonal full of zeros. This must be the case, since it would be impossible for $g *g_i = g_i$  for any non-identity $g$. As a result, we see that $\chi^\phi_e = n$ and $\chi^\phi_g = 0$ $\forall g \in G$ with $g \neq e$. So, for any irreducible representation, $\psi_i$, \textbf{Theorem 1.37} tells us that 
\begin{equation}
	\begin{aligned}
		\alpha_i = \sum_{C\in C_G} \frac{|C|}{|G|} \chi^\phi_C \overline{\chi_C^{\psi_i}} = \frac{1}{|G|}|G| \overline{\chi_e^{\psi_i}} = n_{\psi_i} \hspace{1mm}\cite{Tung} \hspace{1mm}\qedsymbol
	\end{aligned}
\end{equation}

\begin{corrolary}
\	For any finite group, $G$, $\sum_{\phi \in \Phi} n_\phi^2 = |G|$ where $\Phi=\{\phi \mid \phi \text{ is a distinct, irreducible representation of }G\}$.
\end{corrolary}

\noindent(Pf.) Since the left regular representation ($\phi$) has irreducible decomposition $\phi = T\left(\bigoplus_{i=1}^k n_{\psi_i}*\psi_i\right)T^{-1}$, we can use \textbf{Theorem 1.36} to show that
\begin{equation}
	\begin{aligned}
		|G| = trace(\phi(e)) = trace\left(\bigoplus_{i=1}^k n_{\psi_i}*\psi_i(e)\right) = \sum_{i=1}^k n_{\psi_i}trace(\psi_i(e)) = \sum_{i=1}^k n_{\psi_i}^2 \hspace{1mm}\cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

\section{Completeness Conditions for Irreducible Representations and Characters}


\begin{theorem}
	Let $G$ be a finite group, let $\Phi = \{\phi \mid \phi$ is a unitary, irreducible representation of $G\}$, and let $n_\phi$ denote the degree of the representation $\phi$. Then for any pair $g$, $h \in G$, 
$$\sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}$$
This is referred to as the \textbf{completeness condition} of unitary, irreducible representations.
\end{theorem}

\noindent (Pf.) Referencing the construction of a $|G|$-dimensional vector space, $V$, in \textbf{Corrolary 1.30}, we can create a set of orthonormal, linearly independent vectors. Thus, for any fixed $\phi$, $i$, $j$, the corresponding orthonormal vector is defined by:

\begin{equation}
	\begin{aligned}
		e_{\phi,i,j} \coloneq \sqrt{\frac{n_\phi}{|G|}}\left([\phi_{g_1}]_{ij},\hspace{1mm} [\phi_{g_2}]_{ij}, \hspace{1mm}\hdots \hspace{1mm},\hspace{1mm} [\phi_{g_{|G|}}]_{ij}\right)
	\end{aligned}
\end{equation}

As a result of our work in \textbf{Corrolary 1.41}, we can see the set $\{e_{\phi,i,j}\}_{\phi,i,j}$ is in fact an orthonormal basis of our vector space, given that the dimension of our vector space is now identically 
$$n \coloneq |G| = \sum_{\phi\in \Phi} n_\phi^2$$

Therefore, every $v\in V$ has the following orthogonal decomposition:
\begin{equation}
	\begin{aligned}
		v &= \sum_{\phi,i,j} \langle v , e_{\phi,i,j} \rangle e_{\phi,i,j} &= \left(\sum_{\phi,i,j} e_{\phi,i,j} \otimes e_{\phi,i,j}\right) v
	\end{aligned}
\end{equation}

where the tensor symbol is defined here as the outer-product of vectors in $V$, as realized in the following matrix:

\begin{equation}
	\begin{aligned}
		e_{\phi,i,j} \otimes e_{\phi,i,j} &= \frac{n_\phi}{|G|}\begin{bmatrix}
			[\phi_{g_1}]_{ij}\overline{[\phi_{g_1}]_{ij}} & \hdots & [\phi_{g_1}]_{ij}\overline{[\phi_{g_n}]_{ij}}\\
			\vdots & \ddots & \vdots\\
			[\phi_{g_n}]_{ij}\overline{[\phi_{g_1}]_{ij}} & \hdots & [\phi_{g_n}]_{ij}\overline{[\phi_{g_n}]_{ij}}
		\end{bmatrix}
	\end{aligned}
\end{equation}

Upon closer inspection of \textbf{Equation 1.29}, we can see that 

\begin{equation}
	\begin{aligned}
		\sum_{\phi,i,j} {|G|} \hspace{1mm}e_{\phi,i,j} \otimes e_{\phi,i,j} &= I_n
	\end{aligned}
\end{equation}

or equivalently, for any given $x$, $y \in \{1,\hdots,|G|\}$ 
\begin{equation}
	\begin{aligned}
		\sum_{\phi,i,j} \frac{n_\phi}{|G|} [\phi_{g_x}]_{ij}[\phi^\dag_{g_y}]_{ji} &= \sum_{\phi,i,j} \left[e_{\phi,i,j} \otimes e_{\phi,i,j}\right]_{xy} &= \left[\sum_{\phi,i,j} e_{\phi,i,j} \otimes e_{\phi,i,j}\right]_{xy} &= \begin{cases}
																	1 & \text{if } x=y\\
																	0 & \text{else}
																\end{cases}
	\end{aligned}
\end{equation}
which is exactly what our completeness relation is defined to be. \qedsymbol \\







\begin{theorem}
Let $G$ be a group, let $C_G$ be the set of all distinct conjugacy classes of $G$. Let $\Phi$ be the set of all inequivalent irreducible representations of $G$. Then for any $g,h \in G$, the following equation is defined to be the \textbf{completeness relation of irreducible characters of representations}.
$$\sum_{\phi \in \Phi} \frac{|C_g|}{|G|} \chi^{\phi}_{C_g} \overline{\chi^{\phi}_{C_h}} = \begin{cases}
																1 & if \hspace{1mm} C_g = C_h \\
																0 & else
															\end{cases}$$
\end{theorem}

\noindent (Pf.) Utilizing \textbf{Theorem 1.37}, we see that for any $g, h \in G$, $$\sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}$$
Taking this equality and summing both sides over the entire conjugacy class of $g$ and $h$ respectively, we aquire
\begin{equation}
	\begin{aligned}
		\sum_{g \in C_g} \sum_{h\in C_h} \sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \sum_{g \in C_g} \sum_{h\in C_h} \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}
Utilizing \textbf{Theorem 1.35} and analyizing the $ij-th$ component of both sides of its conclusion, we can equivalently write the above equality in the following way:
\begin{equation}
	\begin{aligned}
		 \sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{|C_g||C_h|}{|G|n_\phi}\chi^\phi_{C_g}\overline{\chi^\phi_{C_h}} \left[I_{n_\phi}\right]_{ij}\left[I_{n_\phi}\right]_{ji} = \begin{cases}
																										|C_h| & if \hspace{1mm} C_g = C_h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}

Interpreting the inner-most sum as a matrix product of both identity matrices and the second sum as spanning the diagonal of the resulting product matrix, we obtain

\begin{equation}
	\begin{aligned}
		 \sum_{\phi \in \Phi} \frac{|C_g|}{|G|}\chi^\phi_{C_g}\overline{\chi^\phi_{C_h}} = \begin{cases}
																										1 & if \hspace{1mm} C_g = C_h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}

to achieve our desired result. \cite{Tung} \qedsymbol


With our main results developed for unitary, irreducible representations and irreducible characters, we can use these ideas as motivation for our study of specific groups. While most of these results are only useful in the finite group setting, we will continue to develop equivalent conditions and conclusions for groups of infinite order. Through our analysis, we will characterize our selected groups using the techniques and tools established in this chapter as a guide.



\chapter{$SO(2)$: The Rotation Group in Two Dimensions}

The group $SO(2)$ is a very abstract structure meant to represent a very concrete physical idea. Elements of this group correspond to the action of rotating vectors in two-dimensional space about some central point. Although its construction can be more generalized, we can gleam the most important features of the group's structure are not altered in any meaningful way when care is exercise. That said, throughout this discussion, we will develop the structure of $SO2$ with the assumptions that rotations (elements) act on real-valued two-dimensional vectors who are all centered at the origin. The way we will distinguish rotations will be by the measure of the angle (traditionally denoted by $\theta$) rotated in the counter-clockwise direction. Consequently, $\theta$ will always be real-valued. Rotations will occur about the origin. Unless otherwise specified, we will always take $\{e_1,$ $e_2\}$ to be an orthonormal basis of $\R^2$.

\section{Construction and Properties}

To begin, consider $\R^2$ as a the natural two-dimensional vector space. Let $(x,y)\in\R^2$. Then, any rotation of this vector, by some angle $\theta$, in the manner described above will change the coordinates. If we refer to this new, rotated vector as $(x',y')$, we can visualize this action.

\begin{center}
            \begin{tikzpicture}[scale=1]
                \draw[step=1cm ,color=gray] (-4,-4) grid (4,4);
                \draw[thick,<->] (-4.1,0) -- (4.1,0) node[anchor=north west] {$e_1$};
                \draw[thick,<->] (0,-4.1) -- (0,4.1) node[anchor=south west] {$e_2$};
			\draw[fill=black] (0,0) coordinate (o);
			\draw[thick, ->] (0,0) -- (2,3) coordinate (a) node[anchor=south west] {$(x,y)$};
			\draw[thick, ->] (0,0) -- (-3,2) coordinate (b) node[anchor=south] {$(x',y')$};
			\pic [draw, ->, angle eccentricity=0.7, angle radius=0.8cm, "$\theta$"]{angle=a--o--b};
            \end{tikzpicture}
    \end{center}

For a more intuitive understanding of the action, we can turn our attention to the polar coordinates corresponding to these two vectors. Letting $r = \sqrt{x^2 + y^2}$, we can see that the polar form of these vectors can be written in the following way:
\begin{center}
	$\begin{aligned}
		(x,y) &\sim (r,\psi)\\
		(x',y') &\sim (r,\psi+\theta)
	\end{aligned}$
\end{center}
where $\psi$ is the angle that the vector $(x,y)$ makes with the positive $e_1$-axis. Through use of trigonometric identities, we can observe the following equations hold true:
\begin{equation}
	\begin{aligned}
		x' &= r\cos(\psi+\theta) = r\left(\cos(\psi)\cos(\theta) - \sin(\psi)\sin(\theta)\right) = \cos(\theta)*x -\sin(\theta)*y
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		y' &= r\sin(\psi+\theta) = r\left(\cos(\psi)\sin(\theta) + \sin(\psi)\cos(\theta)\right) = \sin(\theta)*x +\cos(\theta)*y
	\end{aligned}
\end{equation}

We can see that the rotated coordinates are completely determined by the originial coordinates and the angle of rotation. With the use of matrix algebra, we can combine \textbf{Equations 2.1 and 2.2} into 

\begin{equation}
	\begin{aligned}
		\begin{bmatrix}
			x' \\
			y'
		\end{bmatrix} &=
		\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}
		\begin{bmatrix}
			x \\
			y
		\end{bmatrix}
	\end{aligned}
\end{equation}

While this matrix algebra is useful, its construction is somewhat ad-hoc. Questions about the well-defined nature of this relationship are bound to be raised. In order for us to make real use of these matrices, we must first establish a bona-fide connection between the abstract notion of a rotation and the concrete matrix that acts on vectors in $\R^2$ in the same way.

We will define the group of rotations in two dimensions to be $G = \{g_\theta\}$ where $g_\theta$ represents the action of rotating vectors in $\R^2$ by angle $\theta$. It's main properties arise from physical observations, but we can give them axiomatic rigor. When rotating vectors in $\R^2$, we can see that the successive rotations by the angles $\theta_1$ and $\theta_2$ are identical to a singular rotation by the angle $\theta_1+\theta_2$. In this way, the group law must be defined in the following way: $g_{\theta_1} * g_{\theta_2} = g_{(\theta_1 + \theta_2)}$ for any $\theta_1,\theta_2\in\R$. Further, due to the abelian nature of $\R$, we can see that our group law will necessarily guarantee that $G$ is abelian:  $g_{\theta_1} * g_{\theta_2} = g_{(\theta_1 + \theta_2)}= g_{(\theta_2 + \theta_1)}= g_{\theta_2} * g_{\theta_1} \hspace{1mm}\forall \theta_1,\theta_2\in\R$. Finally, due to the physical symmetry of rotation in the path of a circle, it must be the case that $g_\theta = g_{\theta \pm 2\pi}$ for any $\theta \in \R$. The last property suggests that while $G$ is clearly an infinite group, the physical rotation corresponding to each of its elements is not unique. With these key ideas in mind we will establish the link between $G$ and the set of matrices defined in \textbf{Equation 2.3}.

\begin{theorem}
	Let $G/(g_{2\pi})$ be the quotient group defined by taking equivalence classes of physical rotations of vectors in $\R^2$ by regarding rotations that are off by integer multiples of $2\pi$ as equivalent. Let $H$ be the group defined by taking the set of all matrices of the form defined in \textbf{Equation 2.3}. together with the operation of matrix multiplication. Then the map $\phi$ (defined below) is an isomorphism of groups.
$$\phi:G/(g_{2\pi})\rightarrow H$$
$$[g_\theta] \overset{\phi}{\mapsto} \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}$$
\end{theorem}
\noindent(Pf.) 

(Homomorphism) Let $g_{\theta_1},g_{\theta_2},\in G$. Then 
\begin{equation}
	\begin{aligned}
		\phi([g_{\theta_1+\theta_2]}) &= \begin{bmatrix}
			\cos(\theta_1+\theta_2) & -\sin(\theta_1+\theta_2) \\
			\sin(\theta_1+\theta_2) & \cos(\theta_1+\theta_2)
		\end{bmatrix} \\
		&\overset{*}{=} \begin{bmatrix}
			\cos(\theta_1) & -\sin(\theta_1) \\
			\sin(\theta_1) & \cos(\theta_1)
		\end{bmatrix}
		\begin{bmatrix}
			\cos(\theta_2) & -\sin(\theta_2) \\
			\sin(\theta_2) & \cos(\theta_2)
		\end{bmatrix}\\
		&= \phi([g_{\theta_1}])\phi([g_{\theta_2}])
	\end{aligned}
\end{equation}
where a more detailed expansion for the starred($*$) equality can be found in \textbf{Example 1.4}.

(Injective) Suppose $\phi([g_\theta]) = I_2$. Then $\cos(\theta) = 1$ and $\sin(\theta) = 0$. This means that $\theta = 2\pi n$ for $n\in\Z$. Therefore, $ker(\phi) = \{[0]\}$. Note: if we instead used the group of rotations itself as our domain, we would not have a trivial kernel, since all interger multiples of $2\pi$ would have rotations that get mapped to $I_2$. For one-to-one correspondence to occur, we need to focus our attention on the quotient group (which is physically acceptable).

(Surjective) Let $\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}\in H$. Then clearly, $\theta$ is real-valued and $g_\theta\in G$ is a well-defined rotation. Then $[g_\theta]$ is the coset of $g_\theta$ and as a result, $[g_\theta] \overset{\phi}{\mapsto} \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}$. Therefore, $im(\phi) = H$.

Thus, $\phi$ is an isomorphism of groups. \qedsymbol\\


Through this interpretation, we are justified in viewing any rotation, by angle $\theta$, as the result of a matrix multiplication. Although our map is defined on the quotient group, we can see that we lose no information for angle values outside of the range $[0,2\pi)$. That is, we can always use the equivalence of rotations in $G$ that are off by $\pm 2\pi n$ to do our algebra exclusively in the $[0,2\pi)$ range. We will refer to the matrices in $H$ as $R(\theta)$, or $R_\theta$ interchangeably, $\theta$ is the angle we rotate vectors of $\R^2$ by. 

Due to the isomophic nature of $G/(g_{2\pi})$ and $H$, $H$ inherets most of the properties of $G$. It turns out that $H$ is abelian and periodic in $\theta$ (the latter was already known by using properties of trigonometric functions). We can observe some other interesting properties about $H$ through the lens of $G$.

\begin{definition}
	Let $M$ be an $n\times n$ matrix. $M$ is said to be an \textbf{orthogonal matrix} if $$MM^\intercal=I_n$$
\end{definition}

\begin{theorem}
	For any $\theta$, $R(\theta)$ is an orthogonal matrix.
\end{theorem}
\noindent(Pf.) It would not be difficult to use a matrix product argument. However, in the spirit of the isomorphism, we will use $\phi$ from \textbf{Theorem 2.1} to articulate the argument. It is clear that for any $g_\theta \in G$, $g_\theta^{-1} = g_{-\theta}$. Therefore, $[g_\theta]^{-1} = [g_{-\theta}]$. So, for any $\theta$, 
\begin{equation}
	\begin{aligned}
		I_2 &= \phi([g_\theta][g_\theta]^{-1})\\
		 &= \phi([g_\theta][g_{-\theta}]) \\ 
		 &= \phi([g_\theta])\phi([g_{-\theta}])\\
		 &= \begin{bmatrix}
			\cos(\theta) & \sin(\theta) \\
			-\sin(\theta) & \cos(\theta)
		\end{bmatrix}
\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}\\
		 &= R(\theta)R(\theta)^\intercal
	\end{aligned}
\end{equation}
where the computation of $\phi([g_{-\theta}])$ uses the facts that cosine is an even function and sine is an odd function. Therefore, $R(\theta)^{-1} = R(\theta)^\intercal$. \qedsymbol

While not explicitly stated previously, rotations are practically understood to be an action that preserves a vector's magnitude. More rigorously put, if $(x,y)$ is rotated to the vector $(x',y')$, $x^2+y^2 = x^{\prime^2}+y^{\prime^2}$. The matrices in $H$ also satisfy this property. After a vector in $\R^2$ is multiplied by a matrix in $H$, its magnitude is preserved. This property is both a result of the orthogonal nature of these matrices and another property which we will explore below.

\begin{definition}
	Let $M$ be an $n\times n$ matrix. $M$ is said to be \textbf{special} if $$\det(M)=1$$
\end{definition}

\begin{theorem}
	For any $\theta$, $R(\theta)$ is a special matrix.
\end{theorem}
\noindent(Pf.)
\begin{equation}
	\begin{aligned}
		\det(R(\theta)) &= \cos^2(\theta) - (-\sin^2(\theta)) &= 1 \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

Given that elements of the group $H$ are special, orthogonal matrices that act as rotations on $\R^2$, it seems fitting that we give this group a more explicit name. We call this group $SO(2)$, and we will henceforth treat its elements as the rotations that we discussed in the construction of our group $G$. $SO(2)$ is our two-dimensional rotation group.

Since $SO(2)$ is clearly an infinite group, it would be incredibly helpful to determine its generators (if any exist). As it turns out, $SO(2)$ has more structure than we have already covered which will be useful to determining if a generator exists.

\begin{definition}
	A \textbf{Lie Group} is a group that is a finite-dimensional smooth manifold in which the operations of multiplication and inversion are differentiable.
\end{definition}

It turns out that $SO(2)$ is actually a Lie Group. This comes from the idea that as we smoothly vary the parameter for $\theta$, the matrices $R(\theta)$ also vary smoothly (arising from the differentiability of the sine and cosine). If there were to be a generator for this group, it would have to be a generator that represents an arbitrarily small rotation.

Consider then a rotation by angle $d\theta$ (arbitrarily small). The element in $SO(2)$ corresponding to this rotation would need to differ from the identity matrix by an arbitrarily small amount. In other words:

\begin{equation}
	\begin{aligned}
		R(d\theta) \coloneq I_2 + (-i)d\theta * J
	\end{aligned}
\end{equation}  

where $J$ is some fixed matrix, the factor of $-i$ is given as a convention, and the factor of $d\phi$ forces the change to $I_2$ to be arbitrarily small. It will turn out that $J$ will be the generator of our group. With this in mind, for any $\theta$, we can make the following observations:

\begin{equation}
	\begin{aligned}
		R(\theta + d\theta) = R(\theta)R(d\theta) = R(\theta)\left(I_2 + (-i)d\phi * J\right) = R(\theta) - id\theta R(\theta)J
	\end{aligned}
\end{equation}  
\begin{equation}
	\begin{aligned}
		R(\theta + d\theta) = R(\theta) + \frac{d}{d\theta}R(\theta)d\theta
	\end{aligned}
\end{equation}  

Combining these two equations, we uncover the differential equation:

\begin{equation}
	\begin{aligned}
		\frac{d}{d\theta}R(\theta) = - i R(\theta)J
	\end{aligned}
\end{equation}  

This differential equation can be converted into an initial value problem using the following condition: $R(0) = I_2$. This initial value problem has the solution

\begin{equation}
	\begin{aligned}
		R(\theta) = e^{-i\theta J} = \sum_{i=0}^\infty \frac{-i\theta J}{n!}
	\end{aligned}
\end{equation}  

where we interpret our solution in the following way:

\begin{equation}
	\begin{aligned}
		e^{-i\theta J} = \sum_{i=0}^\infty \frac{\left(-i\theta J\right)^n}{n!} = I_2 -i\theta J - \frac{\theta^2}{2} J^2 + \frac{i\theta^3}{3!}J^3 \hdots 
	\end{aligned}
\end{equation}  

Given this formulation, we say that $J$ is the generator of $SO(2)$, since any rotation matrix can be written in terms of this exponential. We can explicitly compute this matrix given its relationship outlined in \textbf{Equation 2.7}. Letting $J = [j]_{ij}$,

\begin{equation}
	\begin{aligned}
		\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix} + (-i)d\theta * \begin{bmatrix}
			j_{11} & j_{12} \\
			j_{21} & j_{22}
		\end{bmatrix} = R(d\theta) = \begin{bmatrix}
			\cos(d\theta) & -\sin(d\theta) \\
			\sin(d\theta) & \cos(d\theta)
		\end{bmatrix} = 
		\begin{bmatrix}
			1 & -d\theta \\
			d\theta & 1
		\end{bmatrix}
	\end{aligned}
\end{equation}

When comparing both sides entry by entry, we see that 

\begin{equation}
	\begin{aligned}
		J = \begin{bmatrix}
			0 & -i \\
			i & 0
		\end{bmatrix}
	\end{aligned}
\end{equation}

This matrix turns out to have the property that its square is the identity, making expansion in \textbf{Equation 2.12} much simpler.

\begin{equation}
	\begin{aligned}
		R(\theta) = \hdots = I_2 -i\theta J - \frac{\theta^2}{2} I_2 + \frac{i\theta^3}{3!}J \hdots = I_2 \cos(\theta) -iJ \sin(\theta) 
	\end{aligned}
\end{equation}

In this fashion, we can express any rotation matrix in $SO(2)$ in terms of the generator. \\

While this construction for rotation matrices is satisfactory, we can recast these concepts when considering these matrices as their corresponding linear operators. Letting $U_\theta$ be the linear operator corresponding to the matrix $R(\theta)$, we construct a generator for the group of rotation operators with the following assertion

\begin{equation}
	\begin{aligned}
		U_{d\theta} \coloneq I + (-i)d\theta * J
	\end{aligned}
\end{equation}  

where $J$ can be shown to be the generator of the group of operators. Tracing the steps, we see that 

\begin{equation}
	\begin{aligned}
		U_\theta = e^{-i\theta J} = \sum_{i=0}^\infty \frac{-i\theta J}{n!}
	\end{aligned}
\end{equation}  

%Unfortunately, since we do not have an explicit formula (or do we? J(v1,v2) = (iv1,-iv2) ? Ask Prof.)

We will use this formulation extensively in the next section.


\section{Irreducible Representations for $SO(2)$}

In the previous section, we illustrated many properties of $SO(2)$, including the fact that is is abelian. According to \textbf{Corrolary 1.24}, this means that all irreducible representations must be degree one. Recalling that in order for a representation to be irreducible, the only invariant subspaces must be trivial, we naturally turn our attention to eigenvalues of our generator operator. Let $v$ be an eigenvector of $J$ corresponding to eigenvalue $\lambda$. Then for any rotation operator, $U_\theta$,


\begin{equation}
	\begin{aligned}
		U_\theta (v) = \left(\sum_{i=0}^\infty \frac{-i\theta J}{n!}\right) (v) = \left(\sum_{i=0}^\infty \frac{-i\theta \lambda}{n!}\right)v = \left(e^{-i\theta \lambda}\right)v
	\end{aligned}
\end{equation}

Therefore, $v$ is an eigenvector of every rotation operator corresponding to eigenvalue $e^{-i\theta \lambda}$. Eigenvalues serve as a great tool for constructing degree one representations. Throughout the rest of this thesis, we will always start with this as our baseline.

It can clearly be seen that for any two rotation operators, $U_{\theta_1}$ and $U_{\theta_2}$,  
\begin{equation}
	\begin{aligned}
		U_{\theta_1}(U_{\theta_2}(v)) = U_{\theta_1}(e^{-i\theta_2 \lambda}v) = e^{-i\theta_1 \lambda}e^{-i\theta_2 \lambda}v = e^{-i(\theta_1 + \theta_2) \lambda}v = U_{\theta_1+\theta_2}(v) 
	\end{aligned}
\end{equation}

the group operation of composition (and therefore, corresponding matrix multiplication) respect our eigenvalues. But in order for the representation to be a true homomorphism, we need $2\pi n \mapsto 1$ for any $n\in\Z$. This quickly forces a restriction on $\lambda$ in the following way:

\begin{equation}
	\begin{aligned}
		e^{-i2\pi n\lambda} = 1 \Rightarrow \lambda \in \Z
	\end{aligned}
\end{equation}

With this restriction in mind, we can define an irredcible representation for $SO(2)$ with each choice of integer in the following way:

$$\begin{aligned}
	\phi_m:SO(2)\rightarrow \C \\
	U_\theta \mapsto e^{-i\theta m} \\
\end{aligned}$$

We can also show that these representations are unitary. Consider a one dimensional inner-product space one which the operators/matrices $\phi_m$ are defined upon. Letting $x,y\in V$, consider the following argument:

\begin{equation}
	\begin{aligned}
		\langle \phi_m(x) , \phi_m(y) \rangle &= \langle e^{-i\theta m}*x , e^{-i\theta m}*y \rangle \\
												&= e^{-i\theta m}*\overline{e^{-i\theta m}} \langle x , y \rangle\\
												&=\langle x , y \rangle
	\end{aligned}
\end{equation}

Therefore, $\phi_m$ is unitary for every $m\in\Z$. \\

Now that we have our unitary, irreducible representations, we turn our attention to casting these representations into our orthonormality and completeness conditions. However, those conditions were defined over finite groups, and the sums that characterize those conditions are not well-defined. Further, elements of $SO(2)$ are characterized by a continuous variable, making the number of elements uncountable. Any kind of natural generalization of these condition would need to utilize integration in some meaningful way. However, as of yet, integration is not well-defined either, since we have done all of our work in a group theory setting.

We must exercise care when constructing our integration. When setting up the integration, we will do so in the context of a continuous parameterization of the group elements. However, we want to ensure that the value of the integral is the same for any parameterization we choose. That is to say, for any representation, $f$, and two nonequivalent parameterizations ($\theta$ and $\psi$) of the interval $[0,2\pi]$, we should have the following:

\begin{equation}
	\begin{aligned}
		\int_G f(g) dg = \int_0^{2\pi}f(g)\rho_{g}(\theta)  d\theta
	\end{aligned}
\end{equation}

where $\rho_g(\theta)$ is a weight function. Therefore, we can think of $dg$ as being equivalent to $\rho_g(\theta)  d\theta$. More explicitly, any integration should respect the group multiplication law. That is, for any fixed $h\in G$, we should have

\begin{equation}
	\begin{aligned}
		\int_G f(g) dg = \int_G f(g) d(hg)
	\end{aligned}
\end{equation}

To this end, we will construction "group integration" in the following way:

\begin{definition}
	Let $g_\theta$ parameterize group elements in $G$ and let $\rho_{g}$ be a weight function corresponding to elements of this group. This parametrization of the group and weight function are said to provide an \textbf{invariant integration measure} if for any fixed $h\in G$, $$dg = d(hg)$$ or more explicitly, $$\rho_{g}[d\theta]_g = \rho_{hg}[d\theta]_{hg}$$
\end{definition}

If we choose the weight function to that satisfied the following equation

\begin{equation}
	\begin{aligned}
		\rho_{g}(\theta) = \frac{[d\theta]_e}{[d\theta]_g}
	\end{aligned}
\end{equation}
 
then we can see that the above condition is quickly satisfied by the following trick:


\begin{equation}
	\begin{aligned}
		\rho_g =  \frac{[d\theta]_e}{[d\theta]_g} =  \frac{[d\theta]_e}{[d\theta]_{hg}}\frac{[d\theta]_{hg}}{[d\theta]_g} = \rho_{hg}\frac{[d\theta]_{hg}}{[d\theta]_g}
	\end{aligned}
\end{equation}

If we take $\theta$ to be the parameterization of $[0,2\pi)$ that gives us the exact value of the rotation angle, then the relationship 

\textbf{Come back and fix this utter nonsense}\\

Now that we have well defined integration, it is sensible to discuss the orthonormality and completeness relations for our irreducible (unitary) representations of $SO(2)$.

\begin{theorem}
	The irreducible (unitary) representaions of $SO(2)$ satisfy the following conditions:
\begin{equation}
	\begin{aligned}
		\frac{1}{2\pi} \int_0^{2\pi} \phi_m(\theta) \phi_n^\dag(\theta) d\theta= \begin{cases}
																			1 & \text{if } m=n \\
																			0 & \text{else}
																		\end{cases}\hspace{2mm} \text{Orthonormality Relation}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		\sum_{n\in\Z} \phi_n(\theta) \phi_n^\dag(\theta') = \begin{cases}
																			1 & \text{if } \phi=\phi' \\
																			0 & \text{else}
																		\end{cases} \hspace{2mm} \text{Completeness Relation}
	\end{aligned}
\end{equation}
\end{theorem}

We can see that the orthonormality condition is clearly true since $\{e^{im\theta}\}_{m\in\Z}$ is an orthogonal set in $C([0,2\pi])$ with each function having norm $2\pi$. The completeness condition \textbf{looks very wrong and should be double checked with professor.} \\


Are these the only irreducible representations of $SO(2)$? So far, we have defined our irreducible representations in terms of integer values. What would happen if we were to replace these integers with other numbers? For example, consider the map $\phi_{\frac{1}{2}}: U_\theta \mapsto e^\frac{i\theta}{2}$. Unfortunately, the physical implications of this map would be problematic, seeing as $\phi_\frac{1}{2}(\theta + 2\pi) \neq  \phi_\frac{1}{2}(\theta)$. However, this mapping is still periodic, with period $4\pi$. If we relax our definition of a representation, this mapping can still be of use to us.

\begin{definition}
	A \textbf{mult-valued rrepresentation} is a multi-valued mapping of a group into $GL_n(\C)$ which is a group homomorphism in the sense that at least one of the outputs (from each input) may be used to satisfy the relation.
\end{definition}

In this sense, $\phi_{\frac{1}{2}}$ is a $2$-valued representation since $\phi_{\frac{1}{2}}(U_\theta) = e^\frac{\pm i\theta}{2}$ and $\phi_{\frac{1}{2}}(U_\theta U_\psi) =  e^\frac{\pm i(\theta + \psi)}{2} =  e^\frac{\pm i\theta}{2} e^\frac{\pm i\psi}{2} = \phi_{\frac{1}{2}}(U_\theta)\phi_{\frac{1}{2}}(U_\psi)$.

Thus, for any $\frac{m}{n} \in \Q$, we can define an $m$-valued representation in the following way:

$$\begin{aligned}
	\phi_\frac{m}{n}:SO(2)\rightarrow \C \\
	U_\theta \mapsto e^\frac{-i\theta m}{n} \\
\end{aligned}$$

As it turns out, $2$-valued representations actually play a significant role in applications to physics. We will see more of this come up in later sections.

\chapter{$SO(3)$: The Rotation Group in Three Dimensions}

Similar to $SO(2)$, the group $SO(3)$ represents all possible ways to take the physical action or rotating a vector in space. However, the space we can rotate within has changed to a three-dimensional space for this group, allowing us to have far more possibilities for our rotations. While we will make some simplifying generalizations here, this group still becomes incredibly more complicated than its two-dimensional counterpart. We will assume all rotations take place in $\R^3$. Like before, we will always assume our angle of rotation is real-valued, but we do not specify yet an orientation about which positive rotation occurs. Unless otherwise indicated, we will take $\{e_1,e_2,e_3\}$ to be an orthonormal basis of $\R^3$.

\section{Construction and Properties}

If we define $SO(3)$ to act on $\R^3$ in a similar way that $SO(2)$ acts on $\R^2$, we could not as easily uncover a formulation for every matrix. Instead, we use the characteristics of $SO(2)$ to guide our intuition for a way to understand the matrices that compose $SO(3)$. Firstly, any rotation should leave the magnitude of a vector in $\R^3$ unchanged. This leads us to conclude that the matrices must be orthogonal. Further, it will also follow that these matrices have determinant $1$, since all rotations can be reached continuously from the identity. With this information alone, we can deduce that the $SO(3)$ matrices form a group under matrix multiplication. The question of classifying these matrices is still open for use to answer. There are two main ways we can do so.

Let us consider any $n\in\R^3$. Once picking $n$, we can consider rotating about the vector, treating it as an axis, considering any positive angle measure to be a counter-clockwise rotation about this axis.

\tdplotsetmaincoords{60}{120}
\begin{center}
		\begin{tikzpicture}
				[scale=1.2,
					tdplot_main_coords,
					cube/.style={very thick,black},
					grid/.style={very thin,gray},
					axis/.style={->,blue,thick},
					vector/.style={-stealth,red,very thick}]
		

			%draw the axes
			\draw[axis] (0,0,0) -- (3,0,0) node[anchor=west]{$y$};
			\draw[axis] (0,0,0) -- (0,3,0) node[anchor=west]{$z$};
			\draw[axis] (0,0,0) -- (0,0,3) node[anchor=west]{$x$};
		
			\coordinate (O) at (0,0,0);
			\coordinate (P) at (3,3,5);
			
			\draw[vector] (O) -- (P);


		\end{tikzpicture}
\end{center}





\vspace{2cm}
Hello











% ------------- End main chapters ----------------------



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% the following two commands set up the bibliography and references 
%% automatically throughout the document.  The file my_special_bibliography.bib 
%% is one you create with all the info about all your references.  The alpha.bst file 
%% is included in your LaTeX distribution, but you can modify it if you want.  (But 
%% you don't want.)  
%\bibliography{my_special_ibliography}
%\bibliographystyle{alpha}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}
\bibliographystyle{IEEEtran}

\bibitem{Tung}

\bibitem{Mendes}
%A.~F. Blumberg and G.~L. Mellor.
%\newblock A description of a three-dimensional coastal circulation model.
%\newblock In N.~Heaps, editor, {\em Three Dimensional Coastal Ocean Models},
  %pages 1--16. Amer. Geophys. Union, 1987.

%\bibitem{Gill}
%A.~Gill.
%\newblock {\em Atmosphere - {O}cean {D}ynamics}.
%\newblock Academic Press, 1982.

%\bibitem{Chob}
%P.~F. Choboter, R.~M. Samelson, and J.~S. Allen.
%\newblock A {N}ew {S}olution of a {N}onlinear {M}odel of {U}pwelling.
%\newblock {\em J. Phys Oceanogr.}, 35:532--544, 2005.

%\bibitem{Lentz}
%S.~J. Lentz and D.~C. Chapman.
%\newblock The importance of non-linear cross-shelf momentum flux during
  %wind-driven coastal upwelling.
%\newblock {\em J. Phys. Oceanogr.}, 34:2444--2457, 2004.

%\bibitem{Ped2}
%J.~Pedlosky.
%\newblock A {N}onlinear {M}odel of the {O}nset of {U}pwelling.
%\newblock {\em J. Phys Oceanogr.}, 8:178--187, 1978.

\end{thebibliography}


% Indents Appendix in Table of Contents
\makeatletter
\addtocontents{toc}{\let\protect\l@chapter\protect\l@section}
\makeatother

% Hack to make Appendices to appear in Table of Contents
\addtocontents{toc}{%
   \noindent APPENDICES
}

\begin{appendices}
\chapter{Appendix A Title} \label{Appendix A}

Blah blah

\end{appendices}


%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}