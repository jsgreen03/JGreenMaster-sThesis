\documentclass[10pt]{ucthesis}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%    \pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue \fi

\usepackage{url}
%\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=black,citecolor=black,%
                                       linkcolor=black,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor={YOUR NAME},
                                       pdftitle={YOUR THESIS TITLE},
                                       pdfkeywords={thesis, masters, cal poly}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

%\else
%    \usepackage{graphicx}
%\fi

\usepackage{booktabs} % To thicken table lines
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[letterpaper]{geometry}	
\usepackage[overload]{textcase}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage{array}
%\hypersetup{draft}
%\usepackage[draft]{hyperref}
%\usepackage{nohyperref}  % This makes hyperref commands do nothing without errors
%\usepackage{url}  % This makes \url work
%\usepackage[morefloats=125]{morefloats}
%\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}
\usepackage{color}
\usepackage[nonumberlist,toc]{glossaries}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{morefloats}
\usepackage{float}
\usepackage{listings}
\usepackage{makecell}
\usepackage{appendix}
\usepackage[]{algorithm2e}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{angles,quotes}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{caption}

%\usepackage[breaklinks=true,hidelinks,pdfusetitle]{hyperref}
% \usepackage{cleveref}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% Added to avoid windows and orphans
\usepackage[all]{nowidow}
% Added to fix spacing between footnote entries
\usepackage{setspace}

\makeatletter
\let\normalsize\relax
\let\@currsize\normalsize
\makeatother


\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{example}[definition]{Example}
\newtheorem{algo}[definition]{Algorithm}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{assumption}[definition]{Assumption}
\newtheorem{remark}[definition]{Remark}
\newtheorem{corrolary}[definition]{Corrolary}


%\bibliographystyle{abbrv}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{bluegray}{rgb}{0.4, 0.6, 0.8}
\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}
\definecolor{pastelred}{rgb}{1.0, 0.41, 0.38}
\definecolor{wildwatermelon}{rgb}{0.99, 0.42, 0.52}
\definecolor{classicrose}{rgb}{0.98, 0.8, 0.91}
\definecolor{cobalt}{rgb}{0.0, 0.28, 0.67}
\definecolor{columbiablue}{rgb}{0.61, 0.87, 1.0}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1in,bmargin=1in,lmargin=1.5in,rmargin=1.5in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

   
\titleformat{\chapter}[hang]
{\normalfont%
    % \huge% %change this size to your needs for the first line
    \bfseries}{\chaptertitlename\ \thechapter}{10pt}{%
    % \huge %change this size to your needs for the second line
    }
    
\titleformat{\section}[hang]
{\normalfont%
    % \huge% %change this size to your needs for the first line
    \bfseries}{ \thesection}{10pt}{%
    % \huge %change this size to your needs for the second line
    }

% \renewcommand*{\chapterheadendvskip}{%
%   \vspace{0.725\baselineskip plus 0.115\baselineskip minus 0.192\baselineskip}%
% }

\titleformat{\chapter}[display]
        {\normalfont\normalsize\centering%\bfseries
        }
        {\ifthenelse{\equal{\thechapter}{A}}{APPENDICES\\[4.3ex]}{}\chaptertitlename\ \thechapter}
        {0pt}{\normalsize\uppercase}
\titlespacing*{\chapter}{0pt}{-10pt}{4.3ex plus .2ex}


\titleformat*{\section}{\normalsize%\bfseries
}
\titleformat*{\subsection}{\small%\bfseries
}
\titleformat*{\subsubsection}{\small%\bfseries
}
\titleformat*{\paragraph}{\small%\bfseries
}
\titleformat*{\subparagraph}{\small%\bfseries}
}

% \hypersetup{nolinks=true}
\begin{document}
\hypersetup{nolinks=true}


% Declarations for Front Matter

% Update fields below!
\title{Representation Theory in Braid Groups}
\author{Jaxon Green}
\degreeyear{2024}
\degreesemester{June}
\degree{Master of Science}
\chair{Professor Ben Richert\\  Professor of Mathematics} 
\othermembers{Professor Sean Gasiorek}
%\othermemberA{OTHER MEMBER HERE \\ & Professor of Mathematics}
%\othermemberB{OTHER MEMBER HERE\\ & Professor of Mathematics} 
\prevdegrees{None}
\numberofmembers{1}
\field{Mathematics} 
\campus{San Luis Obispo}
%\copyrightyears{seven}



\maketitle

\begin{frontmatter}

\copyrightpage


\begin{abstract}

Write an abstract here.

\vspace*{-10pt}
\end{abstract}

\begin{acknowledgements}

Any acknowledgements?

\end{acknowledgements}

\tableofcontents


\listoftables

\listoffigures

% Add CHAPTER into table of contents.

%\addtocontents{toc}{%
   %\noindent Representation Theory\\
   %\noindent Representations of Groups in Physics
%}

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


% \chapter{CHAPTER}
% ------------- Main chapters here --------------------
%\chapter{Sample Chapter 1}
%\label{intro}

 %\section{First Section of Introduction}
 %\label{intro1}
 %This is an equation:
 %\begin{equation}
 %c^2=a^2+b^2.
 %\end{equation}










%\chapter{Sample Chapter 2}

%\section{MY FIRST SECTION}
%There are lots of great resources on the internet to help you learn \LaTeX.  
%Perhaps start with examples like the ones at 
%\begin{verbatim}http://en.wikibooks.org/wiki/LaTeX/Sample_LaTeX_documents.\end{verbatim} 

%It is important to cite references.
%\cite{Blum}  \cite{Gill} \cite{Ped2}
%\cite{Blum, Gill, Ped2}

%Organize the paper into sections and subsections.  

%\subsection{Interesting subsection title}

%You get the idea.  Hey, this one has some displayed math, 
%\[
   %\frac{2}{x} = \sin(\epsilon), 
%\]
%not that it makes any sense whatsoever.  And here is how you 
%do a numbered equation, 
%\begin{equation}
  % \int_{0}^{y^2} f(x) \, dx = \sqrt{z+y}.  
%\end{equation}
%Don't forget to punctuate your equations as part of the sentence.  
%You can do inline math, too, as in $f(x) = \lim_{n\to \infty} n f(x)/n$, which is trivial. 

%\subsection{Another interesting subsection title}

%Okay, not really.  



%\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   %\centering
   %\includegraphics[width=4in]{meme.jpg} 
   %\caption{This is a sample figure.  It is not interesting. Note that 
   %figures will ``float'' to wherever \LaTeX \ wants to put them.  }
   %\label{fig:example}
%\end{figure}


\chapter{Representation Theory}

\section{Introduction to Representations}

Over the course of this chapter, we will develop the theory and utility of representations. At a glance, representations give us the ability to dial back the complexity of a mysterious group by viewing its elements as matrices. Thanks to the rigorous development and study of linear algebra, groups of matrices are well-understood structures. Representations allow us to unravel the mystery of an unknown group structure and reveal a group's fundamental properties as results of linear algebra techniques. 

\begin{definition}
	(First Definition) A \textbf{representation} of degree n is a group homomorphism that maps a group into $GL_n(\mathbb{C})$
	$$\phi:G\rightarrow GL_n(\mathbb{C})$$
	We say that $\phi$ is a representation of $G$. If $\phi$ is an injective homomorphism, we say that the representation is \textbf{faithful}. Otherwise, the representation is called \textbf{degenerate}.
\end{definition}

To illustrate to concept of representations, we will consider the group of all roots of unity, $G$, for the following examples. We can construct multiple homomorphisms from $G$ to showcase different kinds of representations. 

\noindent Note: Since $GL_1(\mathbb{C})$ as $\mathbb{C}$ are isomorphic, we identify 3each $1x1$ matrix with its corresponding entry with its element in $\mathbb{C}$.

\begin{example}
	(Trivial Representation)\\\\
	\renewcommand{\arraystretch}{0.7}
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_1(\mathbb{C})$\\
		$\hspace{6mm}g\mapsto 1$
		\end{tabular}
\end{example}
\noindent This map is the trivial homomorphism from $G$ to $GL_1(\mathbb{C})$ and therefore it easily satisfies the requirement of a degree $1$ representation of $G$. We say that $\phi$ is the \textbf{trivial representation} of $G$.

	

\begin{example}
	(Nontrivial Degree 1 Representation) \\
	By construction of $G$, if $g\in G$, then $g = e^{\frac{2\pi im }{n}}$ where $m,n\in \mathbb{Z}$	
	\renewcommand{\arraystretch}{0.7}\\\\
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_1(\mathbb{C})$\\
		$\hspace{6mm}g\mapsto g$
		\end{tabular}
\end{example}
\noindent where we view $G$ as a multiplicative subgroup of $\mathbb{C}$. This observation trivializes the argument that $\phi$ is a homomorphism. Therefore, $\phi$ is a degree $1$ representation of $G$.		


\begin{example}
	(Degree 2 Representation) \\\\
	\renewcommand{\arraystretch}{1}
	Let   \begin{tabular}{l}$\phi:G\rightarrow GL_2(\mathbb{C})$\\
		$\hspace{0mm}e^{2\pi i\frac{m}{n}}\mapsto \begin{bmatrix}
							\cos(\frac{2\pi m}{n}) & \sin(\frac{2\pi m}{n}) \\
							-\sin(\frac{2\pi m}{n}) & \cos(\frac{2\pi m}{n})
						      \end{bmatrix}$
		\end{tabular}
\end{example}
\noindent To show this map is a homomorphism, we will take two elements of $G$, say $e^{2\pi i\frac{x}{y}}$ and $e^{2\pi i\frac{a }{b}}$ and track the image of their product under $\phi$. \\
	\begin{equation}
		\begin{aligned}
			\phi(e^{2\pi i\frac{x}{y}} * e^{2\pi i\frac{a}{b}} ) &= \phi(e^{2\pi i(\frac{x}{y}+\frac{a}{b})})\\ 
												    &= \begin{bmatrix}
														\cos(2\pi(\frac{x}{y}+\frac{a}{b})) & \sin(2\pi(\frac{x}{y}+\frac{a}{b})) \\
														-\sin(2\pi(\frac{x}{y}+\frac{a}{b})) & \cos(2\pi(\frac{x}{y}+\frac{a}{b}))
													  \end{bmatrix}\\
												    &= \begin{bmatrix}
														\cos(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \sin(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})   &\sin(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) + \cos(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})\\
														-\sin(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \cos(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b}) & \cos(2\pi\frac{x}{y})\cos(2\pi\frac{a}{b}) - \sin(2\pi\frac{x}{y})\sin(2\pi\frac{a}{b})
													  \end{bmatrix}\\ 
												    &= \begin{bmatrix}
														\cos(2\pi\frac{x}{y}) & \sin(2\pi\frac{x}{y}) \\
														-\sin(2\pi\frac{x}{y}) & \cos(2\pi\frac{x}{y})
												          \end{bmatrix}
											  		  \begin{bmatrix}
														\cos(2\pi\frac{a}{b}) & \sin(2\pi\frac{a}{b}) \\
														-\sin(2\pi\frac{a}{b}) & \cos(2\pi\frac{a}{b})
													  \end{bmatrix} \\
		                                                                                    &= \phi(e^{2\pi i\frac{x}{y}})*\phi(e^{2\pi i\frac{a}{b}})
		\end{aligned}
	\end{equation}
	Since, $\phi$ has been shown to be a homomorphism, we can conclude that $\phi$ is also a degree $2$ representation of $G$.\\

	\noindent Is $\phi$ faithful or degenerate?A faithful representation would have a trivial kernel. Suppose $\phi(e^{2\pi i\frac{x}{y}}) = I_2$ ($I_n$ is the Identity Matrix of dimension $n\times n$). 
	\begin{equation}
		%\begin{aligned}
			\begin{bmatrix}
				\cos(2\pi\frac{x}{y}) & \sin(2\pi\frac{x}{y}) \\
				-\sin(2\pi\frac{x}{y}) & \cos(2\pi\frac{x}{y})
			\end{bmatrix}\\
			= \begin{bmatrix}
				1 & 0 \\
				0 & 1 \\
			\end{bmatrix}\\
		%\end{aligned}
	\end{equation}
	\noindent Comparing entrywise, we see that $\cos(2\pi\frac{x}{y}) = 1$ and $\pm\sin(2\pi\frac{x}{y}) = 0$. Using any of these equations, we see that $\frac{x}{y}= n$ for some $n\in\mathbb{Z}$. Therefore, $ker(\phi)=\mathbb{Z}$ and this representation is degenerate.\\


Alternatively, we can formulate the definition of a representation in a different context, illuminating a useful interpretation that will be used extensively throughout this paper.

\begin{definition}
	(Second Definition) Let $G$ be a group, let $V$ be a linear vector space, and let $\mathcal{L}(V)$ be the group of linear operators on V together with the operation of composition. A \textbf{representation} of $G$ is a group homomorphism that maps $G$ into $\mathcal{L}(V)$.
	$$\phi : G \rightarrow \mathcal{L}(V)$$
The degree of the representation is the dimension of $V$.
\end{definition}

\begin{remark}
	In the case where we have a finite dimensional vector space, we can make an interesting observation. Suppose $V$ is finite dimensional and $G$ is a group. It is easy to identify both definitions of representations with one another. Let $\{e_i\}_{i=1}^n$ be a basis for $V$. Let $\phi : G \rightarrow \mathcal{L}(V)$ be a representation of $G$. Then $\forall g \in G$, $U_g\coloneq\phi(g)$ is a linear operator on $V$. $U_g$ has a corresponding matrix, $M(U_g)$, with coefficients defined by the image of our basis vectors of $V$.
$$M(U_g) = \begin{array}{c c}
			U_g(e_1) \hspace{2mm} U_g(e_2) \hspace{2mm} \hdots \hspace{2mm}  U_g(e_n) & \\
			\begin{bmatrix}
				m_{11} & m_{12} & \hdots & m_{1n}\\
				m_{21} & m_{22} & \hdots & m_{2n} \\
				\vdots & \vdots & \ddots & \vdots\\
				m_{n1} & m_{n2} & \hdots & m_{nn}\\
			\end{bmatrix}
			&
		        %\def\arraystretch{0.75}
			\begin{array}{c}
				e_1\\
				e_2\\
				\vdots\\
				e_n\\
			\end{array}
		\end{array}$$
$$U_g(e_j) = \sum_{i=1}^n m_{ij}e_i$$
\renewcommand{\arraystretch}{0.5}
Does the map \begin{tabular}{l}
			$\psi:G\rightarrow GL_n(\mathbb{C})$\\
			\hspace{6mm}$g\mapsto M(U_g)$
       		 \end{tabular}
satisfy the criteria to be considered a representation (by the first definition)? If $g,h \in G$, then

	$$\psi(gh) = M(\phi(gh)) = M(\phi(g)\circ \phi(h)) = M(\phi(g))*M(\phi(h)) = \psi(g)\psi(h)$$

Where the homomorphism property of $\phi$ is used in succession with the relationship between the composition of operators and the multiplication of their corresponding matrices. This observation illustrates a special conenction between the two definitions of a representation. If we are given a representation defined in the either way, we can interpret the target space of the homomorphism in the context of both definitions. That is to say, every $n\times n$ matrix can be interpreted as a linear operator on an $n$-dimensional vector space, and vice-versa. The homomorphism property of one definition is a necessary condition for the homomorphism in the other definition. Hence, we can see the two definitions of representations are equivalent in the case of a finite dimensional vector space.
\end{remark}

\begin{example}
	Let $G$ be the group defined by the complex unit circle and the operation of multiplication and let $V = \mathbb{C}$ 
	\begin{center}
		 \begin{tabular}{l}$\phi:G\rightarrow \mathcal{L}(V)$\\
				$\hspace{4mm}e^{i\theta}\mapsto U_{e^{i\theta}}$
		\end{tabular}
	\end{center}
	where $U_{e^{i\theta}}$ is the linear operator (on $V$) that multiplies its input by $e^{i\theta}$. 
\end{example}

Each operator is clearly linear. The process of confirming a map is a representation is relatively standard. However, there does not seem to be any intuitive way to come up with a new representation. The rest of this chapter will be devoted the process of comparing and characterizing every representation of a given group. We appeal to Definition 3.5 to argue that this map is a representation. Let $e^{i\theta}$, $e^{i\psi} \in G$. Then $\phi(e^{i\theta} * e^{i\psi}) = U_{e^{i(\theta+\psi)}}$. For all $re^{i\gamma} \in V$, we have

	\begin{equation}
		\begin{aligned}
			U_{e^{i(\theta+\psi)}}(re^{i\gamma}) &= re^{i\gamma} * e^{i(\theta+\psi)} \\
										&= re^{i\gamma + i\psi + i\theta} \\
										&= U_{e^{i\theta}}(re^{i\gamma + i\psi}) \\
										&= U_{e^{i\theta}}(U_{e^{i\psi}}(re^{i\gamma})) \\
										&= (U_{e^{i\theta}}\circ U_{e^{i\psi}}) (re^{i\gamma})
		\end{aligned}
	\end{equation}

Since $\phi(e^{i\psi})*\phi(e^{i\theta}) = U_{e^{i\theta}}\circ U_{e^{i\psi}}$, we have shown that the homomorphism property of $\phi$ is satisfied. Therefore, $phi$ is a representation. We can now identify this definition of a representation with the initial formulation in two possible ways.

\begin{assumption}
	$V$ is a vector space over $\mathbb{C}$ (as a field). 
\end{assumption}

If we consider $V$ to be a vector space over $\mathbb{C}$, then it is a one-dimensional vector space. This means that the matrix of any operator defined on $V$ will be a $1\times 1$ matrix (or, an element of $\mathbb{C}$). Taking the basis $\{1\}$ of $V$ and $e^{i\theta} \in G$, we see that 

$$M(U_{e^{i\theta}}) = \begin{bmatrix}
					e^{i\theta}
				\end{bmatrix} = e^{i\theta} $$
This is clearly a degree one representation given by Definition 3.1.

\begin{assumption}
	$V$ is a vector space over $\mathbb{R}$
\end{assumption}

If $V$ is a vector space over $\mathbb{R}$, then it is a two-dimensional vector space. This means that the matrix of any operator defined on $V$ will have be of shape $2\times2$. Taking the basis $\{1,i\}$ of $V$, any $e^{i\theta} \in G$, and the identity $e^{i\theta} = \cos(\theta) + i\sin(\theta)$ we see that the following equalities hold:
$$(a+bi)e^{i\theta} = (a\cos(\theta)-b\sin(\theta))+i(a\sin(\theta)+b\cos(\theta))$$
$$M(U_{e^{i\theta}}) = \begin{bmatrix}
					\cos(\theta) & -\sin(\theta) \\
                                        \sin(\theta) & \cos(\theta)
				\end{bmatrix}$$

This representation will be revisited later in greater detail as multiplication of complex numebrs by $e^{i\theta}$ corresponds to rotation in the complex plane by the angle $\theta$ about the origin.

For the rest of the paper, we shall almost exclusively be considering finite dimensional vector spaces and therefore will interchangeably use both definitions of representations as needed.

\section{Decomposing and Characterizing Representations}

While we have shown it is relatively straightforward to argue whether a given map is a representation, it is not yet clear how we can come up with our own, compare different ones, or what kinds of properties a representation has. This section will explore the properties of representations and how we can use them to deepen our understanding of representations.

\begin{definition}
	Two representations, $\phi$ and $\psi$, are said to be \textbf{equivalent representations} if there exists some invertible operator/matrix (depending on definition of representation), $M$, such that $$\phi = M \psi M^{-1}$$
\end{definition}

In the context of linear algebra, this conjugation by an invertible matrix can most easily be thought of as a change of basis transformation. With this in mind, we can see that representations of groups can be spilt into equivalence classes based on matrix similarity (or similarity of any matrix of the operator). In order to deduce whether or not representations are equivalent, we need to utilize matrix-similarity-preserving opertations to find common traits. A natural first choice is the trace operation on matrices.

\begin{definition}
	The \textbf{character} of a representation, $\phi$, on $g \in G$, denoted $\chi^{\phi}(g)$, is defined by $$\chi^{\phi}(g)=trace(\phi(g))$$
\end{definition}

\begin{theorem}
	If two representations are equivalent, then character of both representations are the same.
\end{theorem}

(Pf.) Suppose $\phi$ and $\psi$ be two equivalent representations. Then $\exists M$ such that $\forall g \in G$, $\phi(g) = M\psi(g)M^{-1}$. Then $\forall g \in G$,

\begin{equation}
	\begin{aligned}
		\chi^{\phi}(g) &= trace(\phi(g)) \\
						&= trace(M\psi(g)M^{-1}) \\
						&= trace(\psi(g)M^{-1}M) \\
						&= trace(\psi(g)) = \chi^{\psi}(g) \\
	\end{aligned}
\end{equation}

Therefore, $\chi^{\phi} = \chi^{\psi}$. $\qedsymbol$\\

The character of a representation gives us the ability to quickly rule out equivalence of representations without getting into messy matrix calculations, especially in higher degree representations.

\begin{example}
	Let $S_3$ be the symmetric group of degree 3 and $\phi$ and $\psi$ be defined below. Comparing the outputs of each map, it is clear that the maps are not identical. Are these representations equivalent? We can use the trace argument to justify why they are not. We can observe the following equalities directly: 
$$\chi^{\phi}(\sigma) = \chi^{\psi}(\sigma) \hspace{1mm} for \hspace{1mm} \sigma \in \{e, (12), (13), (23)\}$$
$$\chi^{\phi}(\sigma) \neq \chi^{\psi}(\sigma) \hspace{1mm} for \hspace{1mm} \sigma \in \{(132), (123)\}$$
As a result, $\chi^{\phi} = \chi^{\psi}$, and therefore, these representations are not equivalent. Despite this fact, the significance of this example is that we can identify similarities in both representations that lead us to believe that there is something inherently similar about them. Specifically, both representations send every permutation in $S_3$ to a matrix with its first row (column) fixed as $[1 \hspace{1mm} 0 \hspace{1mm} 0](^T)$. This observation is reminiscent of the trivial representation, defined in Example 3.2. We will revisit this matter later.
\\
	\renewcommand{\arraystretch}{1.25}
	\begin{center}
		$\begin{array}{c c}
			\begin{array}{c}
				\phi:S_3\rightarrow M_3(\mathbb{R}) \\
				\hspace{0mm} e \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (12) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (13) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (23) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (123) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (132) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
			\end{array} &
	
			\begin{array}{c}
			\psi:S_3\rightarrow M_3(\mathbb{R}) \\
				\hspace{0mm} e \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (12) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 1 & 0 \\
												0 & -1 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (13) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & -1 & -1 \\
												0 & 0 & 1 \\
											\end{bmatrix}\\
				\hspace{0mm} (23) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 0 & 1 \\
												0 & 1 & 0 \\
											\end{bmatrix}\\
				\hspace{0mm} (123) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & 0 & 1 \\
												0 & -1 & -1 \\
											\end{bmatrix}\\
				\hspace{0mm} (132) \mapsto \begin{bmatrix}
												1 & 0 & 0 \\
												0 & -1 & -1 \\
												0 & 1 & 0 \\
											\end{bmatrix}\\
			\end{array}
		\end{array}$
	\end{center}
\end{example} 

As eluded to in the previous example, it seems that representations can share "pieces" in common without being considered equivalent. In the same way we decompose linear operators (and their matrices) into a block diagonal structure for the simplicity of our study, we can decompose the linear operators (and matrices) of a representation in the same way to make strikingly similar conclusions. In this way, we can reveal a more intuitive understanding of the underlying representation and create an natural way to fully decompose any arbitrary representation into natural components. 

\begin{definition}
	Let $\phi$ be a representation of the group $G$ and $U_G \coloneq \{\phi(g)=U_g: V\rightarrow V \hspace{1mm}| \hspace{1mm} g\in G\}$. Let $W \subset V$. $W$ is said to be an \textbf{invariant subspace} with respect to $U_G$ if $\forall v \in W$ and $\forall g \in G$, $U_g(v)\in W$.
\end{definition}

In general, we say that a subspace satisfying the above condition is invariant with respect to $\phi$. Our most familiar understanding of invariant subspaces comes from our study of decomposing generic linear operators (matrices) into its corresponding eigenspaces. This process is not unfamiliar from what we will study next, but with a heavier restriction, since our new definition considers many operators at once.

\begin{definition}
	A representation is said to be \textbf{irreducible} if there is no nontrivial, invariant subspace with respect to it. Otherwise, we say that the representation is reducible.
\end{definition}

It will turn out that the irreducible representations of a group will be the "pieces" that we can recognize as the building blocks of each representation. Due to the nature of invariant subspaces, it is no surprise that we can see a block diagonal structure form in the matrices of representations.

\begin{definition}
	If $M$ and $N$ are square matrices, then let $$M\oplus N \coloneq \begin{bmatrix}
																			M & 0\\
																			0 & N\\
																		\end{bmatrix}$$
	we call this new matrix the \textbf{direct sum of M and N}
\end{definition}

\begin{example}
	Referring back to one of the matrices from Example 3.13, we can see that
$$\begin{bmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & -1 & -1
\end{bmatrix} = \begin{bmatrix}
					1
					\end{bmatrix} \oplus
					\begin{bmatrix}
						1 & 0 \\
						-1 & -1
					\end{bmatrix}$$
It becomes increasingly clear that the fixed first row and column of these matrices are directly linked to the trivial representation.
\end{example}

\begin{theorem}
	Let $\phi$ be a representation of a group $G$.  Then there exists a set of irreducible representations of $G$, $\{\psi_i\}_{i=1}^j$, such that $$\phi = T\left(\bigoplus_{i=1}^j \alpha_i*\psi_i\right)T^{-1}$$ where $\alpha_i*\psi_i \coloneq \underbrace{\psi_i \oplus \psi_i \oplus \hdots \oplus \psi_i}_{\alpha_i times}$ and $T$ is some invertible matrix/operator. 
\end{theorem}
 
\noindent (Pf.) By induction on the degree of the representation $n$. \\

(Base Case: $n = 1$) Suppose that $\phi$ is a degree one representation of $G$, with $U_G$ being defined as in Definition 3.14. Then $\forall U_g \in U_G$, we have seeking to show that every invariant subspace of $V$ with respect to $U_g$ is trivial. Suppose that $W \subset V$ is a subspace and let $dim(W)$ denote the dimension of $W$. Then, $dim(W) \leq dim(V) = 1$ as given by the degree of the representation. If $dim(W) = 1$, then $W=V$ and therefore, $W$ is a trivial subspace. If $dim(W) < dim(V)$, then it can only be the case that $dim(W) = 0$, and therefore, $W= \{0\}$, which is also trivial. Therefore, every possible subspace of $V$ must be trivial, and as a result, every invariant subspace must also be trivial. Hence, $\phi$ is an irreducible representation. \\

(Inductive Hypothesis) Suppose that for any representation of degree $0<k\leq n$, $\phi$, we have that $\phi =T\left(\bigoplus_{i=1}^j \alpha_i*\psi_i\right)T^{-1}$ where $T$ is some invertible matrix/operator and $\{\psi_i\}_{i=1}^j$ is some set of irreducible representations of $G$. \\

Let $\phi'$ be a representation of G of degree $n+1$. If $\phi'$ is irreducible, then we are done. If $\phi'$ is reducible, then $\exists W \subset V$ such that $W$ is a nontrivial, $\phi'$-invariant subspace. Let $W = span\{w_i\}_{i=1}^k$ where $k\leq n$ and choosing a basis for $V$ to be $\{w_i\}_{i=1}^k \cup \{v_i\}_{i=k+1}^{n+1}$ with $v_i \notin W \hspace{1.5mm} \forall i$. Then, there exists an invertible matrix $T$ such that
\begin{equation}
	M(\phi') = T\begin{bmatrix}
					M_W& 0\\
					0 & M_{X}\\
					\end{bmatrix}T^{-1}
\end{equation}
where $M_W$ is a $k\times k$ block representing the invariant subspace $W$ and $M_{X}$ is a $(n-k+1)\times (n-k+1)$ block representing the subspace defined by $X \coloneq span\{v_i\}_{i=k+1}^{n+1}$. Given the structure of our block matrices, both the $M_W$ and $M_X$ blocks are $k$ degree and $n+1-k$ degree representations of $G$. Therefore, we can apply our induction hypothesis to each of the blocks and to argue that 
$$M_W = A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}$$
$$M_W = B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}$$
\begin{equation}
	M(\phi') = T\begin{bmatrix}
					A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}& 0\\
					0 &  B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}\\
					\end{bmatrix}T^{-1}
\end{equation}
We can break up our matrix $T$ into block structure to match the size of our blocks in Equation 3.5.
\begin{equation}
	M(\phi') =  \begin{bmatrix}
					T_{11} & T_{12} \\
					T_{21} & T_{22}
				\end{bmatrix}
				\begin{bmatrix}
					A\left(\bigoplus_{i=1}^j \alpha_i\psi_i \right)A^{-1}& 0\\
					0 &  B\left(\bigoplus_{i=1}^{l} \beta_i\mu_i  \right)B^{-1}\\
				\end{bmatrix}
				\begin{bmatrix}
					T_{11}^{-1} & T_{12}^{-1} \\
					T_{21}^{-1} & T_{22}^{-1}
				\end{bmatrix}
\end{equation}
Using algebraic manipulations, we can pull back our change of basis matrices into corresponding blocks of $T$.
\begin{equation}
	M(\phi') =  \begin{bmatrix}
					T_{11}A & T_{12}B \\
					T_{21}A & T_{22}B
				\end{bmatrix}
				\begin{bmatrix}
					\bigoplus_{i=1}^j \alpha_i\psi_i & 0\\
					0 &  \bigoplus_{i=1}^{l} \beta_i\mu_i \\
				\end{bmatrix}
				\begin{bmatrix}
					A^{-1}T_{11}^{-1} & A^{-1}T_{12}^{-1} \\
					B^{-1}T_{21}^{-1} & B^{-1}T_{22}^{-1}
				\end{bmatrix}
\end{equation}
%Prove that our new T and T^{-1} are actually inverses
Given that the flanking matrices are both inverses of each other, which we will refer to as $P$ and $P^{-1}$ respectively, we take $\{\nu_i\}_{i=1}^{j+l}$ and  $\{\gamma_i\}_{i=1}^{j+l}$ to be defined by 
$$\begin{array}{c c}
	\nu_i = \begin{cases}
		\psi_i & i \leq j\\
		\mu_{i-j} & i > j
	\end{cases}
&
	\gamma_i = \begin{cases}
		\alpha_i & i \leq j\\
		\beta_{i-j} & i > j
	\end{cases}
\end{array}$$
to conclude that
\begin{equation}
	\phi' = P\left(\bigoplus_{i=1}^{j+l} \gamma_i*\nu_i\right)P^{-1}
\end{equation} \qedsymbol

\begin{remark}
	The main purpose of this theorem is to illustrate that any representation can be thought of as a "linear combination" of irreducible representations of that group. Being able to readily compare the irreducible representation decomposition of any two given representations is key to understanding similarities and differences between them.
\end{remark}

\begin{example}
	Let $\phi$ and $\psi$ be defined as they were in Example 3.13. 
\end{example}

\noindent There have been three different irreducible representations that were used to construct these maps. Consider $\mu_1$ to be the trivial representation and $\mu_2$ and $\mu_3$ to be defines as below:
$$\begin{array}{c c}
	\begin{array}{c}
		\mu_2 : S_3 \rightarrow \mathbb{R} \\
		\sigma \mapsto sign(\sigma) = \begin{cases}
										1 & \text{if even permutation} \\
										-1 & \text{if odd permutation}
									   \end{cases}\\
		\text{Sign Representation}
	\end{array}
&
	\begin{array}{c}
		\mu_3 : S_3 \rightarrow M_2(\mathbb{R}) \\
		\sigma \mapsto \text{Bottom Right } 2\times2 \text{ Block of } \psi(\sigma)\\
		\text{Degree 2 Irreducible Representation}
	\end{array}
\end{array}$$
We can see the following two equalities hold:
$$\phi = \mu_1 \oplus \mu_1 \oplus \mu_2 = 2\mu_1 \oplus \mu_2$$
$$\psi = \mu_1 \oplus \mu_3$$

Now that we have established that representations are best understood by studying the irreducible representations that compose them, we shall focus our attention to characterizing the irreducible representations. There are many theorems and useful corrolaries that we will make use of later that will be established now.

\begin{theorem}
	If $\phi$ is an irreducible representation of a group $G$, then any representation equivalent to $\phi$ must also be irreducible.
\end{theorem}

\noindent (Pf.) For this proof, we will consider the linear operator definition of representations. Let $\phi_g \coloneq \phi(g)$ and consider each operator to be defined on the vector space $V$. Let $\psi$ be defined as an equivalent representation of $G$, so there exists an invertible, linear operator, $S$ such that $\phi_g = S\psi_g S^{-1}$ $\forall g \in G$. we can assume that for every $g$, $\phi_g$ is defined as an operator on the vector space $V'$. It will suffice to show that any $\psi_g$-invariant subspace of $V'$ is a trivial subspace. \\

Let $W' \subset V'$ be a $\psi_g$-invariant subspace. Let $w' \in W'$. Then, the following equation holds:

$$(\phi_gS)(w') = (S\psi_g)(w')$$

This equality illustrates that for any $v \in S(W')$ (the image of $W'$ under $S$, $\phi_g (v) \in S(W')$. Therefore, $S(W')$ is a $\phi_g$-invariant subspace. By definition, $S(W') = \{0\}$ or $S(W') = V$. \\

Since $S$ is an invertible operator, the rank-nullity theorem tells us that $dim(W')=0$ or $dim(W') = dim(V)$. Then, either $W' = \{0\}$ or the equivalence of representations gives us that $dim(V)=dim(V')$, and therefore, $W' = V'$. \\

Since there are no non-trivial $\psi_g$-invariant subspaces, $\psi$ must also be an irreducible representation of $G$. \qedsymbol



\begin{theorem} \textbf{Schur's Theorem}
	Let $\phi$ and $\psi$ be two irreducible representations of the group $G$. Let $M$ be a matrix/linear map defined such that $M\phi(g) = \psi(g)M \hspace{1.5mm}\forall g \in G$. Then $M$ is invertible or $0$. 
\end{theorem}

\noindent (Pf.) Reminder: When viewing this proof from the perspective of operators, interpret the product of operators as composition as I defined at the beginning. 

Suppose the degree of $\phi$ is $n$ and the degree of $\psi$ is $m$. Then $M$ must be an $m\times n$ matrix. Let $v\in ker(M)$. Then $\forall g \in G$, we have 
\begin{equation}
	 (M\phi(g))v= (\psi(g)M)v = 0
\end{equation}

This shows us that $\phi(g)v \in ker(M)$ as well. As a result, $ker(M)$ shown to be a $\phi$-invariant subspace. Since $\phi$ is irreducible, it must be the case that either $ker(M) =\{0\}$ or $ker(M)$ is the entire space.  

Similarly, if $w \in im(M)$, then we can show that $im(M)$ is a $\psi$-invariant operator with the following argument: $\forall g\in G$,

\begin{equation}
	\psi(g)w =\psi(g)M(x) = M\phi(g)x \in im(M)
\end{equation}

for some $x$ in our space. Then, $im(M)$ must be a $\psi$-invariant subspace, and therefore, $im(M) = \{0\}$ or the whole space.

If $ker(M)$ is the whole space or $im(M) = \{0\}$, then clearly, $M=0$. However, if this is not the case, then $ker(M) = \{0\}$ and $im(M)$ is the whole space, and we have $m=n$ giving us an invertible $M$. \qedsymbol

\begin{corrolary}
	Let $\phi$ be an irreducible representation and $M$ be a matrix/operator such that $M\phi(g)=\phi(g)M \hspace{1.5mm}\forall g \in G$. Then $M$ is a mulptiple of the identity matrix/map.
\end{corrolary}

\noindent (Pf.) Let $\lambda$ be an eigenvalue of $M$. Then $M - \lambda I$ is not invertible. Following from Schur's Theorem, we see that $\forall g \in G$
$$(M-\lambda I)\phi(g) = \phi(g) (M-\lambda I)$$
Therefore, it must be the case that $M-\lambda I = 0$ \cite{Mendes} \qedsymbol 

\begin{corrolary}
	If $G$ is an abelian group, then any irreducible representation of $G$ is can be viewed as a degree one representation.
\end{corrolary}

\noindent(Pf.) Let $\phi$ be an irreducible representation of $G$. Let $g \in G$. Then, $\forall h \in G$, $\phi(g)\phi(h) = \phi(h)\phi(g)$. Then, Schur's Theorem tells us that $\phi(g) = \lambda_g I$ for some $\lambda_g \in \C$. Since $g$ is arbitrary, this argument forces every matrix in the representation to be a scalar multiple of the identity matrix. We can take the degree one representation to be the first entry of each matrix. \cite{Tung} \qedsymbol

\section{Unitary Representations}

While irreducible representations are of great importance to us, we can also deepend our understanding of a representation when our underlying vector space has an inner-product structure. In this case, we can further characterize our representations. 

\begin{definition}
	Let $V$ be an inner-product space. Let $U \in \mathcal{L}(V)$. $U$ is said to be \textbf{unitary} if $U$ is surjective and $\forall x,y \in V$, $\langle x , y \rangle = \langle U(x) , U(y) \rangle$.
\end{definition}

\begin{definition}
	A representation, $\phi$, of a group, $G$, is said to be a \textbf{unitary representation} if $\forall g\in G$, $\phi(g)$ is unitary.
\end{definition}

\begin{theorem}
	Every representation of a finite group on an inner-product space is equivalent to a unitary representation
\end{theorem}

\noindent (Pf.) Let $\phi$ be a non-unitary representation of a finite group $G$ defined on an inner-product space. Let $\phi_g$ denote $\phi(g)$, and let $S$ be defined such that $\langle S(x) , S(y) \rangle = \sum_{g \in G} \langle \phi_g (x) , \phi_g (y) \rangle$. We will show that the operator $\forall g \in G$, $U_g \coloneq S\phi_gS^{-1}$ is unitary. Fixing $g\in G$,


\begin{equation}
	\begin{aligned}
		\langle U_g(x) , U_g(y) \rangle &= \langle S\phi_gS^{-1}(x) , S\phi_gS^{-1}(y)\rangle \\
									&= \sum_{h \in G} \langle \phi_h(\phi_gS^{-1}(x)) , \phi_h(\phi_gS^{-1}(y))\rangle \\
									&= \sum_{h\in G} \langle \phi_{hg}(S^{-1}(x)) , \phi_{hg}S^{-1}(y))\rangle \\
									&= \sum_{l\in G} \langle \phi_{l}(S^{-1}(x)) , \phi_{l}S^{-1}(y))\rangle \\
									&= \langle S(S^{-1}(x)) , S(S^{-1}(y))\rangle \\
									&= \langle x , y \rangle 
	\end{aligned}
\end{equation}

Therefore, $U_g$ is unitary $\forall g \in G$. \qedsymbol

\begin{remark}
	Combining \textbf{Theorem 1.21} and \textbf{Theorem 1.27} allows us to grant the property of unitary to any irreducible representation through similarity transform. 
\end{remark}

The next portion of this section will be devoted to identifying and proving some of the key properties of irreducible and unitary representations. We will extensively use these properties in later sections.

\begin{theorem}
	Let $G$ be a finite group, let $\Phi = \{\phi \mid \phi \text{ is a distinct (inequivalent to others in set) irreducible, unitary representation of }G\}$. Let $\phi,\psi \in \Phi$ with degrees $n_{\phi}$ and $n_{\psi}$ respectively. Let $\phi_g \coloneq \phi(g)$ and $\psi_g \coloneq \psi(g)$. Then the following equality holds:
$$\frac{n_\phi}{|G|} \sum_{g\in G} \left[\phi_g \right]_{ij} \left[\psi_g^\dag \right]_{kl} = \begin{cases}
																						1 & \text{if } \psi = \phi,\hspace{1mm} j=k,\hspace{1mm} \text{and } i=l\\
																						0 & else
																					 \end{cases}$$
where for any matrix $M=\left[m\right]_{ij}$, $\left[M^\dag\right]_{ij} = \overline{m}_{ji} $. We refer to this as the \textbf{orthonormality condition} of unitary, irreducible representations.
\end{theorem}

\noindent (Pf.) Let $A$ be a $n_\phi \times n_\psi$ matrix and let $M_A \coloneq \sum_{g\in G} \phi_{g} A \psi_g^{\dag}$. Note: $\forall h \in G$, we have $\phi_h M_A  = M_A \psi_h^\dag$ by as seen from the following calculation:

\begin{equation}
	\begin{aligned}
		\phi_h M_A &=  \phi_h \left(\sum_{g\in G} \phi_{g} A \psi_g^{\dag}\right) &= \sum_{g\in G} \phi_{hg} A \psi_g^{\dag} &= \sum_{k = hg \hspace{1mm} \in G} \phi_k A \psi_{h^{-1}k}^{\dag} &= \left(\sum_{g\in G} \phi_{k} A \psi_k^{\dag} \right)\psi_h \\ &= M_A\psi_h 
	\end{aligned}
\end{equation}

By Schur's Theorem, $M_A$ is either the zero matrix with $\phi \neq \psi$ or $M_A$ is invertible with $\phi=\psi$. 

Suppose that $M_A = 0$. Then we can recover entries in the right hand side of our equation by observing the following chain of equations:
\begin{equation}
	\begin{aligned}
		    0 &= \left[M_A\right]_{il} \\ 
			&= \left[\sum_{g\in G} \phi_{g} A \psi_g^{\dag}\right]_{il} \\
			&= \sum_{g\in G} \left[\phi_{g} A \psi_g^{\dag}\right]_{il} \\
			&= \sum_{g\in G} \sum_{x=1}^{n_\phi} \sum_{y=1}^{n_\psi}\left[\phi_{g}\right]_{ix} \left[A\right]_{xy} \left[\psi_g^{\dag}\right]_{yl} \\
			&= \sum_{g\in G} \left[\phi_{g}\right]_{i1} \left[A\right]_{11} \left[\psi_g^{\dag}\right]_{1l} + \left[\phi_{g}\right]_{i1} \left[A\right]_{12} \left[\psi_g^{\dag}\right]_{2l} + \hdots + \left[\phi_{g}\right]_{i2} \left[A\right]_{21} \left[\psi_g^{\dag}\right]_{1l} + \hdots
	\end{aligned}
\end{equation}

We can see that this summand contains $n_\phi n_\psi$ terms where $[A]_{xy}$ are arbitrary entries. This means that we can choose $A$ specifically to recover relationships between matrix entries in $\phi_g$, $\psi_g$ and the left hand side of our chain of equations. That is, for a fixed $j,k$, choose $$[A]_{xy} = \begin{cases}
												1 & \text{if }x=j\text{ and } y=k \\
												0 & \text{else}
\end{cases}$$
To uncover that for any combination of $i,j,k,l,$
\begin{equation}
	\begin{aligned}
		\sum_{g\in G} \left[\phi_g\right]_{ij}\left[\psi^\dag_g\right]_{kl} = 0
	\end{aligned}
\end{equation}

Therefore, $\sum_{g\in G}\phi_{g}\psi_g^{\dag}=0$ \\

Suppose that $M_A$ is invertible and $\phi=\psi$. It must be the case then that $M_A = \lambda I_{n_\phi}$ where $\lambda \in \C$ following from Schur's Theorem. Choosing $A$ to be defined as it was in the previous case, we can see that for any $i,j,k,l,$
\begin{equation}
	\begin{aligned}
		\sum_{g\in G} \left[\phi_g\right]_{ij}\left[\phi^\dag_g\right]_{kl} = \begin{cases}
																			\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
	\end{aligned}
\end{equation}

The value of $\lambda$ will be shown to be dependent on the relationship between $j$ and $k$. If $j\neq k$, we can sum over every possible value for $i$ and then interepret this process as the standard inner-product (in $\C^{n_\phi}$) between two columns of a unitary matrix.

\begin{equation}
	\begin{aligned}
		n_\phi \lambda &= \sum_{g\in G} \sum_{i=1}^{n_\phi}\left[\phi_g\right]_{ij}\overline{\left[\phi_g\right]_{ik}} &= \sum_{g\in G} 0 &= 0
	\end{aligned}
\end{equation}


Therefore, whenever $j\neq k$, $\lambda = 0$. However, in the case that $j=k$, we can considering the fact that there are $n_\phi$ ways to choose $j$ and $k$ such that $j=k$. Summing over all these choices gives us a way to solve for $\lambda$:

$$\begin{aligned}
			\sum_{g\in G} \sum_{x=1}^{n_\phi}\left[\phi_g\right]_{ix}\left[\phi^\dag_g\right]_{xl} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			\sum_{g\in G} \left[\phi_g\phi^\dag_g\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			\sum_{g\in G} \left[I_{n_\phi}\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\Leftrightarrow
			|G|\left[I_{n_\phi}\right]_{il} = \begin{cases}
																			n_\phi\lambda &\text{if } i = l\\
																			0 & \text{else}
																			\end{cases}\\
\end{aligned}$$ 

Therefore, whenever $i=l$, $j=k$, and $\phi=\psi$, $\lambda = \frac{|G|}{n_\phi}$. \qedsymbol \\



\begin{corrolary}
	The number of inequivalent, unitary, irreducible representations is bounded in the following way:
$$\sum_{\phi \in \Phi}n^2_\phi \leq |G|$$
where $\Phi = \{\phi \mid \phi$ is a unitary, irreducible representation of $G\}$
\end{corrolary}

\noindent (Pf.) Consider the entry of each matrix of a given representation $\phi$ for every $g \in G$ as denoted $[\phi_g]_{ij}$. We can create vectors in a $|G|$-dimensional vector space in the following way:
$$\sqrt{\frac{n_\phi}{|G|}}\left([\phi_{g_1}]_{ij},\hspace{1mm} [\phi_{g_2}]_{ij}, \hspace{1mm}\hdots \hspace{1mm},\hspace{1mm} [\phi_{g_{|G|}}]_{ij}\right)$$
where each choice of $i$ and $j$ gives us a new vector. Since there are $n_\phi$ choices for both $i$ and $j$, it follows that for a fixed $\phi$, there are $n_\phi^2$ choices of vectors in this space. Thus, we can make $\phi$ arbitrarily to see that the number of vectors that can be formed in this way is identically $\sum_{\phi \in \Phi}n^2_\phi$. The orthonormality condition gives us the pairwise orthogonality of these vectors in $\phi$, $i$, and $j$. Since orthogonal vectors are linearly independent, and the number of linearly independent vectors in a vector space must be at most as large as its dimension, we conclude that $\sum_{\phi \in \Phi}n^2_\phi \leq |G|$. \cite{Tung} \qedsymbol\\


This conclusion is incredibly important because it tells us that the task of finding \textit{every} unitary, irreducible representation is possible to accomplish. In this way, we can not only characterize representations in terms of their unitary, irreducible counterparts, but we can do so for all representations with a finite number of special components. We further develop this idea with the next theorem.


\section{Irreducible Characters and the Regular Representation}

The process of fully characterizing representations in terms of irreducible components will ultimately come to analyzing and comparing characters. Due to the nature of the trace operation, analyzing characters of representation can lead to more powerful conclusions. The main two reasons come down to the matrix-similarity-preserving property of the trace operation. Firstly, representations need not be unitary, and as a result, any conclusion about an irreducible representation will be basis-independent (in construction of matrix or operator). Secondly, we can utilize the structure of the given group to identify how the character function takes values on multiple group elements simultaneously. 

\begin{definition}
	Let $G$ be a group. Two elements, $g,\hspace{1mm}h \in G$ are said to be \textbf{conjugate} to one another if $\exists k \in G$ such that $g = khk^{-1}$.
\end{definition}

Conjugacy is an equivalence relation, we we can use this relation to partition our group into different subsets.

\begin{definition}
	Let $G$ be a group and let $g \in G$. The \textbf{conjugacy class} of $g$ is defined in the following way:
$$C_g = \{h\in G \mid g = khk^{-1} \text{ for some } k \in G \}$$
\end{definition}

Conjugacy classes are especially relevant in the contexts of representation theory because the character operation on representation is invariant on conjugacy classes.

\begin{theorem}
	Let $G$ be a group, $\phi$ be a representation of $G$, and $g\in G$. Then 
$$\chi^\phi(g) = \chi^\phi(h) \hspace{1mm}\forall h \in C_g$$
For future reference, we will denote $\chi^\phi_{C_g}$ to be the character of the representation $\phi$ on the conjugacy class $C_g$
\end{theorem}
\noindent (Pf.) Since $h\in C_g$, $\exists k\in G$ such that $g = khk^{-1}$. So 

\begin{equation}
	\begin{aligned}
		\chi^\phi(g) &= trace(\phi(g)) \\
					&= trace(\phi(khk^{-1}))\\ 
					&= trace(\phi(k)\phi(h)\phi(k^{-1})) \\
					&= trace(\phi(k^{-1})\phi(k)\phi(h)) \\
					&= trace(\phi(h)) \\
					&= \chi^\phi(h) \hspace{1mm}\cite{Mendes}\hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

We can use the character of representations to develop a similar set of condition as we have developed on representations themselves to give us a little more information about while relaxing the condition of unitarity.

\begin{theorem}
	Let $G$ be a group and let $\phi$ be a representation of $G$. Let $C$ be some conjugacy class of $G$. Then 
$$\sum_{g\in C} \phi(g) =\frac{|C|}{n_\phi} \chi^\phi_C I_{n_\phi}$$
where $\chi^\phi_C$ is the character of $\phi$ on the conjugacy class $C$.
\end{theorem}
\noindent (Pf.) Let $A\coloneq \sum_{g\in C} \phi(g)$. It is easy to see that for any $h\in G$, $\phi(h)A\phi(h)^{-1} = A$ due to the nature of conjugacy classes. Therefore, by \textbf{Schur's Theorem}, $A = \lambda I_{n_\phi}$ for some $\lambda \in \C$. Taking the trace of both sides of this equation, we get 
\begin{equation}
	\begin{aligned}
		|C| * \chi_C^\phi = n_\phi * \lambda \hspace{1mm} \cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

With this theorem, we are armed to establish the orthonormality and completeness relation for characters of representations.

\begin{theorem}
	Let $G$ be a group, let $C_G$ be the set of all distinct conjugacy classes of $G$. Let $\Phi$ be the set of all inequivalent irreducible representations of $G$ and let $\phi,\psi\in\Phi$. Then the following equation is defined to be the \textbf{orthoronomality relation of irreducible characters of representations}.

$$\sum_{C \in C_G} \frac{|C|}{|G|} \chi^{\phi}_C \overline{\chi^{\psi}_C } = \begin{cases}
																1 & if \hspace{1mm} \phi = \psi \\
																0 & else
															\end{cases}$$
\end{theorem}
\noindent (Pf.) Following from \textbf{Theorem 1.29}, we have 
$$\frac{n_\phi}{|G|} \sum_{g\in G} \left[\phi_g \right]_{ij} \left[\psi_g^\dag \right]_{kl} = \begin{cases}
																						1 & \text{if } \psi = \phi,\hspace{1mm} j=k,\hspace{1mm} \text{and } i=l\\
																						0 & else
																					 \end{cases}$$
Fixing $i=j$, $k=l$, and summing over $i$ and $k$ respectively, we obtain the character of each representation in the following equality:

\begin{equation}
	\begin{aligned}
		\frac{n_\phi}{|G|} \sum_{g\in G} \chi^\phi(g) \overline{\chi^\psi}(g) = \begin{cases}
																						n_\phi & \text{if } \psi = \phi \\
																						0 & else
																					 \end{cases}
	\end{aligned}
\end{equation}
Observing the fact that the character operation is constant on conjugacy class, we can alter this equation to take the following equivalent form:

\begin{equation}
	\begin{aligned}
		\frac{1}{|G|} \sum_{C\in C_G} |C| \chi^\phi_C \overline{\chi^\psi_C}= \begin{cases}
																						1 & \text{if } \psi = \phi \\
																						0 & else
																					 \end{cases}
	\end{aligned}
\end{equation}
giving the desired conclusion. \cite{Tung} \qedsymbol

Now that we have established some valuable results about irreducible characters, we can establish one of the main results. It turns out that we can compare irreducible characters to the character of a generic representation to determine the number of copies of a given irreducible character in its irreducible decomposition. In other words, we can recover the coeffeicients ($\alpha_i$) from \textbf{Theorem 1.18}. In order to establish this, we first need the following useful theorems.

\begin{theorem}
	Let $M = \bigoplus_{i=1}^k A_i$ where $A_i$ is a square matrix for every $i$. Then $$trace(M) = \sum_{i=1}^k trace(A_i)$$
\end{theorem} 

\noindent (Pf.) $$M = \begin{bmatrix}
						A_1 & 0 & 0 & \hdots & 0 \\
						0 & A_2 & 0 & \hdots & 0 \\
						\vdots & \ddots & \ddots & \ddots & \vdots \\
						0 & 0 & 0 & \hdots & A_k
					\end{bmatrix}$$

$trace(M)$ is the sum of the diagonal entries in $M$, and since $M$ is a block-diagonal matrix (with blocks corresponding to $A_i$) the diagonal entries of $M$ coincide with the diagonal entries in $A_i$ for every $i$.

\begin{equation}
	\begin{aligned}
		trace(M) &= \sum_i [A_1]_{ii} + \sum_i [A_2]_{ii} + \hdots \sum_i [A_k]_{ii} &= \sum_{i=1}^k trace(A_i) \hspace{1mm}\qedsymbol
	\end{aligned}
\end{equation}

\begin{theorem}
	Let $G$ be a group, let $\phi$ be a representation of $G$, and let the irreducible decomposition of $\phi$ be denoted $\phi = T \left( \bigoplus_{i=1}^k \alpha_i *\psi_i\right) T^{-1}$ where $\{\psi_i\}_{i=1}^k$ is a set of irreducible representations of $G$. Then $$\alpha_i = \sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^{\psi_i}_C}$$
\end{theorem}

\noindent (Pf.) First, note that using \textbf{Theorem 1.36}
\begin{equation}
	\begin{aligned}
		\chi^\phi_C = \chi^{ \bigoplus_{i=1}^k \alpha_i *\psi_i}_C = \sum_{i=1}^k \alpha_i \chi^{\psi_i}_C
	\end{aligned}
\end{equation}

So, using the Orthonormality Condition of Irreducible Characters, we get 

\begin{equation}
	\begin{aligned}
\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^{\psi_i}_C} &= \sum_{C\in C_G} \frac{|C|}{|G|} \sum_{j=1}^k \alpha_j \chi^{\psi_j}_C \overline{\chi^{\psi_i}_C} &= \sum_{j=1}^k \alpha_j \sum_{C\in C_G} \frac{|C|}{|G|} \chi^{\psi_j}_C \overline{\chi^{\psi_i}_C}  &= \alpha_i \hspace{1mm} \cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

\begin{theorem}
	Let $G$ be a group, let $\phi$ be a representation of $G$. $\phi$ is an irreducible representation of $G$ if and only if $\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^\phi_C} = 1$
\end{theorem}

\noindent (Pf.) $\Rightarrow$ Use \textbf{Theorem 1.35} for immediate conclusion. 

\hspace{2mm}$\Leftarrow$ Let $\bigoplus_{i=1}^k \alpha_i\psi_i$ be the irreducible decomposition of $\phi$. Then utilizing this decomposition on the character operation and the orthonormality condition, we get the following equation.

\begin{equation}
	\begin{aligned}
		1 &=\sum_{C\in C_G} \frac{|C|}{|G|}\chi^\phi_C \overline{\chi^\phi_C} &= \sum_{i=1}^k \sum_{j=1}^k \alpha_i \overline{\alpha_j} \sum_{C\in C_G} \frac{|C|}{|G|} \chi^{\psi_i}_C \overline{\chi^{\psi_j}_C} &= \sum_{i=1}^k |\alpha_i|^2
	\end{aligned}
\end{equation}

Since the sum of square integers is equal to $1$, it must be the case that $\alpha_m = 1$ for some $m$ and $\alpha_n=0$  $\forall n \neq m$. \cite{Tung} \qedsymbol


The last two theorems give us tools to determine when representations are irreducible and determine how many copies of a given irreducible representation are present in the irreducible decomposition of a generic representation. Armed with these techniques, we can analyze a new representation.

\begin{definition}
	Let $G=\{g_i\}^n_{i=1}$ be a finite group. We define the \textbf{Left Regular Representation} to be the matrix representation given by the following formula:
$$g\stackrel{\phi}{\mapsto} [\phi_g]_{ij} = \begin{cases}
								1 & \text{if } gg_j = g_i \\
								0 & \text{else}
							\end{cases}$$
\end{definition}

One could verify that this map is a representation through explicitly showing the multiplication rule in $G$ is satisfied by the matrices in $\phi(G)$. Due to the construction of our matrices, this representation has degree $|G|$. 

\begin{theorem}
	For any finite group, $G$, the left regular representation ($\phi$) contains every irreducible representation of $G$, $\{\psi_i\}_{i=1}^k$, exactly $n_{\psi_i}$ times.
\end{theorem}

\noindent(Pf.) Let $n=|G|$. Consider the matrices in given by the representation $\phi$. We know that $\phi(e) = I_n$, but what can we say about can we say about non-identity elements of $G$? It turns out, for any $g\in G$ such that $g\neq e$, $\phi_g$ always has a diagonal full of zeros. This must be the case, since it would be impossible for $g *g_i = g_i$  for any non-identity $g$. As a result, we see that $\chi^\phi_e = n$ and $\chi^\phi_g = 0$ $\forall g \in G$ with $g \neq e$. So, for any irreducible representation, $\psi_i$, \textbf{Theorem 1.37} tells us that 
\begin{equation}
	\begin{aligned}
		\alpha_i = \sum_{C\in C_G} \frac{|C|}{|G|} \chi^\phi_C \overline{\chi_C^{\psi_i}} = \frac{1}{|G|}|G| \overline{\chi_e^{\psi_i}} = n_{\psi_i} \hspace{1mm}\cite{Tung} \hspace{1mm}\qedsymbol
	\end{aligned}
\end{equation}

\begin{corrolary}
\	For any finite group, $G$, $\sum_{\phi \in \Phi} n_\phi^2 = |G|$ where $\Phi=\{\phi \mid \phi \text{ is a distinct, irreducible representation of }G\}$.
\end{corrolary}

\noindent(Pf.) Since the left regular representation ($\phi$) has irreducible decomposition $\phi = T\left(\bigoplus_{i=1}^k n_{\psi_i}*\psi_i\right)T^{-1}$, we can use \textbf{Theorem 1.36} to show that
\begin{equation}
	\begin{aligned}
		|G| = trace(\phi(e)) = trace\left(\bigoplus_{i=1}^k n_{\psi_i}*\psi_i(e)\right) = \sum_{i=1}^k n_{\psi_i}trace(\psi_i(e)) = \sum_{i=1}^k n_{\psi_i}^2 \hspace{1mm}\cite{Tung} \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

\section{Completeness Conditions for Irreducible Representations and Characters}


\begin{theorem}
	Let $G$ be a finite group, let $\Phi = \{\phi \mid \phi$ is a unitary, irreducible representation of $G\}$, and let $n_\phi$ denote the degree of the representation $\phi$. Then for any pair $g$, $h \in G$, 
$$\sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}$$
This is referred to as the \textbf{completeness condition} of unitary, irreducible representations.
\end{theorem}

\noindent (Pf.) Referencing the construction of a $|G|$-dimensional vector space, $V$, in \textbf{Corrolary 1.30}, we can create a set of orthonormal, linearly independent vectors. Thus, for any fixed $\phi$, $i$, $j$, the corresponding orthonormal vector is defined by:

\begin{equation}
	\begin{aligned}
		e_{\phi,i,j} \coloneq \sqrt{\frac{n_\phi}{|G|}}\left([\phi_{g_1}]_{ij},\hspace{1mm} [\phi_{g_2}]_{ij}, \hspace{1mm}\hdots \hspace{1mm},\hspace{1mm} [\phi_{g_{|G|}}]_{ij}\right)
	\end{aligned}
\end{equation}

As a result of our work in \textbf{Corrolary 1.41}, we can see the set $\{e_{\phi,i,j}\}_{\phi,i,j}$ is in fact an orthonormal basis of our vector space, given that the dimension of our vector space is now identically 
$$n \coloneq |G| = \sum_{\phi\in \Phi} n_\phi^2$$

Therefore, every $v\in V$ has the following orthogonal decomposition:
\begin{equation}
	\begin{aligned}
		v &= \sum_{\phi,i,j} \langle v , e_{\phi,i,j} \rangle e_{\phi,i,j} &= \left(\sum_{\phi,i,j} e_{\phi,i,j} \otimes e_{\phi,i,j}\right) v
	\end{aligned}
\end{equation}

where the tensor symbol is defined here as the outer-product of vectors in $V$, as realized in the following matrix:

\begin{equation}
	\begin{aligned}
		e_{\phi,i,j} \otimes e_{\phi,i,j} &= \frac{n_\phi}{|G|}\begin{bmatrix}
			[\phi_{g_1}]_{ij}\overline{[\phi_{g_1}]_{ij}} & \hdots & [\phi_{g_1}]_{ij}\overline{[\phi_{g_n}]_{ij}}\\
			\vdots & \ddots & \vdots\\
			[\phi_{g_n}]_{ij}\overline{[\phi_{g_1}]_{ij}} & \hdots & [\phi_{g_n}]_{ij}\overline{[\phi_{g_n}]_{ij}}
		\end{bmatrix}
	\end{aligned}
\end{equation}

Upon closer inspection of \textbf{Equation 1.29}, we can see that 

\begin{equation}
	\begin{aligned}
		\sum_{\phi,i,j} {|G|} \hspace{1mm}e_{\phi,i,j} \otimes e_{\phi,i,j} &= I_n
	\end{aligned}
\end{equation}

or equivalently, for any given $x$, $y \in \{1,\hdots,|G|\}$ 
\begin{equation}
	\begin{aligned}
		\sum_{\phi,i,j} \frac{n_\phi}{|G|} [\phi_{g_x}]_{ij}[\phi^\dag_{g_y}]_{ji} &= \sum_{\phi,i,j} \left[e_{\phi,i,j} \otimes e_{\phi,i,j}\right]_{xy} &= \left[\sum_{\phi,i,j} e_{\phi,i,j} \otimes e_{\phi,i,j}\right]_{xy} &= \begin{cases}
																	1 & \text{if } x=y\\
																	0 & \text{else}
																\end{cases}
	\end{aligned}
\end{equation}
which is exactly what our completeness relation is defined to be. \qedsymbol \\







\begin{theorem}
Let $G$ be a group, let $C_G$ be the set of all distinct conjugacy classes of $G$. Let $\Phi$ be the set of all inequivalent irreducible representations of $G$. Then for any $g,h \in G$, the following equation is defined to be the \textbf{completeness relation of irreducible characters of representations}.
$$\sum_{\phi \in \Phi} \frac{|C_g|}{|G|} \chi^{\phi}_{C_g} \overline{\chi^{\phi}_{C_h}} = \begin{cases}
																1 & if \hspace{1mm} C_g = C_h \\
																0 & else
															\end{cases}$$
\end{theorem}

\noindent (Pf.) Utilizing \textbf{Theorem 1.37}, we see that for any $g, h \in G$, $$\sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}$$
Taking this equality and summing both sides over the entire conjugacy class of $g$ and $h$ respectively, we aquire
\begin{equation}
	\begin{aligned}
		\sum_{g \in C_g} \sum_{h\in C_h} \sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{n_\phi}{|G|} \left[\phi_g\right]_{ij}\left[\phi_{h}^\dag\right]_{ji} = \sum_{g \in C_g} \sum_{h\in C_h} \begin{cases}
																										1 & if \hspace{1mm} g = h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}
Utilizing \textbf{Theorem 1.35} and analyizing the $ij-th$ component of both sides of its conclusion, we can equivalently write the above equality in the following way:
\begin{equation}
	\begin{aligned}
		 \sum_{\phi \in \Phi} \sum_{i=1}^{n_\phi} \sum_{j=1}^{n_\phi} \frac{|C_g||C_h|}{|G|n_\phi}\chi^\phi_{C_g}\overline{\chi^\phi_{C_h}} \left[I_{n_\phi}\right]_{ij}\left[I_{n_\phi}\right]_{ji} = \begin{cases}
																										|C_h| & if \hspace{1mm} C_g = C_h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}

Interpreting the inner-most sum as a matrix product of both identity matrices and the second sum as spanning the diagonal of the resulting product matrix, we obtain

\begin{equation}
	\begin{aligned}
		 \sum_{\phi \in \Phi} \frac{|C_g|}{|G|}\chi^\phi_{C_g}\overline{\chi^\phi_{C_h}} = \begin{cases}
																										1 & if \hspace{1mm} C_g = C_h \\
																										0 & else
																									\end{cases}
	\end{aligned}
\end{equation}

to achieve our desired result. \cite{Tung} \qedsymbol


With our main results developed for unitary, irreducible representations and irreducible characters, we can use these ideas as motivation for our study of specific groups. While most of these results are only useful in the finite group setting, we will continue to develop equivalent conditions and conclusions for groups of infinite order. Through our analysis, we will characterize our selected groups using the techniques and tools established in this chapter as a guide.



\chapter{$SO(2)$: The Rotation Group in Two Dimensions}

The group $SO(2)$ is a very abstract structure meant to represent a very concrete physical idea. Elements of this group correspond to the action of rotating vectors in two-dimensional space about some central point. Although its construction can be more generalized, we can gleam the most important features of the group's structure are not altered in any meaningful way when care is exercise. That said, throughout this discussion, we will develop the structure of $SO2$ with the assumptions that rotations (elements) act on real-valued two-dimensional vectors who are all centered at the origin. The way we will distinguish rotations will be by the measure of the angle (traditionally denoted by $\theta$) rotated in the counter-clockwise direction. Consequently, $\theta$ will always be real-valued. Rotations will occur about the origin. Unless otherwise specified, we will always take $\{e_1,$ $e_2\}$ to be an orthonormal basis of $\R^2$.

\section{Construction and Properties}

To begin, consider $\R^2$ as a the natural two-dimensional vector space. Let $(x,y)\in\R^2$. Then, any rotation of this vector, by some angle $\theta$, in the manner described above will change the coordinates. If we refer to this new, rotated vector as $(x',y')$, we can visualize this action.

\begin{center}
            \begin{tikzpicture}[scale=1]
                \draw[step=1cm ,color=gray] (-4,-4) grid (4,4);
                \draw[thick,<->] (-4.1,0) -- (4.1,0) node[anchor=north west] {$e_1$};
                \draw[thick,<->] (0,-4.1) -- (0,4.1) node[anchor=south west] {$e_2$};
			\draw[fill=black] (0,0) coordinate (o);
			\draw[thick, ->] (0,0) -- (2,3) coordinate (a) node[anchor=south west] {$(x,y)$};
			\draw[thick, ->] (0,0) -- (-3,2) coordinate (b) node[anchor=south] {$(x',y')$};
			\pic [draw, ->, angle eccentricity=0.7, angle radius=0.8cm, "$\theta$"]{angle=a--o--b};
            \end{tikzpicture}
    \end{center}

For a more intuitive understanding of the action, we can turn our attention to the polar coordinates corresponding to these two vectors. Letting $r = \sqrt{x^2 + y^2}$, we can see that the polar form of these vectors can be written in the following way:
\begin{center}
	$\begin{aligned}
		(x,y) &\sim (r,\psi)\\
		(x',y') &\sim (r,\psi+\theta)
	\end{aligned}$
\end{center}
where $\psi$ is the angle that the vector $(x,y)$ makes with the positive $e_1$-axis. Through use of trigonometric identities, we can observe the following equations hold true:
\begin{equation}
	\begin{aligned}
		x' &= r\cos(\psi+\theta) = r\left(\cos(\psi)\cos(\theta) - \sin(\psi)\sin(\theta)\right) = \cos(\theta)*x -\sin(\theta)*y
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		y' &= r\sin(\psi+\theta) = r\left(\cos(\psi)\sin(\theta) + \sin(\psi)\cos(\theta)\right) = \sin(\theta)*x +\cos(\theta)*y
	\end{aligned}
\end{equation}

We can see that the rotated coordinates are completely determined by the originial coordinates and the angle of rotation. With the use of matrix algebra, we can combine \textbf{Equations 2.1 and 2.2} into 

\begin{equation}
	\begin{aligned}
		\begin{bmatrix}
			x' \\
			y'
		\end{bmatrix} &=
		\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}
		\begin{bmatrix}
			x \\
			y
		\end{bmatrix}
	\end{aligned}
\end{equation}

While this matrix algebra is useful, its construction is somewhat ad-hoc. Questions about the well-defined nature of this relationship are bound to be raised. In order for us to make real use of these matrices, we must first establish a bona-fide connection between the abstract notion of a rotation and the concrete matrix that acts on vectors in $\R^2$ in the same way.

We will define the group of rotations in two dimensions to be $G = \{g_\theta\}$ where $g_\theta$ represents the action of rotating vectors in $\R^2$ by angle $\theta$. It's main properties arise from physical observations, but we can give them axiomatic rigor. When rotating vectors in $\R^2$, we can see that the successive rotations by the angles $\theta_1$ and $\theta_2$ are identical to a singular rotation by the angle $\theta_1+\theta_2$. In this way, the group law must be defined in the following way: $g_{\theta_1} * g_{\theta_2} = g_{(\theta_1 + \theta_2)}$ for any $\theta_1,\theta_2\in\R$. Further, due to the abelian nature of $\R$, we can see that our group law will necessarily guarantee that $G$ is abelian:  $g_{\theta_1} * g_{\theta_2} = g_{(\theta_1 + \theta_2)}= g_{(\theta_2 + \theta_1)}= g_{\theta_2} * g_{\theta_1} \hspace{1mm}\forall \theta_1,\theta_2\in\R$. Finally, due to the physical symmetry of rotation in the path of a circle, it must be the case that $g_\theta = g_{\theta \pm 2\pi}$ for any $\theta \in \R$. The last property suggests that while $G$ is clearly an infinite group, the physical rotation corresponding to each of its elements is not unique. With these key ideas in mind we will establish the link between $G$ and the set of matrices defined in \textbf{Equation 2.3}.

\begin{theorem}
	Let $G/(g_{2\pi})$ be the quotient group defined by taking equivalence classes of physical rotations of vectors in $\R^2$ by regarding rotations that are off by integer multiples of $2\pi$ as equivalent. Let $H$ be the group defined by taking the set of all matrices of the form defined in \textbf{Equation 2.3}. together with the operation of matrix multiplication. Then the map $\phi$ (defined below) is an isomorphism of groups.
$$\phi:G/(g_{2\pi})\rightarrow H$$
$$[g_\theta] \overset{\phi}{\mapsto} \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}$$
\end{theorem}
\noindent(Pf.) 

(Homomorphism) Let $g_{\theta_1},g_{\theta_2},\in G$. Then 
\begin{equation}
	\begin{aligned}
		\phi([g_{\theta_1+\theta_2]}) &= \begin{bmatrix}
			\cos(\theta_1+\theta_2) & -\sin(\theta_1+\theta_2) \\
			\sin(\theta_1+\theta_2) & \cos(\theta_1+\theta_2)
		\end{bmatrix} \\
		&\overset{*}{=} \begin{bmatrix}
			\cos(\theta_1) & -\sin(\theta_1) \\
			\sin(\theta_1) & \cos(\theta_1)
		\end{bmatrix}
		\begin{bmatrix}
			\cos(\theta_2) & -\sin(\theta_2) \\
			\sin(\theta_2) & \cos(\theta_2)
		\end{bmatrix}\\
		&= \phi([g_{\theta_1}])\phi([g_{\theta_2}])
	\end{aligned}
\end{equation}
where a more detailed expansion for the starred($*$) equality can be found in \textbf{Example 1.4}.

(Injective) Suppose $\phi([g_\theta]) = I_2$. Then $\cos(\theta) = 1$ and $\sin(\theta) = 0$. This means that $\theta = 2\pi n$ for $n\in\Z$. Therefore, $ker(\phi) = \{[0]\}$. Note: if we instead used the group of rotations itself as our domain, we would not have a trivial kernel, since all interger multiples of $2\pi$ would have rotations that get mapped to $I_2$. For one-to-one correspondence to occur, we need to focus our attention on the quotient group (which is physically acceptable).

(Surjective) Let $\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}\in H$. Then clearly, $\theta$ is real-valued and $g_\theta\in G$ is a well-defined rotation. Then $[g_\theta]$ is the coset of $g_\theta$ and as a result, $[g_\theta] \overset{\phi}{\mapsto} \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}$. Therefore, $im(\phi) = H$.

Thus, $\phi$ is an isomorphism of groups. \qedsymbol\\


Through this interpretation, we are justified in viewing any rotation, by angle $\theta$, as the result of a matrix multiplication. Although our map is defined on the quotient group, we can see that we lose no information for angle values outside of the range $[0,2\pi)$. That is, we can always use the equivalence of rotations in $G$ that are off by $\pm 2\pi n$ to do our algebra exclusively in the $[0,2\pi)$ range. We will refer to the matrices in $H$ as $R(\theta)$, or $R_\theta$ interchangeably, $\theta$ is the angle we rotate vectors of $\R^2$ by. 

Due to the isomophic nature of $G/(g_{2\pi})$ and $H$, $H$ inherets most of the properties of $G$. It turns out that $H$ is abelian and periodic in $\theta$ (the latter was already known by using properties of trigonometric functions). We can observe some other interesting properties about $H$ through the lens of $G$.

\begin{definition}
	Let $M$ be an $n\times n$ matrix. $M$ is said to be an \textbf{orthogonal matrix} if $$MM^\intercal=I_n$$
\end{definition}

\begin{theorem}
	For any $\theta$, $R(\theta)$ is an orthogonal matrix.
\end{theorem}
\noindent(Pf.) It would not be difficult to use a matrix product argument. However, in the spirit of the isomorphism, we will use $\phi$ from \textbf{Theorem 2.1} to articulate the argument. It is clear that for any $g_\theta \in G$, $g_\theta^{-1} = g_{-\theta}$. Therefore, $[g_\theta]^{-1} = [g_{-\theta}]$. So, for any $\theta$, 
\begin{equation}
	\begin{aligned}
		I_2 &= \phi([g_\theta][g_\theta]^{-1})\\
		 &= \phi([g_\theta][g_{-\theta}]) \\ 
		 &= \phi([g_\theta])\phi([g_{-\theta}])\\
		 &= \begin{bmatrix}
			\cos(\theta) & \sin(\theta) \\
			-\sin(\theta) & \cos(\theta)
		\end{bmatrix}
\begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{bmatrix}\\
		 &= R(\theta)R(\theta)^\intercal
	\end{aligned}
\end{equation}
where the computation of $\phi([g_{-\theta}])$ uses the facts that cosine is an even function and sine is an odd function. Therefore, $R(\theta)^{-1} = R(\theta)^\intercal$. \qedsymbol

While not explicitly stated previously, rotations are practically understood to be an action that preserves a vector's magnitude. More rigorously put, if $(x,y)$ is rotated to the vector $(x',y')$, $x^2+y^2 = x^{\prime^2}+y^{\prime^2}$. The matrices in $H$ also satisfy this property. After a vector in $\R^2$ is multiplied by a matrix in $H$, its magnitude is preserved. This property is both a result of the orthogonal nature of these matrices and another property which we will explore below.

\begin{definition}
	Let $M$ be an $n\times n$ matrix. $M$ is said to be \textbf{special} if $$\det(M)=1$$
\end{definition}

\begin{theorem}
	For any $\theta$, $R(\theta)$ is a special matrix.
\end{theorem}
\noindent(Pf.)
\begin{equation}
	\begin{aligned}
		\det(R(\theta)) &= \cos^2(\theta) - (-\sin^2(\theta)) &= 1 \hspace{1mm} \qedsymbol
	\end{aligned}
\end{equation}

Given that elements of the group $H$ are special, orthogonal matrices that act as rotations on $\R^2$, it seems fitting that we give this group a more explicit name. We call this group $SO(2)$, and we will henceforth treat its elements as the rotations that we discussed in the construction of our group $G$. $SO(2)$ is our two-dimensional rotation group.

Since $SO(2)$ is clearly an infinite group, it would be incredibly helpful to determine its generators (if any exist). As it turns out, $SO(2)$ has more structure than we have already covered which will be useful to determining if a generator exists.

\begin{definition}
	A \textbf{Lie Group} is a group that is a finite-dimensional smooth manifold in which the operations of multiplication and inversion are differentiable.
\end{definition}

It turns out that $SO(2)$ is actually a Lie Group. This comes from the idea that as we smoothly vary the parameter for $\theta$, the matrices $R(\theta)$ also vary smoothly (arising from the differentiability of the sine and cosine). If there were to be a generator for this group, it would have to be a generator that represents an arbitrarily small rotation.

Consider then a rotation by angle $d\theta$ (arbitrarily small). The element in $SO(2)$ corresponding to this rotation would need to differ from the identity matrix by an arbitrarily small amount. In other words:

\begin{equation}
	\begin{aligned}
		R(d\theta) \coloneq I_2 + (-i)d\theta * J
	\end{aligned}
\end{equation}  

where $J$ is some fixed matrix, the factor of $-i$ is given as a convention, and the factor of $d\phi$ forces the change to $I_2$ to be arbitrarily small. It will turn out that $J$ will be the generator of our group. With this in mind, for any $\theta$, we can make the following observations:

\begin{equation}
	\begin{aligned}
		R(\theta + d\theta) = R(\theta)R(d\theta) = R(\theta)\left(I_2 + (-i)d\phi * J\right) = R(\theta) - id\theta R(\theta)J
	\end{aligned}
\end{equation}  
\begin{equation}
	\begin{aligned}
		R(\theta + d\theta) = R(\theta) + \frac{d}{d\theta}R(\theta)d\theta
	\end{aligned}
\end{equation}  

Combining these two equations, we uncover the differential equation:

\begin{equation}
	\begin{aligned}
		\frac{d}{d\theta}R(\theta) = - i R(\theta)J
	\end{aligned}
\end{equation}  

This differential equation can be converted into an initial value problem using the following condition: $R(0) = I_2$. This initial value problem has the solution

\begin{equation}
	\begin{aligned}
		R(\theta) = e^{-i\theta J} = \sum_{i=0}^\infty \frac{-i\theta J}{n!}
	\end{aligned}
\end{equation}  

where we interpret our solution in the following way:

\begin{equation}
	\begin{aligned}
		e^{-i\theta J} = \sum_{i=0}^\infty \frac{\left(-i\theta J\right)^n}{n!} = I_2 -i\theta J - \frac{\theta^2}{2} J^2 + \frac{i\theta^3}{3!}J^3 \hdots 
	\end{aligned}
\end{equation}  

Given this formulation, we say that $J$ is the generator of $SO(2)$, since any rotation matrix can be written in terms of this exponential. We can explicitly compute this matrix given its relationship outlined in \textbf{Equation 2.7}. Letting $J = [j]_{ij}$,

\begin{equation}
	\begin{aligned}
		\begin{bmatrix}
			1 & 0 \\
			0 & 1
		\end{bmatrix} + (-i)d\theta * \begin{bmatrix}
			j_{11} & j_{12} \\
			j_{21} & j_{22}
		\end{bmatrix} = R(d\theta) = \begin{bmatrix}
			\cos(d\theta) & -\sin(d\theta) \\
			\sin(d\theta) & \cos(d\theta)
		\end{bmatrix} = 
		\begin{bmatrix}
			1 & -d\theta \\
			d\theta & 1
		\end{bmatrix}
	\end{aligned}
\end{equation}

When comparing both sides entry by entry, we see that 

\begin{equation}
	\begin{aligned}
		J = \begin{bmatrix}
			0 & -i \\
			i & 0
		\end{bmatrix}
	\end{aligned}
\end{equation}

This matrix turns out to have the property that its square is the identity, making expansion in \textbf{Equation 2.12} much simpler.

\begin{equation}
	\begin{aligned}
		R(\theta) = \hdots = I_2 -i\theta J - \frac{\theta^2}{2} I_2 + \frac{i\theta^3}{3!}J \hdots = I_2 \cos(\theta) -iJ \sin(\theta) 
	\end{aligned}
\end{equation}

In this fashion, we can express any rotation matrix in $SO(2)$ in terms of the generator. \\

While this construction for rotation matrices is satisfactory, we can recast these concepts when considering these matrices as their corresponding linear operators. Letting $U_\theta$ be the linear operator corresponding to the matrix $R(\theta)$, we construct a generator for the group of rotation operators with the following assertion

\begin{equation}
	\begin{aligned}
		U_{d\theta} \coloneq I + (-i)d\theta * J
	\end{aligned}
\end{equation}  

where $J$ can be shown to be the generator of the group of operators. Tracing the steps, we see that 

\begin{equation}
	\begin{aligned}
		U_\theta = e^{-i\theta J} = \sum_{i=0}^\infty \frac{-i\theta J}{n!}
	\end{aligned}
\end{equation}  

%Unfortunately, since we do not have an explicit formula (or do we? J(v1,v2) = (iv1,-iv2) ? Ask Prof.)

We will use this formulation extensively in the next section.


\section{Irreducible Representations for $SO(2)$}

In the previous section, we illustrated many properties of $SO(2)$, including the fact that is is abelian. According to \textbf{Corrolary 1.24}, this means that all irreducible representations must be degree one. Recalling that in order for a representation to be irreducible, the only invariant subspaces must be trivial, we naturally turn our attention to eigenvalues of our generator operator. Let $v$ be an eigenvector of $J$ corresponding to eigenvalue $\lambda$. Then for any rotation operator, $U_\theta$,


\begin{equation}
	\begin{aligned}
		U_\theta (v) = \left(\sum_{i=0}^\infty \frac{-i\theta J}{n!}\right) (v) = \left(\sum_{i=0}^\infty \frac{-i\theta \lambda}{n!}\right)v = \left(e^{-i\theta \lambda}\right)v
	\end{aligned}
\end{equation}

Therefore, $v$ is an eigenvector of every rotation operator corresponding to eigenvalue $e^{-i\theta \lambda}$. Eigenvalues serve as a great tool for constructing degree one representations. Throughout the rest of this thesis, we will always start with this as our baseline.

It can clearly be seen that for any two rotation operators, $U_{\theta_1}$ and $U_{\theta_2}$,  
\begin{equation}
	\begin{aligned}
		U_{\theta_1}(U_{\theta_2}(v)) = U_{\theta_1}(e^{-i\theta_2 \lambda}v) = e^{-i\theta_1 \lambda}e^{-i\theta_2 \lambda}v = e^{-i(\theta_1 + \theta_2) \lambda}v = U_{\theta_1+\theta_2}(v) 
	\end{aligned}
\end{equation}

the group operation of composition (and therefore, corresponding matrix multiplication) respect our eigenvalues. But in order for the representation to be a true homomorphism, we need $2\pi n \mapsto 1$ for any $n\in\Z$. This quickly forces a restriction on $\lambda$ in the following way:

\begin{equation}
	\begin{aligned}
		e^{-i2\pi n\lambda} = 1 \Rightarrow \lambda \in \Z
	\end{aligned}
\end{equation}

With this restriction in mind, we can define an irredcible representation for $SO(2)$ with each choice of integer in the following way:

$$\begin{aligned}
	\phi_m:SO(2)\rightarrow \C \\
	U_\theta \mapsto e^{-i\theta m} \\
\end{aligned}$$

We can also show that these representations are unitary. Consider a one dimensional inner-product space one which the operators/matrices $\phi_m$ are defined upon. Letting $x,y\in V$, consider the following argument:

\begin{equation}
	\begin{aligned}
		\langle \phi_m(x) , \phi_m(y) \rangle &= \langle e^{-i\theta m}*x , e^{-i\theta m}*y \rangle \\
												&= e^{-i\theta m}*\overline{e^{-i\theta m}} \langle x , y \rangle\\
												&=\langle x , y \rangle
	\end{aligned}
\end{equation}

Therefore, $\phi_m$ is unitary for every $m\in\Z$. \\

Now that we have our unitary, irreducible representations, we turn our attention to casting these representations into our orthonormality and completeness conditions. However, those conditions were defined over finite groups, and the sums that characterize those conditions are not well-defined. Further, elements of $SO(2)$ are characterized by a continuous variable, making the number of elements uncountable. Any kind of natural generalization of these condition would need to utilize integration in some meaningful way. However, as of yet, integration is not well-defined either, since we have done all of our work in a group theory setting.

We must exercise care when constructing our integration. When setting up the integration, we will do so in the context of a continuous parameterization of the group elements. However, we want to ensure that the value of the integral is the same for any parameterization we choose. That is to say, for any representation, $f$, and two nonequivalent parameterizations ($\theta$ and $\psi$) of the interval $[0,2\pi]$, we should have the following:

\begin{equation}
	\begin{aligned}
		\int_G f(g) dg = \int_0^{2\pi}f(g)\rho_{g}(\theta)  d\theta
	\end{aligned}
\end{equation}

where $\rho_g(\theta)$ is a weight function. Therefore, we can think of $dg$ as being equivalent to $\rho_g(\theta)  d\theta$. More explicitly, any integration should respect the group multiplication law. That is, for any fixed $h\in G$, we should have

\begin{equation}
	\begin{aligned}
		\int_G f(g) dg = \int_G f(g) d(hg)
	\end{aligned}
\end{equation}

To this end, we will construction "group integration" in the following way:

\begin{definition}
	Let $g_\theta$ parameterize group elements in $G$ and let $\rho_{g}$ be a weight function corresponding to elements of this group. This parametrization of the group and weight function are said to provide an \textbf{invariant integration measure} if for any fixed $h\in G$, $$dg = d(hg)$$ or more explicitly, $$\rho_{g}[d\theta]_g = \rho_{hg}[d\theta]_{hg}$$
\end{definition}

If we choose the weight function to that satisfied the following equation

\begin{equation}
	\begin{aligned}
		\rho_{g}(\theta) = \frac{[d\theta]_e}{[d\theta]_g}
	\end{aligned}
\end{equation}
 
then we can see that the above condition is quickly satisfied by the following trick:


\begin{equation}
	\begin{aligned}
		\rho_g =  \frac{[d\theta]_e}{[d\theta]_g} =  \frac{[d\theta]_e}{[d\theta]_{hg}}\frac{[d\theta]_{hg}}{[d\theta]_g} = \rho_{hg}\frac{[d\theta]_{hg}}{[d\theta]_g}
	\end{aligned}
\end{equation}

If we take $\theta$ to be the parameterization of $[0,2\pi)$ that gives us the exact value of the rotation angle, then the relationship 

\textbf{Come back and fix this utter nonsense}\\

Now that we have well defined integration, it is sensible to discuss the orthonormality and completeness relations for our irreducible (unitary) representations of $SO(2)$.

\begin{theorem}
	The irreducible (unitary) representaions of $SO(2)$ satisfy the following conditions:
\begin{equation}
	\begin{aligned}
		\frac{1}{2\pi} \int_0^{2\pi} \phi_m(\theta) \phi_n^\dag(\theta) d\theta= \begin{cases}
																			1 & \text{if } m=n \\
																			0 & \text{else}
																		\end{cases}\hspace{2mm} \text{Orthonormality Relation}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		\sum_{n\in\Z} \phi_n(\theta) \phi_n^\dag(\theta') = \begin{cases}
																			1 & \text{if } \phi=\phi' \\
																			0 & \text{else}
																		\end{cases} \hspace{2mm} \text{Completeness Relation}
	\end{aligned}
\end{equation}
\end{theorem}

We can see that the orthonormality condition is clearly true since $\{e^{im\theta}\}_{m\in\Z}$ is an orthogonal set in $C([0,2\pi])$ with each function having norm $2\pi$. The completeness condition \textbf{looks very wrong and should be double checked with professor.} \\


Are these the only irreducible representations of $SO(2)$? So far, we have defined our irreducible representations in terms of integer values. What would happen if we were to replace these integers with other numbers? For example, consider the map $\phi_{\frac{1}{2}}: U_\theta \mapsto e^\frac{i\theta}{2}$. Unfortunately, the physical implications of this map would be problematic, seeing as $\phi_\frac{1}{2}(\theta + 2\pi) \neq  \phi_\frac{1}{2}(\theta)$. However, this mapping is still periodic, with period $4\pi$. If we relax our definition of a representation, this mapping can still be of use to us.

\begin{definition}
	A \textbf{mult-valued rrepresentation} is a multi-valued mapping of a group into $GL_n(\C)$ which is a group homomorphism in the sense that at least one of the outputs (from each input) may be used to satisfy the relation.
\end{definition}

In this sense, $\phi_{\frac{1}{2}}$ is a $2$-valued representation since $\phi_{\frac{1}{2}}(U_\theta) = e^\frac{\pm i\theta}{2}$ and $\phi_{\frac{1}{2}}(U_\theta U_\psi) =  e^\frac{\pm i(\theta + \psi)}{2} =  e^\frac{\pm i\theta}{2} e^\frac{\pm i\psi}{2} = \phi_{\frac{1}{2}}(U_\theta)\phi_{\frac{1}{2}}(U_\psi)$.

Thus, for any $\frac{m}{n} \in \Q$, we can define an $m$-valued representation in the following way:

$$\begin{aligned}
	\phi_\frac{m}{n}:SO(2)\rightarrow \C \\
	U_\theta \mapsto e^\frac{-i\theta m}{n} \\
\end{aligned}$$

As it turns out, $2$-valued representations actually play a significant role in applications to physics. We will see more of this come up in later sections.

\chapter{$SO(3)$: The Rotation Group in Three Dimensions}

Similar to $SO(2)$, the group $SO(3)$ represents all possible ways to take the physical action or rotating a vector in space. However, the space we can rotate within has changed to a three-dimensional space for this group, allowing us to have far more possibilities for our rotations. While we will make some simplifying generalizations here, this group still becomes incredibly more complicated than its two-dimensional counterpart. We will assume all rotations take place in $\R^3$. Like before, we will always assume our angle of rotation is real-valued, but we do not specify yet an orientation about which positive rotation occurs. Unless otherwise indicated, we will take $\{e_1,e_2,e_3\}$ to be an orthonormal basis of $\R^3$.

\section{Construction and Properties}

If we define $SO(3)$ to act on $\R^3$ in a similar way that $SO(2)$ acts on $\R^2$, we could not as easily uncover a formulation for every matrix. Instead, we use the characteristics of $SO(2)$ to guide our intuition for a way to understand the matrices that compose $SO(3)$. Firstly, any rotation should leave the magnitude of a vector in $\R^3$ unchanged. This leads us to conclude that the matrices must be orthogonal. Further, it will also follow that these matrices have determinant $1$, since all rotations can be reached continuously from the identity. With this information alone, we can deduce that the $SO(3)$ matrices form a group under matrix multiplication. The question of classifying these matrices is still open for use to answer. There are two main ways we can do so.

First, let us consider any $n\in\R^3$. Once picking $n$, we can consider rotating about the vector, treating it as an axis, considering any positive angle measure to be a counter-clockwise rotation about this axis. Without taking into account the vector's length, the orientation of $n$ is uniquely determined by two angles, $\theta$ and $\phi$, as defined by spherical coordinates in $\R^3$. In this way, every possible rotation in $\R^3$ is characterized by one angle of counter-clockwise rotation and the two angles that determine the orientation about which we rotate (direction that $n$ points). In other words, the rotation, which we will refer to as $R_n(\psi) = R_{\theta,\phi}(\psi)$, identifies a rotation as displayed in the picture below.

\tdplotsetmaincoords{60}{120}
\tdplotsetrotatedcoords{60}{45}{225}
\begin{center}
		\begin{tikzpicture}
				[scale=1.2,
					tdplot_main_coords,
					axis/.style={->,black,thin},
					vector/.style={-stealth,black,very thick}]
		

			%draw the axes
			\draw[axis] (0,0,0) -- (3,0,0) node[anchor=west]{$x$};
			\draw[axis] (0,0,0) -- (0,3,0) node[anchor=west]{$y$};
			\draw[axis] (0,0,0) -- (0,0,3) node[anchor=west]{$z$};
		
			\coordinate (O) at (0,0,0);
			\tdplotsetcoord{P}{3}{45}{60}

		%	\draw[thick,tdplot_rotated_coords,->](0,0,0)--(.5,0,0)node[anchor=west]{$x'$};
			%\draw[thick,tdplot_rotated_coords,->](0,0,0)--(0,.5,0)node[anchor=west]{$y'$};
			%\draw[thick,tdplot_rotated_coords,->](0,0,0)--(0,0,5)node[anchor=west]{$z'$};
			\draw[dashed,color=red] (O)--(Pxy);
			\draw[dashed,color=red] (P)--(Pxy);
			%x =  y = z=2.09 r = 2.15218

			\tdplotdrawarc[->,tdplot_rotated_coords,color=blue, very thick]{(0,0,0)}{0.5}{0}{360}{anchor=south west}{$\psi$};
			\tdplotdrawarc[->]{(0,0,0)}{2.15218}{0}{60}{anchor=north}{$\theta$};
			\tdplotsetrotatedcoords{-30}{-90}{0}
			\tdplotdrawarc[->,tdplot_rotated_coords]{(0,0,0)}{2}{0}{45}{anchor=north east}{$\phi$};
			\draw[->, very thick] (O) -- (P) node[anchor=south]{$n$} ;			
		\end{tikzpicture}
\end{center}

By identifying rotations with these three parameters, we create some redundancy. Specifically, for any direction, $n$, the following equality holds:

$$R_n(\psi) = R_{-n}(2\pi - \psi)$$

as evidenced by the following picture: 

\begin{tabular}{ccccc}
	\tdplotsetmaincoords{60}{120}

			\begin{tikzpicture}
					[scale=1,
						tdplot_main_coords,
						axis/.style={->,black,thin},
						vector/.style={-stealth,black,very thick}]
			
			
				\coordinate (O) at (0,0,0);
				\coordinate (r1) at (1.414,0,1);
				\coordinate (r2) at (0,-1.414,1);
				\coordinate (r3) at (-1.414,-0,1);
				\coordinate (r4) at (0,1.414,1);
				\coordinate (r5) at (1.414,0,-1);
				\coordinate (r6) at (0,-1.414,-1);
				\coordinate (r7) at (-1.414,0,-1);
				\coordinate (r8) at (0,1.414,-1);
	
	
				\draw[axis,color=gray] (0,0,0) -- (0,0,-2) node[anchor=west]{$-n$};
				
				\draw[color=gray] (r1)--(r2);
				\draw[color=gray] (r2)--(r3);
				\draw[color=gray] (r3)--(r4);
				\draw[color=gray] (r4)--(r1);
				\draw[color=gray] (r1)--(r5);
				\draw[color=gray] (r2)--(r6);
				\draw[color=gray] (r3)--(r7);
				\draw[color=gray] (r4)--(r8);
				\draw[color=gray] (r5)--(r6);
				\draw[color=gray] (r6)--(r7);
				\draw[color=gray] (r7)--(r8);
				\draw[color=gray] (r8)--(r5);
	
				
	
			\end{tikzpicture}
	&
\begin{LARGE}
$\overset{R_{-n}(2\pi-\psi)}{\leftarrow}$
\end{LARGE}
&
\tdplotsetmaincoords{60}{120}
	%\tdplotsetrotatedcoords{60}{45}{225}
			\begin{tikzpicture}
					[scale=1,
						tdplot_main_coords,
						axis/.style={->,black,thin},
						vector/.style={-stealth,black,very thick}]
			
			
				\coordinate (O) at (0,0,0);
				\coordinate (r1) at (1,1,1);
				\coordinate (r2) at (-1,1,1);
				\coordinate (r3) at (-1,-1,1);
				\coordinate (r4) at (1,-1,1);
				\coordinate (r5) at (1,1,-1);
				\coordinate (r6) at (-1,1,-1);
				\coordinate (r7) at (-1,-1,-1);
				\coordinate (r8) at (1,-1,-1);
	
	
				\draw[axis,color=black] (0,0,0) -- (0,0,2) node[anchor=west]{$n$};
				\draw[axis,color=gray] (0,0,0) -- (0,0,-2) node[anchor=west]{$-n$};
				\draw[color=gray] (r1)--(r2);
				\draw[color=gray] (r2)--(r3);
				\draw[color=gray] (r3)--(r4);
				\draw[color=gray] (r4)--(r1);
				\draw[color=gray] (r1)--(r5);
				\draw[color=gray] (r2)--(r6);
				\draw[color=gray] (r3)--(r7);
				\draw[color=gray] (r4)--(r8);
				\draw[color=gray] (r5)--(r6);
				\draw[color=gray] (r6)--(r7);
				\draw[color=gray] (r7)--(r8);
				\draw[color=gray] (r8)--(r5);

				\tdplotsetrotatedcoords{225}{180}{0}

				\coordinate (Shift1) at (0,0,1);
				\tdplotsetrotatedcoordsorigin{(Shift1)};
				\tdplotdrawarc[->,tdplot_rotated_coords,color= blue, thick]{(0,0,0)}{1.414}{0}{45}{anchor=south}{$\psi$};

				\coordinate (Shift2) at (0,0,-1);
				\tdplotsetrotatedcoordsorigin{(Shift2)};
				\tdplotdrawarc[->,tdplot_rotated_coords,color= bluegray, thick]{(0,0,0)}{1.414}{0}{-315}{anchor=north}{$2\pi-\psi$};
				\draw[dashed, color=black] (1.414,0,-1.5) -- (1.414,0,2);
	

				
	
			\end{tikzpicture}	


&
\begin{LARGE}
$\overset{R_n(\psi)}{\rightarrow}$
\end{LARGE}
&
\tdplotsetmaincoords{60}{120}
	%\tdplotsetrotatedcoords{60}{45}{225}
			\begin{tikzpicture}
					[scale=1,
						tdplot_main_coords,
						axis/.style={->,black,thin},
						vector/.style={-stealth,black,very thick}]
			
			
				\coordinate (O) at (0,0,0);
				\coordinate (r1) at (1.414,0,1);
				\coordinate (r2) at (0,-1.414,1);
				\coordinate (r3) at (-1.414,-0,1);
				\coordinate (r4) at (0,1.414,1);
				\coordinate (r5) at (1.414,0,-1);
				\coordinate (r6) at (0,-1.414,-1);
				\coordinate (r7) at (-1.414,0,-1);
				\coordinate (r8) at (0,1.414,-1);
	
	
				\draw[axis,color=black] (0,0,0) -- (0,0,2) node[anchor=west]{$n$};
				\draw[color=gray] (r1)--(r2);
				\draw[color=gray] (r2)--(r3);
				\draw[color=gray] (r3)--(r4);
				\draw[color=gray] (r4)--(r1);
				\draw[color=gray] (r1)--(r5);
				\draw[color=gray] (r2)--(r6);
				\draw[color=gray] (r3)--(r7);
				\draw[color=gray] (r4)--(r8);
				\draw[color=gray] (r5)--(r6);
				\draw[color=gray] (r6)--(r7);
				\draw[color=gray] (r7)--(r8);
				\draw[color=gray] (r8)--(r5);
	
				
	
			\end{tikzpicture}


\end{tabular}

This is truly a consequence of the fact that positive (counter-clockwise) rotation is relative to the direction one faces. Consider placing yourself in the center of the square at the point where the two vectors touch. Looking both up and down will show that the angle of rotation is in the counter-clockwise direction. We can match every rotation in one direction with an opposite direction rotation by another angle using this relationship. In order to keep ourselves from double-counting, we can restrict the values of our parameters to conform to the following convention: 

$$0\leq\psi\leq\pi, \hspace{3mm} 0\leq\theta\leq2\pi, \hspace{3mm} 0\leq\phi\leq\pi$$

While we still have the $\psi=\pi$ case to worry about, this greatly limits the amount of rotations we double count, and is a satisfactoy convention. A useful way to visualize each rotation is to view it as a point in $\R^3$ as an ordered triple in spherical coordinates $(\psi,\phi,\theta)$. $\phi$ and $\theta$ are defined conventionally as they are, but $\psi$ takes the place of the value for the distance from the origin. In this way, we identify every point in the sphere of radius $\pi$ centered at the origin with a rotation in $\R^3$ \\

\textbf{Make a pretty picture} \\

We can use the geometry of three-dimensional space to observe a very simple but powerful fact.

\begin{theorem}
	Let $n, n'\in\R^3$ and $\psi\in[0,\pi]$. Let $R$ be a rotation that takes $n$ and points it in the direction of $n'$. Then, following identity holds:
$$R_{n}(\psi) = R^{-1}R_{n'}(\psi)R$$
\end{theorem}

This identity can be easily verified as seen in the following sequence of images\\

\textbf{Make a pretty picture}\\

With this identifty in mind, we can make a deeper conclusion about the inherent structure of $SO(3)$.

\begin{corrolary}
	All rotations (about any axis) by a fixed angle $\psi$ share a conjugacy class (denoted $C_\psi$).
\end{corrolary}

These classifications of rotations give us our first way of understanding rotations in $SO(3)$. There is another equally useful way to interpret any rotation in $\R^3$. Suppose our coordinate axes are defined by the following convention: $(x,y,z)$. A rotation in $SO(3)$ will change the direction that these axes point in, but still will preserve the inherent mutual orthogonality of the directions. We will refer to the initial axes as the "fixed frame" and the rotated axes, $(x',y',z')$, as the rotated frame. The natural question would be to ask how we can characterize the transition from the fixed frame to the rotated frame.

\begin{definition}
	Let $(x,y,z)$ be the fixed frame and let $(x',y',z')$ be any rotated frame. Then this rotation's \textbf{Euler Angles} decompose the rotation in the following way:
$$R(\alpha,\beta,\gamma) \coloneq R_{z'}(\gamma)R_n(\beta)R_z(\alpha)$$
where $n$ points in the direction defined by the intersection of the $xy$ and $x'y'$ planes (or equivalently, the $z\times z'$ direction). Note: we use the subscript $z$ and $z'$ as a shorthand to mean the $z$-axis and $z'$-axis.
\end{definition}

In this way, we view every rotation in $SO(3)$ as the composition of three rotations about mutually orthogonal axes. Conventionally, we restrict the parameters to the followign conditions: $\alpha,\gamma\in[0,2\pi)$, $\beta\in[0,\pi]$. This limits double-counting in a similar manner to the discussion above. The figure below illustrates the use of Euler angles to complete an arbitrary rotation as the composition of three determined rotations.\\

\begin{figure}[H]
	\centering
	\subfloat[Fixed Frame, Rotated]
		{
		\tdplotsetmaincoords{60}{200}
		\tdplotsetrotatedcoords{60}{45}{225}
				\begin{tikzpicture}
						[scale=2,
							tdplot_main_coords,
							axis/.style={->,black,thin},
							vector/.style={-stealth,black,very thick}]
					\draw[axis,color=gray] (0,0,0) -- (1,0,0) node[anchor=south]{$x$};
					\draw[axis,color=gray] (0,0,0) -- (0,1,0) node[anchor=west]{$y$};
					\draw[axis] (0,0,0) -- (0,0,1) node[anchor=east]{$z$};
					\draw[axis,color=blue] (0,0,0) -- (-0.707106781186548,0.707106781186548,0) node[anchor=west]{$z'$}; 
					\tdplotcrossprod(-0.707106781186548,0.707106781186548,0)(0,0,1);
					\draw[axis,color=red] (0,0,0)--(\tdplotresx,\tdplotresy,\tdplotresz) node[anchor=south]{$n$};
					\tdplotdrawarc[->]{(0,0,0)}{1.2}{0}{315}{anchor=north}{$\alpha$};
				\end{tikzpicture}
		}\\
	\subfloat[$R_z(\alpha)$ applied to Fixed Frame]
		{
		\tdplotsetmaincoords{60}{200}
		\tdplotsetrotatedcoords{45}{90}{90}
				\begin{tikzpicture}
						[scale=2,
							tdplot_main_coords,
							axis/.style={->,black,thin},
							vector/.style={-stealth,black,very thick}]
					\draw[axis,color=lightgray] (0,0,0) -- (1,0,0) node[anchor=south]{$x$};
					\draw[axis,color=black] (0,0,0) -- (0.7071,-0.7071,0) node[anchor=north]{$x$};
					\draw[axis,color=lightgray] (0,0,0) -- (0,1,0) node[anchor=west]{$y$};
					\draw[axis,color=black] (0,0,0) -- (0,0,1) node[anchor=east]{$z$};
					\draw[axis,color=blue] (0,0,0) -- (-0.707106781186548,0.707106781186548,0) node[anchor=east]{$z'$};
					\tdplotcrossprod(-0.707106781186548,0.707106781186548,0)(0,0,1);
					\draw[axis,color=wildwatermelon] (0,0,0)--(\tdplotresx,\tdplotresy,\tdplotresz) node[anchor=south]{$y$};
					\tdplotdrawarc[->,color=gray]{(0,0,0)}{1.2}{0}{315}{anchor=south east}{$\alpha$};
					\tdplotdrawarc[<-,tdplot_rotated_coords,color=red]{(0,0,0)}{1.2}{0}{90}{anchor=south west}{$\beta$};
				\end{tikzpicture} 
		} \\
		\subfloat[$R_n(\beta)R_z(\alpha)$ applied to fixed frame]
		{
		\tdplotsetmaincoords{60}{200}
		\tdplotsetrotatedcoords{45}{90}{90}
	
				\begin{tikzpicture}
						[scale=2,
							tdplot_main_coords,
							axis/.style={->,black,thin},
							vector/.style={-stealth,black,very thick}]
				
		

					\draw[axis,color=lightgray] (0,0,0) -- (0.7071,-0.7071,0) node[anchor=north]{$x$};
					\draw[axis,color=black] (0,0,0) -- (0,0,1) node[anchor=south]{$x$};
					%\draw[axis,color=lightgray] (0,0,0) -- (0,1,0) node[anchor=west]{$y$};
					%\draw[axis,color=lightgray] (0,0,0) -- (-0.866,0.5,0) node[anchor=west]{$y$};
					%\draw[axis,color=black] (0,0,0) -- (-0.183,-0.183,-0.96591) node[anchor=west]{$y$};
					%\draw[axis,color=lightgray] (0,0,0) -- (0,0,1) node[anchor=east]{$z$};
		
	
					\draw[axis,color=cobalt] (0,0,0) -- (-0.707106781186548,0.707106781186548,0) node[anchor=west]{$z'$}; %(-sqrt(2)/2,sqrt(2)/2,0)
		
					\tdplotcrossprod(-0.707106781186548,0.707106781186548,0)(0,0,1);
					\draw[axis,color=wildwatermelon] (0,0,0)--(\tdplotresx,\tdplotresy,\tdplotresz) node[anchor=south]{$y$};
		
					\tdplotdrawarc[->,color=lightgray]{(0,0,0)}{1.2}{0}{315}{anchor=south east}{$\alpha$};
					\tdplotdrawarc[<-,tdplot_rotated_coords,color=classicrose]{(0,0,0)}{1.2}{0}{90}{anchor=south west}{$\beta$};
					\tdplotsetrotatedcoords{135}{90}{180}
					\tdplotdrawarc[->,tdplot_rotated_coords,color=cobalt]{(0,0,0)}{1.2}{0}{315}{anchor=south}{$\gamma$};
				\end{tikzpicture}
		}\\
		\subfloat[Final Rotated Frame]
		{
		\tdplotsetmaincoords{60}{200}
		\tdplotsetrotatedcoords{45}{90}{90}
	
				\begin{tikzpicture}
						[scale=2,
							tdplot_main_coords,
							axis/.style={->,black,thin},
							vector/.style={-stealth,black,very thick}]
					\draw[axis,color=lightgray] (0,0,0) -- (1,0,0) node[anchor=north]{$x$};
					\draw[axis,color=black] (0,0,0) -- (-0.5,-0.5,0.70711) node[anchor=south]{$x'$};
					\draw[axis,color=lightgray] (0,0,0) -- (0,1,0) node[anchor=west]{$y$};
					\draw[axis,color=wildwatermelon] (0,0,0) -- (0.5,0.5,0.70711) node[anchor=north east]{$y'$};
					\draw[axis,color=lightgray] (0,0,0) -- (0,0,1) node[anchor=east]{$z$};
					\draw[axis,color=cobalt] (0,0,0) -- (-0.707106781186548,0.707106781186548,0) node[anchor=east]{$z'$}; 
					\tdplotcrossprod(-0.707106781186548,0.707106781186548,0)(0,0,1);

		
					\tdplotdrawarc[->,color=lightgray]{(0,0,0)}{1.2}{0}{315}{anchor=north west}{$\alpha$};
					\tdplotdrawarc[<-,tdplot_rotated_coords,color=classicrose]{(0,0,0)}{1.2}{0}{90}{anchor=north west}{$\beta$};
					\tdplotsetrotatedcoords{135}{90}{180}
					\tdplotdrawarc[->,tdplot_rotated_coords,color=columbiablue]{(0,0,0)}{1.2}{0}{315}{anchor=south}{$\gamma$};
				
				\end{tikzpicture}
		}
	\caption{Euler Angles}
\end{figure}
\begin{figure}[H]
	\ContinuedFloat
	\subfloat[Finding $n$]
		{
		\tdplotsetmaincoords{60}{200}
		\tdplotsetrotatedcoords{45}{90}{90}
	
				\begin{tikzpicture}
						[scale=2,
							tdplot_main_coords,
							axis/.style={->,black,thin},
							vector/.style={-stealth,black,very thick}]
					\draw[axis,color=lightgray] (0,0,0) -- (1,0,0) node[anchor=north]{$x$};
					\draw[axis,color=black] (0,0,0) -- (-0.5,-0.5,0.70711) node[anchor=south]{$x'$};
					\draw[axis,color=lightgray] (0,0,0) -- (0,1,0) node[anchor=west]{$y$};
					\draw[axis,color=black] (0,0,0) -- (0.5,0.5,0.70711) node[anchor=north east]{$y'$};
					\draw[axis,color=lightgray] (0,0,0) -- (0,0,1) node[anchor=east]{$z$};
					\draw[axis,color=black] (0,0,0) -- (-0.707106781186548,0.707106781186548,0) node[anchor=east]{$z'$}; 
					\draw[axis,color=red] (0,0,0) -- (0.707106781186548,0.707106781186548,0) node[anchor=south]{$n$}; 

					\coordinate (p1) at (1.1,1.1,0);
					\coordinate (p2) at (-1.1,1.1,0);
					\coordinate (p3) at (-1.1,-1.1,0);
					\coordinate (p4) at (1.1,-1.1,0);
					\coordinate (p5) at  (1.1,1.1,1);
					\coordinate (p6) at (1.1,1.1,-1);
					\coordinate (p7) at (-1.1,-1.1,-1);
					\coordinate (p8) at (-1.1,-1.1,1);



					\draw[color=lightgray] (p1) -- (p2);
					\draw[color=lightgray] (p2) -- (p3);
					\draw[color=lightgray] (p3) -- (p4);
					\draw[color=lightgray] (p4) -- (p1);
					\draw[color=black] (p5) -- (p6);
					\draw[color=black] (p6) -- (p7);
					\draw[color=black] (p7) -- (p8);
					\draw[color=black] (p8) -- (p5);
					\draw[dashed,color=red,very thin] (1.1,1.1,0)--(-1.1,-1.1,0);
				
				\end{tikzpicture}
		}
		\caption{Euler Angles}

\end{figure} 

An couple important observations can be made by observing these figures: $R_n(\beta)$ is the rotation that moves $z$ to $z'$ and $R_z(\alpha)$ is the rotation that moves $y$ to $n$. Using this fact and \textbf{Theorem 3.1}, we can conclude the following:

\begin{equation}
	\begin{aligned}
		R_{z'}(\gamma) = R_n(\beta)R_z(\gamma)R_n(\beta)^{-1}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		R_n(\beta) = R_z(\alpha)R_y(\beta)R_z(\alpha)^{-1}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		R(\alpha,\beta,\gamma) = R_z(\alpha)R_y(\beta)R_z(\gamma)
	\end{aligned}
\end{equation}
where \textbf{Equation 3.3} comes from applying \textbf{Equations 3.1-2} to \textbf{Definition 3.3}. This result tells us that every rotation can be categorized three angles we use to rotate about the $y$ and $z$ axis. While any orthogonal basis can serve as our fixed frame, we get the most use out of choosing the standard basis. In doing so, we focus our attention to $xy$-plane and the $xz$-planes when rotating about the $z$ and $y$ axes respectively. In these two-dimensional settings, we can construct $SO(2)$-like matrices that impact the right coordinates of vectors in $\R^3$. To this end, we see that

\begin{equation}
	\begin{aligned}
		R_z(\theta) = \begin{bmatrix}
						\cos(\theta) & -\sin(\theta) & 0 \\
						\sin(\theta) & \cos(\theta) & 0 \\
						0 & 0 & 1 \\
						\end{bmatrix}
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		R_y(\theta) = \begin{bmatrix}
						\cos(\theta) & 0& \sin(\theta) \\
						0 & 1 & 0 \\
						-\sin(\theta) &0& \cos(\theta) \\
						\end{bmatrix}
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		R_x(\theta) = \begin{bmatrix}
						1 & 0 & 0 \\
						0 & \cos(\theta) & -\sin(\theta) \\
						0 & \sin(\theta) & \cos(\theta) \\
						\end{bmatrix}
	\end{aligned}
\end{equation}

and therefore, any rotation in $SO(3)$ can be written as


\begin{equation}
	\begin{aligned}
		R(\alpha,\beta,\gamma) \\&= \begin{bmatrix}
						\cos(\alpha)\cos(\beta)\cos(\gamma) - \sin(\alpha)\sin(\gamma)& -\cos(\alpha)\cos(\beta)\sin(\gamma) -\sin(\alpha)\cos(\gamma)&  \cos(\alpha)\sin(\beta)\\
						\sin(\alpha)\cos(\beta)\cos(\gamma) + \cos(\alpha)\sin(\gamma)& -\sin(\alpha)\cos(\beta)\sin(\gamma) +\cos(\alpha)\cos(\gamma)  &  \sin(\alpha)\sin(\beta) \\
						-\sin(\beta)\cos(\gamma) & \sin(\beta)\sin(\gamma) & \cos(\beta)\\
						\end{bmatrix}
	\end{aligned}
\end{equation}

While this formulation is unpleasing, it is still useful, especially in context of writing computer algorithm.

Now that we have discussed the general form of a matrix in $SO(3)$, we can explore the concept of generators for this group. We can begin by observing that whenever we fix an axis, $n$, that we rotate about, the rotations of all angles about $n$ form a subgroup, call it $H_n$. The argument for this is as follows: Clockwise rotations about $n$ are inverses of counter-clockwise rotations about $n$ and consecutive rotations about the same axis can be done in one rotations if we first add up the angle measures. The matrices of $SO(3)$ must respect these properties or the group does not represent the physical phenomenon of rotation. This being said, we can see that fixing the axis $n$ forces all rotations to take place in the two dimensional plane normal to $n$. For this reason, we can argue that any subgroup constructed in this way must be isomorphic to $SO(2)$, taking the following map:

$$\phi:H_n \rightarrow SO(2)$$
$$R_n(\theta) \mapsto R(\theta)$$

Given this isomorphic relationship, we can construct a generator for $H_n$ in a very similar way that we did to $SO(2)$. If we derive the generator $J_n$ for each $H_n$, we will see the following must be true:

\begin{equation}
	\begin{aligned}
		R_n(\theta) = e^{i\theta J_n}
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		J_x &= \begin{bmatrix}
					0 & 0 & 0 \\
					0 & 0 & -i \\
					0 & i & 0
					\end{bmatrix}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		J_y &= \begin{bmatrix}
					0 & 0 & i \\
					0 & 0 & 0 \\
					-i & 0 & 0
					\end{bmatrix}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		J_z &= \begin{bmatrix}
					0 & -i & 0 \\
					i & 0 & 0 \\
					0 & 0 & 0
					\end{bmatrix}
	\end{aligned}
\end{equation}


\begin{theorem}
	 For any arbitrary direction, $n\in\R^3$, the generator, $J_n$, of $H_n$, can be written in the following way:
$$J_n = n_1J_x + n_2J_y + n_3J_z$$
\end{theorem}
\noindent (Pf.) If $n$ is determined by the prototypical angles $\phi$ and $\theta$, then we can always rotate the $z$-axis into $n$ with the rotation $R(\theta,\phi,0)$. Therefore, $n_i = [R(\theta,\phi,0)]_{i3}$.

\textbf{Insert Pretty Picture}

Using \textbf{Equation 3.8} and the above matrix, we can establish a relationship between $J_z$ and $J_n$. For any angle of rotation, $\psi$,

\begin{equation}
	\begin{aligned}
		e^{i\psi J_n} \\
							&= R_n(\psi)	\\
							&=R(\theta,\phi,0)R_z(\psi)R(\theta,\phi,0)^{-1}\\
							&=R(\theta,\phi,0) e^{i\psi J_z} R(\theta,\phi,0)^{-1}\\ 
							&= R(\theta,\phi,0) \sum_{n=0}^\infty \frac{(i\psi J_z)^n}{n!} R(\theta,\phi,0)^{-1} \\
							&= \sum_{n=0}^\infty \frac{(i\psi R(\theta,\phi,0)J_zR(\theta,\phi,0)^{-1})^n}{n!} \\
							&= e^{i\psi R(\theta,\phi,0)J_z R(\theta,\phi,0)^{-1}}\\
	\end{aligned}
\end{equation}

Comparing both sides of the equation, we can see that 

\begin{equation}
	\begin{aligned}
		J_n = R(\theta,\phi,0)J_z R(\theta,\phi,0)^{-1}
	\end{aligned}
\end{equation}

Further, arduous matrix algebra can be done explicitly to show the following identity holds: For any rotation matrix, $R$, and a specified generator, $J_k\in\{J_x,J_y,J_z\}$,

\begin{equation}
	\begin{aligned}
		RJ_kR^{-1} = J_x * [R]_{1k} + J_y * [R]_{2k}  +J_z * [R]_{3k} 
	\end{aligned}
\end{equation}

Showing this is true consists of proving it for $R_z(\alpha)$ and $R_y(\beta)$ and arguing that any product of matrices of this form will also work. (Combining \textbf{Equations 3.13 and 3.14}, we get 

\begin{equation}
	\begin{aligned}
		J_n &= R(\theta,\phi,0)J_z R(\theta,\phi,0)^{-1} \\
			&= J_x * [R(\theta,\phi,0)]_{13} + J_y * [R(\theta,\phi,0)]_{23}  +J_z * [R(\theta,\phi,0)]_{33}\\
			 &= n_1J_x + n_2J_y + n_3J_z \cite{Tung} \qedsymbol
	\end{aligned}
\end{equation} 

In this way, we can see that the generator of a rotation about any direction is a linear combination of the generators about our coordinate axes. For any generic rotation, $\psi$, about $n$, we see that 

\begin{equation}
	\begin{aligned}
		R_n(\psi) = e^{-i\psi n_1J_x}e^{-i\psi n_2J_y}e^{-i\psi n_3J_z}
	\end{aligned}
\end{equation} 

We can use this equation to write it in terms of a rotation's Euler angles in the following way:

\begin{equation}
	\begin{aligned}
		R(\alpha,\beta,\gamma) = e^{-i\alpha J_z}e^{-i\beta J_y}e^{-i\gamma J_z}
	\end{aligned}
\end{equation} 

In this sense, rotation in $\R^3$ can be written simply in terms of the angle measures and generators about coordinate axes.

\section{Irreducible Representations of $SO(3)$}

In order to uncover the irreducible representations of $SO(3)$, we will embark upon the same strategy that we used to find irreducible representations of $SO(2)$. Explicitly, we are looking to find invariant subspaces of our generator. However, we now have three generators to check, and the invariant subspace that we find has to coincide with every possible rotation built from the three. To add more complexity to this matter, we no longer are in an abelian group, which makes our struggle greater for a few reasons. First, finding invariant subspaces under these generators will be dependent on the order in which we apply the rotations. Secondly, irreducible representations need not be degree one anymore. We need to find an invariant subspace that exists independent of order of rotation and that works with all our generators that is spanned by possibly an infinite set of vectors.

First, let us tackle the problem of commuting. We begin with the following definition.

\begin{definition}
	The \textbf{commutator} of two matrices/operators $A$ and $B$ is defined by to be 
$$[A,B]=AB-BA$$
\end{definition}

By definition, two operators/matrices commute if and only if their commutator is equal to $0$. With this in mind, we can expect the generators of our rotations to have the following nontrivial commutators.

\begin{theorem}
	Let $J_k,J_l \in \{J_x,J_y,J_z\}$. Consider the word $xyz$, and all 6 rearrangements of it (identifying each rearrangement with a permutation in $S_3$). Then 
$$[J_k , J_l] = \begin{cases}
					0 & \text{if }k = l \\
					i*sign(klm)*J_m& \text{else}
					\end{cases}$$
where $m$ is the label of the remaining generator in $\{J_x,J_y,J_z\}$.
\end{theorem}

\noindent(Pf.) These nine commutators can all be verified by direct calculation. In the case where $k=l$, $[J_k,J_k] = J_kJ_k - J_kJ_k = 0$. However, if $k\neq l$, the calculation is dependent on the choice for $k$ and $l$. We will illustrate one example, and the other five commutators can be verified to follow a similar pattern. Take $k=x$ and $l=y$, our word $klm=xyz$ which represents the trivial permutation. Therefore, $sign(xyz)=1$.

\begin{equation}
	\begin{aligned}
		[J_x,J_y] &= J_xJ_y - J_yJ_x \\
					&= \begin{bmatrix}
							0 & 0 & 0\\
							0 & 0 & -i\\
							0 & i & 0
						\end{bmatrix}
						 \begin{bmatrix}
							0 & 0 & i\\
							0 & 0 & 0\\
							-i & 0 & 0
						\end{bmatrix}
						-
						\begin{bmatrix}
							0 & 0 & i\\
							0 & 0 & 0\\
							-i & 0 & 0
						\end{bmatrix}
						 \begin{bmatrix}
							0 & 0 & 0\\
							0 & 0 & -i\\
							0 & i & 0
						\end{bmatrix}\\
					&=  \begin{bmatrix}
							0 & 0 & 0\\
							-1 & 0 & 0\\
							0 & 0 & 0
						\end{bmatrix}
						-
						 \begin{bmatrix}
							0 & -1 & 0\\
							0 & 0 & 0\\
							0 & 0 & 0
						\end{bmatrix} \\
					&= \begin{bmatrix}
							0 & 1 & 0\\
							-1 & 0 & 0\\
							0 & 0 & 0
						\end{bmatrix} \\
					&=i \begin{bmatrix}
							0 & -i & 0\\
							i & 0 & 0\\
							0 & 0 & 0
						\end{bmatrix} \\
					&= i*sign(xyz)*J_z\\
	\end{aligned}
\end{equation} 

Clearly, transposing $x$ and $y$ will make the commutator negative (applying a transposition to $x$ and $y$ turns the word into $yxz$, so this is kept track of in the sign term). The other four cases can be recovered in a symmetric way. \qedsymbol

While we can clearly see that these relationships give us a non-commutative structue, they are incredibly helpful when viewed through another lens. If we consider the commutator operation as a "multiplication" operation, we can construct create a Lie algebra (using standard addition and scalar multiplication of matrices/operators). Lie algebras have an inherent structure of a vector space. Keeping the generators $\{J_x,J_y,J_z\}$, we will denote this Lie algebra as $\mathfrak{so(3)} := \{a_1J_x + a_2J_y + a_3Jz \mid a_z,a_2,a_3\in\C\}$ We can see than any arbitrary element, $M\in \mathfrak{so(3)}$ takes the following form:

\begin{equation}
	\begin{aligned}
		M=i\begin{bmatrix}
		0 & -a_3 & a_2 \\
		 a_3 & 0 & -a_1\\
		-a_2 & a_1 & 0
		\end{bmatrix}
	\end{aligned}
\end{equation} 

showing us that there is a one-to-one correspondence between the skew-symmetric matrices of size $3\times 3$ and matrices in $\mathfrak{so(3)}$. With this in mind, we can make the arguement that any representation of this Lie algebra ($\mathfrak{so(3)}$) will provide use with a representation of the Lie group ($SO(3)$). The process for this is much the same as embarked upon for $SO(2)$, but since those representations were one-dimensional and revolved only around one generator, it was not necessary to introduce the complexity of Lie algebras just yet to create the representations. 

As we search for the representations of $\mathfrak{so(3)}$, we still need to find commuting generators. For the moment, it seems that we did not reduce our problem whatosever, but rather added a layer of abstraction. In light of this, we can add even more abstraction to eventually shed some clarity on our circumstances.

\begin{definition}
	The \textbf{universal enveloping algebra} of a Lie algebra is the largest embedding of Lie algebra into an algebra. 
\end{definition}

The inherent utility of the universal enveloping algebra (of a Lie algebra) is that it is the most generic algebra that perserves the commutating properties of the Lie algebra. We will later discuss the necessity of appealing to this larger structure, but we will discuss what this space actuall looks like. 

The construction of the universal enveloping algebra of $\mathfrak{so(3)}$ (which we will denote $A_{\mathfrak{so(3)}}$) follows this prototypical scheme: First, we construct the tensor algebra of $\mathfrak{so(3)}$. This takes the following form:

\begin{equation}
	\begin{aligned}
		T(\mathfrak{so(3)}) = \bigoplus_{i=0}^\infty \left(\bigotimes_{k=0}^i \mathfrak{so(3)}\right) = \left(\C\right) \oplus \left(\mathfrak{so(3)}\right) \oplus \left(\mathfrak{so(3)}\otimes \mathfrak{so(3)}\right) \oplus \hdots
	\end{aligned}
\end{equation} 

Now that we have created (free) tensor algebra on elements of $\mathfrak{so(3)}$, we can establish the commutation relations on elements. We identify each of our generators $J_i$ with its counterpart in the tensor algebra, $\mathfrak{J}_i$. We can construct an ideal, $I$ to be generated by the three elements defined below: 

$$\mathfrak{g_i} \coloneq \mathfrak{J}_k \otimes \mathfrak{J}_l - \mathfrak{J}_l\otimes \mathfrak{J}_k - \begin{cases}
	0 & \text{if }k=l	\\
	i*sign(klm) \mathfrak{J}_m &\text{else}
\end{cases}$$

We then gather our universal enveloping algebra on $\mathfrak{so(3)}$ by taking the corresponding quotient space.

$$A_{\mathfrak{so(3)}} \coloneq T(\mathfrak{so(3)}) / I$$

As of now, we have shifted out problem to looking for commutative elements in $A_{\mathfrak{so(3)}}$. What is the inherent significance of this? If we view our generators $J_x$, $J_y$, $J_z$ as elements in $A_{\mathfrak{so(3)}}$, we can see that the commutator relations are still satisfied as they are in $\mathfrak{so(3)}$. However, due to the construction, there is no way to meaningfully combine them. All that we know is that any formal combination of our generators of can potentially be simplified with our commutator relations, and no other information is given. So, we must set out to find an element in $A_{\mathfrak{so(3)}}$ that commutes with all other elements.

\begin{definition}
	A \textbf{Casimir element} of a Lie algebra, $A$, is any element in the center of the universal enveloping algebra of $A$.
\end{definition}

This process, while a seemingly daunting task, is easily solved by taking a well-known physics concept to do the work for us. We define the element $\mathfrak{J^2} \coloneq (\mathfrak{J}_x \otimes \mathfrak{J}_x) \oplus (\mathfrak{J}_y \otimes \mathfrak{J}_y) \oplus (\mathfrak{J}_z \otimes \mathfrak{J}_z) = \mathfrak{J}_x^2 + \mathfrak{J}_y^2 + \mathfrak{J}_z^2$ where we suppress the tensor algebra notation for easier readbility. (It can be intuitively understood that any element referenced in $\mathfrak{this}$ $\mathfrak{script}$ will be contained in $A_{\mathfrak{so(3)}}$ and abide by its properties of multiplication).

\begin{theorem}
	$\mathfrak{J^2}$ is a Casimir element.
\end{theorem}

\noindent (Pf.) Let $k\in \{x,y,z\}$.
\begin{equation}
\begin{aligned}
	[\mathfrak{J^2}, \mathfrak{J}_k] &= [\mathfrak{J}_x^2, \mathfrak{J}_k] + [\mathfrak{J}_y^2, \mathfrak{J}_k] + [\mathfrak{J}_z^2, \mathfrak{J}_k]\\
										 &=\sum_{\substack{m\in\{x,y,z\} \\ m\neq k}} \mathfrak{J}_m [\mathfrak{J}_m, \mathfrak{J}_k] + [\mathfrak{J}_m, \mathfrak{J}_k]\mathfrak{J}_m\\
										&= \mathfrak{J}_{m_1}(i\mathfrak{J}_{l_1})+ (i\mathfrak{J}_{l_1}) \mathfrak{J}_{m_1} + \mathfrak{J}_{m_2}(-i\mathfrak{J}_{l_2}) +(-i\mathfrak{J}_{l_2}) \mathfrak{J}_{m_2} \\
										&= 0
\end{aligned}
\end{equation}
where the last equality is established by the fact that our choice is limited to $m_2$ and $l_2$ are forced to be exactly $l_1$ and $m_1$ respectively. \qedsymbol

The natural question to ask is why did we have to build the structure of the universal enveloping algebra to be able to use a seemingly straightforward element. The answer comes from the fact that there is no corresponding $J^2$ element in the Lie algebra $\mathfrak{so(3)}$. A direct computation would show us that:

\begin{equation} 
	\begin{aligned}
		J^2  &= J_x^2 + J_y^2 + J_z^2 \\
					&= \begin{bmatrix}
								0& 0 & 0 \\
								0 & 0 & -i \\
								0 & i & 0
							\end{bmatrix}^2 +\begin{bmatrix}
													0 & 0 & i \\
													0 & 0 & 0 \\
													-i & 0 & 0
												\end{bmatrix}^2 + \begin{bmatrix}
																			0 & -i & 0\\
																			i & 0 & 0 \\
																			0 & 0 & 0
																		\end{bmatrix}^2 \\
					&= \begin{bmatrix}
								0& 0 & 0 \\
								0 & 1 & 0 \\
								0 & 0 & 1
							\end{bmatrix} +\begin{bmatrix}
													1 & 0 & 0 \\
													0 & 0 & 0 \\
													0 & 0 & 1
												\end{bmatrix} + \begin{bmatrix}
																			1 & 0 & 0\\
																			0 & 1 & 0 \\
																			0 & 0 & 0
																		\end{bmatrix} \\
					&= \begin{bmatrix}
								2& 0 & 0 \\
								0 & 2 & 0 \\
								0 & 0 & 2
							\end{bmatrix} \\
					&= 2I_3 \notin \mathfrak{so(3)}
	\end{aligned}
\end{equation} 

Therefore, our discussion must be specific to $A_\mathfrak{so(3)}$ in order for us to develop any useful insight with commutative elements.

Letting $\phi$ be any irreducible representation of $A_\mathfrak{so(3)}$, the commutativity of $\mathfrak{J^2}$ shows us that for any $\mathfrak{J}_n \in A_\mathfrak{so(3)}$, $\phi(\mathfrak{J}_n)\phi(\mathfrak{J^2}) = \phi(\mathfrak{J^2})\phi(\mathfrak{J}_n)$. Schur's Theorem asserts that $\phi(\mathfrak{J^2}) = \lambda I_m$ where $\lambda\in\C$ and $m$ is the degree of the representation. This means that for any $v\in V$ (on which the representations are defined to be operators), $v$ is an eigenvector of $\phi(\mathfrak{J^2})$ with eigenvalue $\lambda$.

In search of our invariant subspace, we now must choose an eigenbasis of $V$ with respect to a basis of commuting generators (whose span collects all possible generators in $A_\mathfrak{so(3)}$). We can easily select $\mathfrak{J^2}$ and an additional generator from our typical collection $\{\mathfrak{J}_x,\mathfrak{J}_y,\mathfrak{J}_z\}$, but after choosing one, we can't choose any of the remaining options for fear of not commuting. Conventionally, we select $\{\mathfrak{J^2},\mathfrak{J}_z\}$ as our generators, and choose an eigenbasis of $V$ to be defined with respect to both these generators. With the remaining two generators, we construct two useful linear combinations that will aid us in our efforts.

\begin{equation}
	\begin{aligned}
		\mathfrak{J}_\pm = \mathfrak{J}_x \pm i\mathfrak{J}_y
	\end{aligned}
\end{equation} 

$\mathfrak{J}_+$ is referred to as the "raising" generator and $\mathfrak{J}_-$ is referred to as the "lowering generator". It is straightforward to verify that these new generators have special relationships with our conventionally selected generators as seen below:

\begin{equation}
	\begin{aligned}
		[\mathfrak{J}_z,\mathfrak{J}_+] = \mathfrak{J}_+
	\end{aligned}
\end{equation} 
\begin{equation}
	\begin{aligned}
		[\mathfrak{J}_z,\mathfrak{J}_-] = -\mathfrak{J}_-
	\end{aligned}
\end{equation} 
\begin{equation}
	\begin{aligned}
		[\mathfrak{J}_+,\mathfrak{J}_-] = 2\mathfrak{J}_z
	\end{aligned}
\end{equation} 
\begin{equation}
	\begin{aligned}
		\mathfrak{J}^2 = (\mathfrak{J}_z)^2  -\mathfrak{J}_z +\mathfrak{J}_+\mathfrak{J}_- = (\mathfrak{J}_z)^2 +\mathfrak{J}_z+ \mathfrak{J}_-\mathfrak{J}_+
	\end{aligned}
\end{equation} 
\begin{equation}
	\begin{aligned}
		\phi_{\mathfrak{J}_\pm}^\dag = \phi_{\mathfrak{J}_\mp}
	\end{aligned}
\end{equation} 

Using these identities, one can see that for any eigenvector, $v_m$, corresponding to the generator $J_z$ with eigenvalue $m$, then the following two identities hold:

\begin{equation}
	\begin{aligned}
		\phi_{\mathfrak{J}_z\mathfrak{J}_+}(v_m) &= \phi_{\mathfrak{J}_z\mathfrak{J}_+ (-\mathfrak{J}_+\mathfrak{J}_z + \mathfrak{J}_+\mathfrak{J}_z)}(v_m) \\
					&= \phi_{[\mathfrak{J}_z,\mathfrak{J}_+]}(v_m) + \phi_{\mathfrak{J}_+\mathfrak{J}_z}(v_m) \\
					&= \phi_{\mathfrak{J}_+}(v_m) + \phi_{\mathfrak{J}_+\mathfrak{J}_z}(v_m) \\
					&= \phi_{\mathfrak{J}_+(I + \mathfrak{J}_z)}(v_m) \\					
					&= \phi_{\mathfrak{J}_+}((\phi_{I} + \phi_{\mathfrak{J}_z})(v_m)) \\
					&= \phi_{\mathfrak{J}_+}  ((1+m)v_m) \\
					&= (m + 1)\phi_{J_+}(v_m)
	\end{aligned}
\end{equation} 
\begin{equation}
	\begin{aligned}
		\phi_{\mathfrak{J}_z\mathfrak{J}_-}(v_m) &= \phi_{\mathfrak{J}_z\mathfrak{J}_- (-\mathfrak{J}_-\mathfrak{J}_z + \mathfrak{J}_-\mathfrak{J}_z)}(v_m) \\
&=\phi_{[\mathfrak{J}_z,\mathfrak{J}_- ]}(v_m) + \phi_{\mathfrak{J}_-\mathfrak{J}_z}(v_m)\\
					&= \phi_{-\mathfrak{J}_-}(v_m) + \phi_{\mathfrak{J}_-\mathfrak{J}_z}(v_m)\\
					&= \phi_{\mathfrak{J}_-(-I + \mathfrak{J}_z)}(v_m) \\
					&= \phi_{\mathfrak{J}_-}((\phi_{-I} + \phi_{\mathfrak{J}_z})(v_m)) \\
					&= \phi_{\mathfrak{J}_-} ((-1+m)(v_m) \\
					&= (m - 1)\phi_{\mathfrak{J}_-}(v_m)\\
	\end{aligned}
\end{equation} 

These identities give an explicit reason for the naming convention (the raising generator increases the eigenvalue by $1$ and the lowering generator decreases it by $1$). With this, we conclude that repeated applications of $\phi_{\mathfrak{J}_+}$ or $\phi_{\mathfrak{J}_-}$ give us new eigenvectors of $\phi_{\mathfrak{J}_z}$, corresponding to eigenvalues $m + k$ and $m-l$ where $k$ represents the number of repeated applications of $\phi_{\mathfrak{J}_+}$ and $l$ represents the number of repeated applications of $\phi_{\mathfrak{J}_-}$ respectively. We can only repeat this process a finite number of times, as we are working in a finite dimensional vector space. Let us shift our focus somewhat to consider $k$ to be the maximum number of applications of $\phi_{\mathfrak{J}_+}$ before $\phi_{\mathfrak{J}_+}^{k+1}(v_k) = 0$. Let $\lambda$ be the eigenvalue corresponding to eigenvector $\phi_{\mathfrak{J}_+}^{k}(v_{k-1})$ of the operator $\phi_{\mathfrak{J}_z}$. Then, 

\begin{equation}
	\begin{aligned}
		\phi_{\mathfrak{J^2}}\phi_{\mathfrak{J}_+}^k(v_{k-1}) = \phi_{(\mathfrak{J}_z)^2 +\mathfrak{J}_z+ \mathfrak{J}_-\mathfrak{J}_+}(\phi_{\mathfrak{J}_+}^k(v_{k-1})) = \lambda(\lambda + 1) \phi_{J_+}^kv_{k-1}
	\end{aligned}
\end{equation} 

Since $\phi_{\mathfrak{J^2}}$ is a multiple of the identity, it must be the case that $\lambda(\lambda +1)$ is the only eigenvalue of $\phi_{\mathfrak{J^2}}$. Further, repeated applications of $\phi_{\mathfrak{J}_-}$ will decrease the eigenvalue of $\phi_{\mathfrak{J}_z}$ by $1$ for each application. However, this can only happen a finite number of times before we terminate. Therefore, if $l$ is the last application number of applications before  $\phi_{\mathfrak{J}_-}^{l+1}(v_{l}) = 0$ and $\phi_{J_z}v_l = l$.
\begin{equation}
	\begin{aligned}
		0 &= v_{l}  \phi_{\mathfrak{J}_-}^\dag \phi_{\mathfrak{J}_-} v_{l} \\
		&= v_{l} \phi_{\mathfrak{J}_+}\phi_{\mathfrak{J}_-}v_{l} \\
			&= v_{l}\phi_{\mathfrak{J^2} - (\mathfrak{J}_z)^2 + \mathfrak{J}_z}v_{l} \\
		&= \lambda(\lambda+ 1) - l(l - 1)\\
	\end{aligned}
\end{equation} 

Which implies that $\lambda = -l$. Given that we arrived at eigenvalue $l$ from $\lambda$ by applying the lowering operator an integer number of times, $\lambda - l \in \Z^+ \implies 2\lambda\in\Z^+ \implies \lambda = \frac{n}{2}$ for some $n\in\Z^+.$


\begin{theorem}
	The irreducible representations of the $A_\mathfrak{so(3)}$ are characterized by eigenvalues that take on positive integer and positive half-integers. If $\lambda$ is one such eigenvalue, then we can construct our eigenvectors, $\{v_m\}_{m=-\lambda}^\lambda$, corresponding to $\phi_{\mathfrak{J}_z}$ with eigenvalue $m$, using the raising or lowering operators. This gives us a degree $2\lambda +1$ representation characterized by the following relationships:
$$\phi_{\mathfrak{J}^2}(v_m) = \lambda(\lambda+1)v_m$$
$$\phi_{\mathfrak{J}_z}(v_m) =  mv_m$$ 
$$v_{m+1} =  \frac{\phi_{\mathfrak{J}_\pm}(v_m)}{\sqrt{\lambda(\lambda+1) - m(m\pm 1)}}$$
\end{theorem}

where the normalizing factor comes from a brief calculation similar to the matrix product calculation done in \textbf{Equation 3.32}. \textbf{Theorem 3.10} gives us a way to calculate an irreducible, unitary representation up to any degree. The unitary aspect comes from the fact that $\{v_m\}$ are normalized and orthogonal by their construction. When we do the calculations, we have constructed our operators (in the image of our representation) and seek to construct an orthonormal basis with respect to these operators. As we have shown, following this form will ensure that we cannot create a nontrivial-invariant subspace, due to the fact that the cyclical nature of our eigenbasis construction. Explicitly, every vector in this basis will be an eigenvector of $\phi_{\mathfrak{J^2}}$, and since this operator commutes with every other, we inheret a set of eigenvectors that will span the entire space.

Given our hard work to establish a representation on $A_\mathfrak{so(3)}$, we can immediately conclude that we can construct a representation on $SO(3)$. Any rotation under the image of such an irreducible representation must behave in the following way:

\begin{equation}
\begin{aligned}
	\phi_j(R(\alpha,\beta,\gamma)) &= e^{-i\alpha\phi_{\mathfrak{J}_z}}e^{-i\beta\phi_{\mathfrak{J}_y}}e^{-i\gamma\phi_{\mathfrak{J}_z}}	
\end{aligned}
\end{equation}

We are required to compute explicitly $\phi_{\mathfrak{J}_z}$ and $\phi_{\mathfrak{J}_y}$ to make use of these formulas. The following example will illustrate this process more closely.

\begin{example}
	Take $\lambda= \frac{1}{2}$. We expect the degree of our representation to be $2$. 
\end{example}

We can immediately conclude that:

\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J^2}} = \begin{bmatrix}
									\frac{3}{4} & 0 \\
									0 & \frac{3}{4}
								\end{bmatrix}
\end{aligned}
\end{equation}
and 
\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J}_z} = \begin{bmatrix}
									\frac{1}{2} & 0 \\
									0 & \frac{-1}{2}
								\end{bmatrix}
\end{aligned}
\end{equation}

Using the identities found in \textbf{Equations 3.24-3.28}, we can calculate $\phi_{\mathfrak{J}_\pm}$ as 

\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J}_+} = \begin{bmatrix}
									0 & 1 \\
									0 & 0
								\end{bmatrix}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J}_-} = \begin{bmatrix}
									0 & 0 \\
									1 & 0
								\end{bmatrix}
\end{aligned}
\end{equation}
We can use these two matrices to solve a system of equations to aquire $\phi_{\mathfrak{J}_y}$ and $\phi_{\mathfrak{J}_x}$.
\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J}_x} = \begin{bmatrix}
									0 & \frac{1}{2} \\
									\frac{1}{2} & 0
								\end{bmatrix}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	\phi_{\mathfrak{J}_y} = \begin{bmatrix}
									0 & \frac{1}{2i} \\
									\frac{-1}{2i} & 0
								\end{bmatrix}
\end{aligned}
\end{equation}

We note that $\phi_{\mathfrak{J}_z}$ and $\phi_{\mathfrak{J}_y}$ are a scalar multiple away from having the desirable property that its square is the identity. Therefore, any rotation under $\phi$ is represented by the matrix

\begin{equation}
\begin{aligned}
	\phi_\frac{1}{2}(R(\alpha,\beta,\gamma)) &= e^{-i\alpha\phi_{\mathfrak{J}_z}}e^{-i\beta\phi_{\mathfrak{J}_y}}e^{-i\gamma\phi_{\mathfrak{J}_z}}	\\
													&= (\cos(\frac{\alpha}{2})I_2 -isin(\frac{\alpha}{2})2*\phi_{\mathfrak{J}_z})(\cos(\frac{\beta}{2})I_2 -isin(\frac{\beta}{2})\phi_{\mathfrak{J}_y})(\cos(\frac{\gamma}{2})I_2 -isin(\frac{\gamma}{2})\phi_{\mathfrak{J}_z})\\
													&=\begin{bmatrix}
															\cos(\frac{\alpha}{2}) - i (\sin(\frac{\alpha}{2}) & 0\\
															0 & \cos(\frac{\alpha}{2}) + i (\sin(\frac{\alpha}{2})
														\end{bmatrix}
														\begin{bmatrix}
															\cos(\frac{\beta}{2}) & -\sin(\frac{\beta}{2}) \\
															  \sin(\frac{\beta}{2}) & \cos(\frac{\beta}{2}) 
														\end{bmatrix}
\begin{bmatrix}
															\cos(\frac{\gamma}{2}) - i (\sin(\frac{\gamma}{2}) & 0\\
															0 & \cos(\frac{\gamma}{2}) + i (\sin(\frac{\gamma}{2})
														\end{bmatrix} \\
														&=\begin{bmatrix}
															e^{-i\frac{\alpha}{2}}\cos(\frac{\beta}{2})e^{-i\frac{\gamma}{2}} & -e^{-i\frac{\alpha}{2}}\sin(\frac{\beta}{2})e^{-i\frac{\gamma}{2}} \\
															e^{-i\frac{\alpha}{2}}  \sin(\frac{\beta}{2})e^{-i\frac{\gamma}{2}} & e^{-i\frac{\alpha}{2}}\cos(\frac{\beta}{2}) e^{-i\frac{\gamma}{2}}
														\end{bmatrix}
\end{aligned}
\end{equation}

It is worth noting that this example illustrates a multi-valued representation, seeing as $\phi_\frac{1}{2}(R(0,2\pi,0)) = -I_2$. In general, all of the half integer eigenvalue representations will behave in this way.

\chapter{The Euclidean Group in Two and Three Dimensions}

While we have discussed rotations extensively, there are many more physical phenomena that are not a result of this action. More specifically, this section will focus on actions that can be represented by compositions of physical rotations and translations. We call the group that takes combinations of these actions the Euclidean Group (in $n$ dimensional space). For the purposes of our study, we will focus on the two and three dimensional cases. While we have covered much of the group for the rotational component of this group, we will need to handle the addition of translations with care.

\section{Construction and Properties of $E_2$}

\begin{definition}
	The \textbf{Euclidean Group, $E_n$,} (in $n$ dimensions) is the group of all continuous, isometric, linear transformations on $\R^n$.
\end{definition}

For any transformation of this kind, all vectors, $x\in\R^n$, get mapped to $x'\in\R^n$ in the following way 
$$x' = Rx + b$$
for some fixed $R\in SO(n)$ and $b\in\R^n$. This gives us a natural decomposition of our transformation into a rotation (given by our matrix $R$) and a translation (by a vector, $b$).

In two dimensions, our rotations are characterized by one angle and our translations are characterized by one vector in $\R^2$. Therefore, any element can be reference in $E_2$ by choosing $\theta$ and $b$ respectively. For any $g(\theta,b)\in E_2$, $x\overset{g(\theta,b)}{\mapsto}x'$ by through the following transformation:

$$\begin{bmatrix}x'_1\\x'_2\end{bmatrix} = \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta) 
		\end{bmatrix} \begin{bmatrix}x_1\\x_2\end{bmatrix} + \begin{bmatrix}b_1\\b_2\end{bmatrix} $$

One can see that through this lens, the composition of transformations in $E_2$ follows the general rule:

$$g(\theta_2,b_2)*g(\theta_1,b_1) = g(\theta_2+\theta_1,R(\theta_2)b_1 + b_2)$$

We can construct an invertible matrix to represent each transformation if we take each vector in $\R^2$ to be embedded in $\R^3$ on the $z=1$ plane.

$$\begin{bmatrix}x'_1\\x'_2 \\ 1\end{bmatrix} = \begin{bmatrix}
			\cos(\theta) & -\sin(\theta) & b_1\\
			\sin(\theta) & \cos(\theta) & b_2 \\
			0&0&1
		\end{bmatrix} \begin{bmatrix}x_1\\x_2\\1\end{bmatrix} $$

Adopting this convetion, we can easily derive the subgroup of $E_2$ corresponding to pure rotations in terms of the generator who's exists we proved in Chapter 2. 

$$J=\begin{bmatrix}
			0 & -i & 0\\
			i & 0 & 0 \\
			0 & 0 & 0
		\end{bmatrix}$$

Turning our focus to the translations, we can perform a similar derivation. At first, we focus on the one dimensional case. For any one-dimensional translation, $T_b(x)\coloneq x + b$,  we can see that translating by a vector with arbitrarily small magnitutde should result in an arbitrarily small translation. Explicitly,

$$T_{dx} = I -idxP$$

where we will show $P$ to be the generator of translations in an almost symmetric calculation as we did in $SO(2)$.

We can write $T_x$ in terms of our generator by utilizing the following two equations:

$$T_{x+dx} = T_x + dx \frac{d}{dx}T_x$$
$$T_{x+dx} = T_{dx}*T_x$$

Solving this system gives us the same result as in $SO(2)$:

$$T(x) = e^{-iPx}$$

We can explicitly calculate $P$ by looking at how $T_dx$ should interact with it.

$$x + dx = T_{dx}(x) = x -i*dx * P \Rightarrow P = i$$

Generalizing to two dimensions requires we construct the same scheme twice (treating each component separately). Therefore, we get two generator matrices with respect to each component of the matrix specified to be the location for a component of the translation vector.

$$P_x = \begin{bmatrix}
			0 & 0 & i\\
			0 & 0 & 0 \\
			0 & 0 & 0
		\end{bmatrix}, \hspace{3mm}P_y = \begin{bmatrix}
			0 & 0 & 0\\
			0 & 0 & i \\
			0 & 0 & 0
		\end{bmatrix}$$

The physical properties of translations will allow us to create an abelian subgroup of $E_2$ that is restricted to pure translations. Any translation in this group can be written in terms of the generators in the following way:

$$T_b = g(0,b) = e^{-ib_1P_x}e^{-ib_2P_y}$$

Now that we have dealt with rotations and translations separately, we can begin to consider the two physical actions together. A useful observation we can make to this end deals with the relationship between rotations and translations: For any rotation by angle $\theta$, matrix algebra will show that:

$$R(\theta)P_xR(\theta)^{-1} = [R(\theta)]_{11}P_x +  [R(\theta)]_{21}P_y$$
$$R(\theta)P_yR(\theta)^{-1} = [R(\theta)]_{12}P_x +  [R(\theta)]_{22}P_xy$$

With this in mind, it is not a far jump to conclude that for for any translation, $T_b$,

$$R(\theta)T_bR(\theta)^{-1} = T_{R(\theta)b}$$

This identity will is geometrically intuitive and will prove incredibly useful for calculations later. Most usefully, we can see a natural decomposition form for group elements in $E_2$.

\begin{theorem}
	For any $g(\theta,b)\in E_2$, there is a natural decomposition of the transformation into a rotation times a translation.
$$g(\theta,b) = \overset{T_b}{g(0,b)}\overset{R(\theta)}{g(\theta,0)}$$
\end{theorem}

\noindent(Pf.) 
\begin{equation}
\begin{aligned}
g(\theta,b)R(\theta)^{-1} = g(\theta,b)g(-\theta,0) = g(\theta-\theta,b) = T_b \cite{Tung} \qedsymbol
\end{aligned}
\end{equation}

Having discussed ways to generically refer to elements of $E_2$, we can begin to discuss the construction of its irreducible representations.

\section{Irreducible Representations of $E_2$}

Much like we did in the last chapter, we will consider the irreducible representations defined on the Lie algebra of its generators to gain insight into the underlying group. Let us begin by characterizing the Lie algebra. We will refer to this structure as $\mathfrak{e_2}$. Given our generators $J, P_x,P_y$, we can explicitly construct a generic element of the Lie algebra as a linear combination.

$$\mathfrak{e_2} = \left\{i\begin{bmatrix}
								0 & -a & b \\
								a & 0 & c \\
								0 & 0 & 0
							 \end{bmatrix} \mid a,b,c \in \C\right\}$$

Now that we have defined our space rigidly, we will begin analyzing the commutators of our generators. It can be easily shown through matrix algebra that the following commutator relations hold:

\begin{equation}
\begin{aligned}
	[P_x,P_y] = 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	[J,P_x] = iP_y
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	[J,P_y] = -iP_x
\end{aligned}
\end{equation}

The fact that the generators for translations commute with each other gives us a vantage point to analyze an abelian subgroup of $E_2$. Consider the subgroup of pure translations, $T_2 = \{ T_b \coloneq g(\theta,b)\in E_2 \mid \theta=0\}$. Based on our argument with the commutator of generators, this group is abelian. As a result, the quotient group $E_2/T_2$ is well-defined, and intuitively, it is isomorphic to $SO(2)$. Since irreducible representations on quotient groups are irreducible on whole groups\textbf{go back to chapter 1 to prove}, it is clear that our analysis of $SO(2)$ will immediately show us the following irreducible representations of $E_2$.

$$\phi_m:E_2\rightarrow \C$$
$$g(\theta,b)\mapsto e^{im\theta}$$

Now, if we look to encorporate the other generators, we can follow a similar process to how we constructed irreducible representations in $SO(3)$. We begin my constructing the raising and lowering generators in the following way:

\begin{equation}
\begin{aligned}
	P_\pm \coloneq P_x \pm iP_y
\end{aligned}
\end{equation}

 Then, matrix algebra will show that the new commutators on the generators follow the following equality:

\begin{equation}
\begin{aligned}
	[J,P_\pm] = \pm P_\pm 
\end{aligned}
\end{equation}

As we did in the previous chapter, we have to leave the comfort of the Lie algebra and journey into the universal enveloping algebra to find mutually commuting generators. We will denote the universal enveloping algebra of $E_2$ as $A_{\mathfrak{e_2}}$. All of the commutator identities we stated above will clearly translate to identical relations in our new space. As a reminder, when we discuss generators in this space, we will refer to them with our $\mathfrak{special script}$. 

In $A_{\mathfrak{e_2}}$, there is a Casimir element similar to that of $A_\mathfrak{so(3)}$. 
\begin{equation}
\begin{aligned}
	\mathfrak{P}^2 = (\mathfrak{P}_x)^2 + (\mathfrak{P}_y)^2 = \mathfrak{P}_+\mathfrak{P}_- = \mathfrak{P}_-\mathfrak{P}_+
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	[\mathfrak{P}^2,J] =[\mathfrak{P}^2,\mathfrak{P}_\pm]  = 0
\end{aligned}
\end{equation}

As is typical with Casimir elements, its image under any irreducible representation is a multiple of the identity. We will call this multiple $p$. Further, as we have done before, any eigenvector of one map can be used to generate (through its application) an eigenvector of a commuting map. Since $\mathfrak{J}$ is defined in the same way that that it is in $SO(2)$, its image under an irreducible representation has eigenvalues corresponding to the integers. Allowing $\psi$ to be an irreducible representation of $A_\mathfrak{e_2}$, we let $v_m$ be an eigenvector of $\phi_\mathfrak{J}$ corresponding to eigenvalue $m\in\Z$ Then the following equations characterize the irreducible representation:
\begin{equation}
\begin{aligned}
	 \psi(\mathfrak{P}^2)v_m = pv_m
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	 \psi(\mathfrak{J})v_m = mv_m
\end{aligned}
\end{equation}

We can also see that the raising and lowering generators naturally have the desired effect
\begin{equation}
\begin{aligned}
	 \psi(\mathfrak{J}\mathfrak{P}_+)v_m = (m+1)\mathfrak{P}_+v_m
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	 \psi(\mathfrak{J}\mathfrak{P}_-)v_m = (m-1)\mathfrak{P}_-v_m
\end{aligned}
\end{equation}

Given the way that $\mathfrak{P}^2$ was defined, we can see that $\psi(\mathfrak{P}^2)$ must be positive-definite. Therefore, its eigenvalues are non-zero. In the case where $p=0$, it corresponds to a pure rotation ($\psi(\mathfrak{P}^2) = 0$ and so translation does not occur). Therefore, this corresponds to the irreducible representation we calculated for the quotient group. 

If $p>0$, we will define the sequence of eigenvectors $\{v_m\}_{m\in\Z}$ to be defined by the following way:

\begin{equation}
\begin{aligned}
	 v_{m+1} \coloneq \frac{i}{p}\psi(\mathfrak{P}_+)v_m
\end{aligned}
\end{equation}

As resulting and equivalent way of defining this sequence would be to use the lowering generator $v_{m-1} \coloneq \frac{-i}{p}\psi(\mathfrak{P}_-)v_m$. We will need to use both constructions to collect every vector. Since there is clearly no restriction on $m$, the vector space is infinite dimensional. As a result, any representation we would recover from the method would not have an ability to be written succintly. There are still interesting relationships to explore the basis vectors we generated (which will prove useful later), but for now we will turn our attention to another method for finding irreducible representations.

Recall that the subspace generated by $\mathfrak{P}_x$ and $\mathfrak{P}_y$ is abelian. As a result, it is a perfect candidate for analysis in search for invariant subspace. However, we originally dismissed it because $\mathfrak{J}$ does not commute with any of the generators in this space. As a result, there is only one possible eigenvector that could simultaneously be defined for any one generator and our Casimir element. Explicitly, if we choose $v_0$ to be an eigenvector of say $\psi_{\mathfrak{P}_x}$, corresponding to eigenvalue $\lambda$ of the irreducible, unitary representation $\psi$, then 

\begin{equation}
\begin{aligned}
	\psi_{\mathfrak{P}_x}(v_0) = \lambda v_0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	\psi_{\mathfrak{P}_y}(v_0) = 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	\psi_{\mathfrak{P}^2}(v_0) = \lambda^2 v_0
\end{aligned}
\end{equation}

Consider taking this vector and applying a rotation to it. While it may seem odd that we would do this, there is some utility that will come from it. If $R(\theta)$ is a rotation by the angle $\theta$, then

\begin{equation}
\begin{aligned}
	\psi_{\mathfrak{P}_x} R(\theta)v_0 &= R(\theta)R(\theta)^{-1}\psi_{\mathfrak{P}_x} R(\theta)v_0 \\
									&= R(\theta)([R(-\theta)]_{11}\psi_{\mathfrak{P}_x} + [R(-\theta)]_{21}\psi_{\mathfrak{P}_y} )v_0 \\
									&= \lambda' R(\theta) v_0
\end{aligned}
\end{equation}
where $\lambda'=[R(-\theta)]_{11}\lambda$. A similar calculation can be done if we first considered an eigenvector of $\psi_{\mathfrak{P}_y}$. Therefore, we conclude that $R(\theta)v_0$ is in fact an eigenvector of $\psi_{\mathfrak{P}_x}$. We can generate the set of all vectors of this form $\{R(\theta)v_0\}$ and notice that the span of this set is invariant under operations from $E_2$. Explicitly, for any $R(\theta)v_0$,

\begin{equation}
\begin{aligned}
	T_b(R(\theta)v_0) = e^{-ib_1P_x}e^{-ib_2P_y}R(\theta)v_0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	R(\phi)(R(\theta)v_0) = R(\phi+\theta)v_0
\end{aligned}
\end{equation}

Since the vectors of $\{R(\theta)v_0\}$ are orthogonal to one another (being eigenvectors corresponding to distinct eigenvalues), they form an orthogonal basis of an invariant vector space under $E_2$.
 

\textbf{Clean up explanation and finish up}





\newpage

\section{Construction of $E_3$ and its Irreducible Representations}

$E_3$ consists of all compositions of rotations and translations in three dimensional space. We have already established most of the theory regarding rotations in $\R^3$ and the extension of our work with translations is not too difficult to make rigorous. Following our natrual inclination, it is justified to write any $g\in E_3$ as  $g = R(\alpha,\beta,\gamma)T_b$ for some rotation in $SO(3)$ and some translation in $T_3 = \{T_b \mid b\in\R^3\}$. We summarize the main results of our generators of $E_3$ with the following statements: If $\{J_x,J_y,J_z\}$ are the generators of $SO(3)$ and $\{P_x,P_y,P_z\}$ are the generators of $T_3$, then



\begin{equation}
\begin{aligned}
	T_b = e^{-ib_1P_x}e^{-ib_2P_y}e^{-ib_3P_z}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	R(\alpha,\beta,\gamma) = e^{-i\alpha J_z}e^{-i\beta J_y}e^{-i\gamma J_z}
\end{aligned}
\end{equation}	

We imbed the action of a transformation in $E_3$ into $M_4(\R)$ by adding an additional component to every vector in $\R^3$ (setting it to $1$) and taking a matrix of this form:

\begin{equation}
\begin{aligned}
	x' =
	\begin{bmatrix}
		\underset{3\times 3}{R(\alpha,\beta,\gamma)} & \underset{3\times 1}{b}\\
		\underset{1\times 3}{0} & 1
	\end{bmatrix} x
\end{aligned}
\end{equation}

giving our generators explicit matrix form in the way we expect.

Therefore, it is straightforward to compute the following commutation relations:

\begin{equation}
\begin{aligned}
	[P_k,P_l] = 0 \hspace{3mm} \forall k,l\in \{x,y,z\}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
	[J_k,J_l] = \begin{cases}
					0 & \text{if } k = l\\
					i*sign(klm)*J_m & else
					\end{cases}
\end{aligned}
\end{equation}
where $m$ is the remaining label for the unused generator in the commutator.
\begin{equation}
\begin{aligned}
	[P_k,J_l] = \begin{cases}
					0 & \text{if } k = l\\
					i*sign(klm)*P_m & else \end{cases}
\end{aligned}
\end{equation}
where $m$ is the unused label not in the commutator.

It is worth nothing that the subgroup generated by translations is abelian and therefore forms an invariant subgroup of $E_3$ (as is expected). As a result of this, we explicitly argue that for any rotation $R\in SO(3)$ and translation $T_b\in T_3$,
\begin{equation}
\begin{aligned}
	RT_bR^{-1} = T_{b'}
\end{aligned}
\end{equation}
where $b' = Rb$. We arrive at the above by following the same technique established in $E_2$ above.

Further, it is acceptable to decompose any element in $E_3$ as a composition of a pure rotation and a pure translation:

\begin{equation}
\begin{aligned}
	g\in E_3 \Rightarrow g = R(\alpha,\beta,\gamma)T_b\text{ for some } b\in\R^3, R\in SO(3)
\end{aligned}
\end{equation}

As we continue, we begin our descent into the familiar universal enveloping algebra of our Lie group, we identify each of the generators with its corresponding element. Now that we have more elements to work with, we can actually identifiy two useful Casimir elements in $A_\mathfrak{e_2}$: $\mathfrak{P^2}\coloneq (\mathfrak{P}_x)^2 + (\mathfrak{P}_y)^2 + (\mathfrak{P}_z)^2$ and $\mathfrak{J}* \mathfrak{P} \coloneq \mathfrak{J}_x\mathfrak{P}_x +\mathfrak{J}_y\mathfrak{P}_y + \mathfrak{J}_z\mathfrak{P}_z$.

$\mathfrak{P^2}$ commutes with all the translational generators trivially, and it commutes with the rotational generators because \textbf{insert reason}
$\mathfrak{J}*\mathfrak{P}$ commutes with the rotational generators for the same reason that $\mathfrak{P}^2$ does, and it commutes with the translational generators for the following reason:
\begin{equation}
\begin{aligned}
 [\mathfrak{J}_x\mathfrak{P}_x+\mathfrak{J}_y\mathfrak{P}_y+\mathfrak{J}_z\mathfrak{P}_z, \mathfrak{P}_k] = [\mathfrak{J}_x,\mathfrak{P}_k]\mathfrak{P}_x + [\mathfrak{J}_y,\mathfrak{P}_k]\mathfrak{P}_y + [\mathfrak{J}_z,\mathfrak{P}_k]\mathfrak{P}_z = i\mathfrak{P}_k\mathfrak{P}_m-i\mathfrak{P}_m\mathfrak{P}_k = 0
\end{aligned}
\end{equation}
where $k$ is a label in $\{x,y,z\}$ and $m$ is a label not equal to $k$ that is determined by the choice of $k$.

Since $T_3$ is a nice abelian subgroup, we can look to the well defined quotient group $E_3/T_3$ for inspiration. As it turns out, this quotient group is isomorphic to $SO(3)$ and as a result, we can take the irreducible representations that we defined in terms of the half and whole non-negative integers.

$$\psi_j: E_3 \rightarrow M_{2j+1}(\C)$$
$$g(R(\alpha,\beta,\gamma), T_b) \mapsto \phi_j(R(\alpha,\beta,\gamma))$$

However, since pure translation get mapped to the identity, our image of Casimir elements through these maps will have eigenvalue of zero. In order to make use of the Casimir elements, we will need to take the alternate method for searching for representations that we explored in the previous section. 











\newpage

\chapter{The Lorentz and Poincare Groups}

\section{Construction and Basic Properties}

\begin{definition}
	An \textbf{event} is an ordered triple together with an additional parameter, meant to represent time. An event is conventionally indexed by the integers $\mu = 0,1,2,3$. Referring to an event as $x$ references the event a four (component) vector, and specifying $x^{\mu=0}$ gives us $ct$ where $c$ is the speed of light and $t$ is a time. 
\end{definition}

Our natural inclination is to believe that these vectors are just elements of $\R^4$. However, since this system is based on physical phenomena, the setup is a little more complicated.

\begin{definition}
	The length of an event is defined by the following equation:
$$\mid x\mid^2 \coloneq (x^1)^2+(x^2)^2+(x^3)^2 - (x^0)^2 = (x^1)^2+(x^2)^2+(x^3)^2 - c^2t^2$$
\end{definition}
























Hello











% ------------- End main chapters ----------------------



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% the following two commands set up the bibliography and references 
%% automatically throughout the document.  The file my_special_bibliography.bib 
%% is one you create with all the info about all your references.  The alpha.bst file 
%% is included in your LaTeX distribution, but you can modify it if you want.  (But 
%% you don't want.)  
%\bibliography{my_special_ibliography}
%\bibliographystyle{alpha}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}
\bibliographystyle{IEEEtran}

\bibitem{Tung}

\bibitem{Mendes}
%A.~F. Blumberg and G.~L. Mellor.
%\newblock A description of a three-dimensional coastal circulation model.
%\newblock In N.~Heaps, editor, {\em Three Dimensional Coastal Ocean Models},
  %pages 1--16. Amer. Geophys. Union, 1987.

%\bibitem{Gill}
%A.~Gill.
%\newblock {\em Atmosphere - {O}cean {D}ynamics}.
%\newblock Academic Press, 1982.

%\bibitem{Chob}
%P.~F. Choboter, R.~M. Samelson, and J.~S. Allen.
%\newblock A {N}ew {S}olution of a {N}onlinear {M}odel of {U}pwelling.
%\newblock {\em J. Phys Oceanogr.}, 35:532--544, 2005.

%\bibitem{Lentz}
%S.~J. Lentz and D.~C. Chapman.
%\newblock The importance of non-linear cross-shelf momentum flux during
  %wind-driven coastal upwelling.
%\newblock {\em J. Phys. Oceanogr.}, 34:2444--2457, 2004.

%\bibitem{Ped2}
%J.~Pedlosky.
%\newblock A {N}onlinear {M}odel of the {O}nset of {U}pwelling.
%\newblock {\em J. Phys Oceanogr.}, 8:178--187, 1978.

\end{thebibliography}


% Indents Appendix in Table of Contents
\makeatletter
\addtocontents{toc}{\let\protect\l@chapter\protect\l@section}
\makeatother

% Hack to make Appendices to appear in Table of Contents
\addtocontents{toc}{%
   \noindent APPENDICES
}

\begin{appendices}
\chapter{Appendix A Title} \label{Appendix A}

Blah blah

\end{appendices}


%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}